{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee74684b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# sys.path.insert(0, '../../Utilities/')\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "from scipy.interpolate import griddata\n",
    "#from plotting import newfig, savefig\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import matplotlib.gridspec as gridspec\n",
    "import time\n",
    "# tf.enable_eager_execution()\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5b325f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhysicsInformedNN:\n",
    "    # Initialize the class\n",
    "    def __init__(self, X, u, layers, lb, ub):\n",
    "        \n",
    "        self.lb = lb\n",
    "        self.ub = ub\n",
    "        \n",
    "        self.x = X[:,0:1]\n",
    "        self.t = X[:,1:2]\n",
    "        self.u = u\n",
    "        \n",
    "        self.layers = layers\n",
    "        \n",
    "        # Initialize NNs\n",
    "        self.weights, self.biases = self.initialize_NN(layers)\n",
    "        \n",
    "        # tf placeholders and graph\n",
    "        self.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True,\n",
    "                                                     log_device_placement=True))\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self.lambda_1 = tf.Variable([0.0], dtype=tf.float32)\n",
    "        self.lambda_2 = tf.Variable([-6.0], dtype=tf.float32)\n",
    "        \n",
    "        self.x_tf = tf.placeholder(tf.float32, shape=[None, self.x.shape[1]])\n",
    "        self.t_tf = tf.placeholder(tf.float32, shape=[None, self.t.shape[1]])\n",
    "        self.u_tf = tf.placeholder(tf.float32, shape=[None, self.u.shape[1]])\n",
    "                \n",
    "        self.u_pred = self.net_u(self.x_tf, self.t_tf)\n",
    "        self.f_pred = self.net_f(self.x_tf, self.t_tf)\n",
    "        \n",
    "        self.loss = tf.reduce_mean(tf.square(self.u_tf - self.u_pred)) + \\\n",
    "                    tf.reduce_mean(tf.square(self.f_pred))\n",
    "        \n",
    "#         self.optimizer = tf.contrib.opt.ScipyOptimizerInterface(self.loss, \n",
    "#                                                                 method = 'L-BFGS-B', \n",
    "#                                                                 options = {'maxiter': 50000,\n",
    "#                                                                            'maxfun': 50000,\n",
    "#                                                                            'maxcor': 50,\n",
    "#                                                                            'maxls': 50,\n",
    "#                                                                            'ftol' : 1.0 * np.finfo(float).eps})\n",
    "    \n",
    "        self.optimizer_Adam = tf.train.AdamOptimizer()\n",
    "        self.train_op_Adam = self.optimizer_Adam.minimize(self.loss)\n",
    "        \n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess.run(init)\n",
    "\n",
    "    def initialize_NN(self, layers):        \n",
    "        weights = []\n",
    "        biases = []\n",
    "        num_layers = len(layers) \n",
    "        for l in range(0,num_layers-1):\n",
    "            W = self.xavier_init(size=[layers[l], layers[l+1]])\n",
    "            b = tf.Variable(tf.zeros([1,layers[l+1]], dtype=tf.float32), dtype=tf.float32)\n",
    "            weights.append(W)\n",
    "            biases.append(b)        \n",
    "        return weights, biases\n",
    "        \n",
    "    def xavier_init(self, size):\n",
    "        in_dim = size[0]\n",
    "        out_dim = size[1]        \n",
    "        xavier_stddev = np.sqrt(2/(in_dim + out_dim))\n",
    "        return tf.Variable(tf.truncated_normal([in_dim, out_dim], stddev=xavier_stddev), dtype=tf.float32)\n",
    "    \n",
    "    def neural_net(self, X, weights, biases):\n",
    "        num_layers = len(weights) + 1\n",
    "        \n",
    "        H = 2.0*(X - self.lb)/(self.ub - self.lb) - 1.0\n",
    "        for l in range(0,num_layers-2):\n",
    "            W = weights[l]\n",
    "            b = biases[l]\n",
    "            H = tf.tanh(tf.add(tf.matmul(H, W), b))\n",
    "        W = weights[-1]\n",
    "        b = biases[-1]\n",
    "        Y = tf.add(tf.matmul(H, W), b)\n",
    "        return Y\n",
    "            \n",
    "    def net_u(self, x, t):  \n",
    "        u = self.neural_net(tf.concat([x,t],1), self.weights, self.biases)\n",
    "        return u\n",
    "    \n",
    "    def net_f(self, x, t):\n",
    "        lambda_1 = self.lambda_1        \n",
    "        lambda_2 = tf.exp(self.lambda_2)\n",
    "        u = self.net_u(x,t)\n",
    "        u_t = tf.gradients(u, t)[0]\n",
    "        u_x = tf.gradients(u, x)[0]\n",
    "        u_xx = tf.gradients(u_x, x)[0]\n",
    "        f = u_t + lambda_1*u*u_x - lambda_2*u_xx\n",
    "        \n",
    "        return f\n",
    "    \n",
    "    def callback(self, loss, lambda_1, lambda_2):\n",
    "        print('Loss: %e, l1: %.5f, l2: %.5f' % (loss, lambda_1, np.exp(lambda_2)))\n",
    "        \n",
    "        \n",
    "    def train(self, nIter):\n",
    "        tf_dict = {self.x_tf: self.x, self.t_tf: self.t, self.u_tf: self.u}\n",
    "        \n",
    "        start_time = time.time()\n",
    "        for it in range(nIter):\n",
    "            self.sess.run(self.train_op_Adam, tf_dict)\n",
    "            \n",
    "            # Print\n",
    "            if it % 10 == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                loss_value = self.sess.run(self.loss, tf_dict)\n",
    "                lambda_1_value = self.sess.run(self.lambda_1)\n",
    "                lambda_2_value = np.exp(self.sess.run(self.lambda_2))\n",
    "                print('It: %d, Loss: %.3e, Lambda_1: %.3f, Lambda_2: %.6f, Time: %.2f' % \n",
    "                      (it, loss_value, lambda_1_value, lambda_2_value, elapsed))\n",
    "                start_time = time.time()\n",
    "        \n",
    "#         self.optimizer_Adam.minimize(self.loss)\n",
    "        \n",
    "        \n",
    "#         self.optimizer.minimize(self.sess,\n",
    "#                                 feed_dict = tf_dict,\n",
    "#                                 fetches = [self.loss, self.lambda_1, self.lambda_2],\n",
    "#                                 loss_callback = self.callback)\n",
    "        \n",
    "        \n",
    "    def predict(self, X_star):\n",
    "        \n",
    "        tf_dict = {self.x_tf: X_star[:,0:1], self.t_tf: X_star[:,1:2]}\n",
    "        \n",
    "        u_star = self.sess.run(self.u_pred, tf_dict)\n",
    "        f_star = self.sess.run(self.f_pred, tf_dict)\n",
    "        \n",
    "        return u_star, f_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6337859",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "\n",
      "It: 0, Loss: 3.437e-01, Lambda_1: 0.001, Lambda_2: 0.002479, Time: 0.89\n",
      "It: 10, Loss: 2.439e-01, Lambda_1: -0.008, Lambda_2: 0.002491, Time: 0.17\n",
      "It: 20, Loss: 2.252e-01, Lambda_1: -0.018, Lambda_2: 0.002517, Time: 0.15\n",
      "It: 30, Loss: 2.045e-01, Lambda_1: -0.027, Lambda_2: 0.002543, Time: 0.14\n",
      "It: 40, Loss: 1.636e-01, Lambda_1: -0.031, Lambda_2: 0.002550, Time: 0.15\n",
      "It: 50, Loss: 1.218e-01, Lambda_1: -0.026, Lambda_2: 0.002518, Time: 0.14\n",
      "It: 60, Loss: 8.982e-02, Lambda_1: -0.011, Lambda_2: 0.002477, Time: 0.15\n",
      "It: 70, Loss: 7.008e-02, Lambda_1: 0.007, Lambda_2: 0.002432, Time: 0.13\n",
      "It: 80, Loss: 5.735e-02, Lambda_1: 0.026, Lambda_2: 0.002385, Time: 0.13\n",
      "It: 90, Loss: 4.937e-02, Lambda_1: 0.045, Lambda_2: 0.002342, Time: 0.13\n",
      "It: 100, Loss: 4.547e-02, Lambda_1: 0.057, Lambda_2: 0.002311, Time: 0.13\n",
      "It: 110, Loss: 4.286e-02, Lambda_1: 0.062, Lambda_2: 0.002294, Time: 0.13\n",
      "It: 120, Loss: 4.163e-02, Lambda_1: 0.064, Lambda_2: 0.002287, Time: 0.13\n",
      "It: 130, Loss: 4.075e-02, Lambda_1: 0.067, Lambda_2: 0.002285, Time: 0.13\n",
      "It: 140, Loss: 4.005e-02, Lambda_1: 0.068, Lambda_2: 0.002286, Time: 0.13\n",
      "It: 150, Loss: 3.938e-02, Lambda_1: 0.068, Lambda_2: 0.002289, Time: 0.13\n",
      "It: 160, Loss: 3.874e-02, Lambda_1: 0.069, Lambda_2: 0.002292, Time: 0.13\n",
      "It: 170, Loss: 3.808e-02, Lambda_1: 0.071, Lambda_2: 0.002294, Time: 0.12\n",
      "It: 180, Loss: 3.740e-02, Lambda_1: 0.073, Lambda_2: 0.002297, Time: 0.13\n",
      "It: 190, Loss: 3.669e-02, Lambda_1: 0.075, Lambda_2: 0.002300, Time: 0.12\n",
      "It: 200, Loss: 3.597e-02, Lambda_1: 0.078, Lambda_2: 0.002302, Time: 0.12\n",
      "It: 210, Loss: 3.521e-02, Lambda_1: 0.081, Lambda_2: 0.002305, Time: 0.13\n",
      "It: 220, Loss: 3.441e-02, Lambda_1: 0.085, Lambda_2: 0.002307, Time: 0.14\n",
      "It: 230, Loss: 3.360e-02, Lambda_1: 0.089, Lambda_2: 0.002308, Time: 0.12\n",
      "It: 240, Loss: 3.279e-02, Lambda_1: 0.094, Lambda_2: 0.002308, Time: 0.13\n",
      "It: 250, Loss: 3.200e-02, Lambda_1: 0.100, Lambda_2: 0.002306, Time: 0.13\n",
      "It: 260, Loss: 3.127e-02, Lambda_1: 0.107, Lambda_2: 0.002301, Time: 0.13\n",
      "It: 270, Loss: 3.055e-02, Lambda_1: 0.114, Lambda_2: 0.002293, Time: 0.13\n",
      "It: 280, Loss: 2.983e-02, Lambda_1: 0.122, Lambda_2: 0.002282, Time: 0.13\n",
      "It: 290, Loss: 2.912e-02, Lambda_1: 0.130, Lambda_2: 0.002268, Time: 0.13\n",
      "It: 300, Loss: 2.850e-02, Lambda_1: 0.138, Lambda_2: 0.002253, Time: 0.13\n",
      "It: 310, Loss: 2.769e-02, Lambda_1: 0.145, Lambda_2: 0.002237, Time: 0.13\n",
      "It: 320, Loss: 2.699e-02, Lambda_1: 0.153, Lambda_2: 0.002221, Time: 0.13\n",
      "It: 330, Loss: 2.644e-02, Lambda_1: 0.161, Lambda_2: 0.002204, Time: 0.13\n",
      "It: 340, Loss: 2.548e-02, Lambda_1: 0.169, Lambda_2: 0.002188, Time: 0.14\n",
      "It: 350, Loss: 2.478e-02, Lambda_1: 0.177, Lambda_2: 0.002173, Time: 0.15\n",
      "It: 360, Loss: 2.437e-02, Lambda_1: 0.185, Lambda_2: 0.002158, Time: 0.15\n",
      "It: 370, Loss: 2.359e-02, Lambda_1: 0.193, Lambda_2: 0.002144, Time: 0.13\n",
      "It: 380, Loss: 2.283e-02, Lambda_1: 0.201, Lambda_2: 0.002132, Time: 0.14\n",
      "It: 390, Loss: 2.226e-02, Lambda_1: 0.209, Lambda_2: 0.002120, Time: 0.13\n",
      "It: 400, Loss: 2.176e-02, Lambda_1: 0.217, Lambda_2: 0.002109, Time: 0.14\n",
      "It: 410, Loss: 2.141e-02, Lambda_1: 0.225, Lambda_2: 0.002099, Time: 0.13\n",
      "It: 420, Loss: 2.183e-02, Lambda_1: 0.232, Lambda_2: 0.002089, Time: 0.14\n",
      "It: 430, Loss: 2.081e-02, Lambda_1: 0.239, Lambda_2: 0.002080, Time: 0.14\n",
      "It: 440, Loss: 2.019e-02, Lambda_1: 0.246, Lambda_2: 0.002073, Time: 0.14\n",
      "It: 450, Loss: 1.988e-02, Lambda_1: 0.253, Lambda_2: 0.002067, Time: 0.14\n",
      "It: 460, Loss: 1.950e-02, Lambda_1: 0.259, Lambda_2: 0.002063, Time: 0.14\n",
      "It: 470, Loss: 1.923e-02, Lambda_1: 0.266, Lambda_2: 0.002059, Time: 0.13\n",
      "It: 480, Loss: 1.926e-02, Lambda_1: 0.273, Lambda_2: 0.002057, Time: 0.14\n",
      "It: 490, Loss: 1.886e-02, Lambda_1: 0.279, Lambda_2: 0.002057, Time: 0.13\n",
      "It: 500, Loss: 1.844e-02, Lambda_1: 0.284, Lambda_2: 0.002056, Time: 0.14\n",
      "It: 510, Loss: 1.813e-02, Lambda_1: 0.290, Lambda_2: 0.002057, Time: 0.13\n",
      "It: 520, Loss: 1.814e-02, Lambda_1: 0.296, Lambda_2: 0.002060, Time: 0.14\n",
      "It: 530, Loss: 1.752e-02, Lambda_1: 0.301, Lambda_2: 0.002063, Time: 0.13\n",
      "It: 540, Loss: 1.970e-02, Lambda_1: 0.307, Lambda_2: 0.002068, Time: 0.14\n",
      "It: 550, Loss: 1.848e-02, Lambda_1: 0.311, Lambda_2: 0.002070, Time: 0.13\n",
      "It: 560, Loss: 1.708e-02, Lambda_1: 0.316, Lambda_2: 0.002074, Time: 0.14\n",
      "It: 570, Loss: 1.671e-02, Lambda_1: 0.320, Lambda_2: 0.002080, Time: 0.13\n",
      "It: 580, Loss: 1.651e-02, Lambda_1: 0.325, Lambda_2: 0.002087, Time: 0.14\n",
      "It: 590, Loss: 1.627e-02, Lambda_1: 0.330, Lambda_2: 0.002094, Time: 0.13\n",
      "It: 600, Loss: 1.603e-02, Lambda_1: 0.335, Lambda_2: 0.002103, Time: 0.14\n",
      "It: 610, Loss: 1.578e-02, Lambda_1: 0.341, Lambda_2: 0.002112, Time: 0.13\n",
      "It: 620, Loss: 1.557e-02, Lambda_1: 0.346, Lambda_2: 0.002122, Time: 0.14\n",
      "It: 630, Loss: 2.007e-02, Lambda_1: 0.351, Lambda_2: 0.002133, Time: 0.13\n",
      "It: 640, Loss: 1.523e-02, Lambda_1: 0.355, Lambda_2: 0.002139, Time: 0.14\n",
      "It: 650, Loss: 1.514e-02, Lambda_1: 0.359, Lambda_2: 0.002146, Time: 0.13\n",
      "It: 660, Loss: 1.500e-02, Lambda_1: 0.363, Lambda_2: 0.002154, Time: 0.14\n",
      "It: 670, Loss: 1.459e-02, Lambda_1: 0.368, Lambda_2: 0.002162, Time: 0.13\n",
      "It: 680, Loss: 1.438e-02, Lambda_1: 0.373, Lambda_2: 0.002171, Time: 0.14\n",
      "It: 690, Loss: 1.419e-02, Lambda_1: 0.378, Lambda_2: 0.002181, Time: 0.13\n",
      "It: 700, Loss: 1.707e-02, Lambda_1: 0.383, Lambda_2: 0.002190, Time: 0.14\n",
      "It: 710, Loss: 1.417e-02, Lambda_1: 0.387, Lambda_2: 0.002198, Time: 0.13\n",
      "It: 720, Loss: 1.382e-02, Lambda_1: 0.391, Lambda_2: 0.002204, Time: 0.14\n",
      "It: 730, Loss: 1.359e-02, Lambda_1: 0.395, Lambda_2: 0.002211, Time: 0.15\n",
      "It: 740, Loss: 1.334e-02, Lambda_1: 0.399, Lambda_2: 0.002218, Time: 0.13\n",
      "It: 750, Loss: 1.338e-02, Lambda_1: 0.403, Lambda_2: 0.002224, Time: 0.13\n",
      "It: 760, Loss: 1.360e-02, Lambda_1: 0.407, Lambda_2: 0.002231, Time: 0.13\n",
      "It: 770, Loss: 1.372e-02, Lambda_1: 0.411, Lambda_2: 0.002236, Time: 0.13\n",
      "It: 780, Loss: 1.312e-02, Lambda_1: 0.415, Lambda_2: 0.002242, Time: 0.13\n",
      "It: 790, Loss: 1.276e-02, Lambda_1: 0.418, Lambda_2: 0.002249, Time: 0.13\n",
      "It: 800, Loss: 1.259e-02, Lambda_1: 0.422, Lambda_2: 0.002255, Time: 0.13\n",
      "It: 810, Loss: 1.263e-02, Lambda_1: 0.426, Lambda_2: 0.002262, Time: 0.13\n",
      "It: 820, Loss: 1.286e-02, Lambda_1: 0.429, Lambda_2: 0.002268, Time: 0.13\n",
      "It: 830, Loss: 1.358e-02, Lambda_1: 0.432, Lambda_2: 0.002271, Time: 0.13\n",
      "It: 840, Loss: 1.230e-02, Lambda_1: 0.435, Lambda_2: 0.002277, Time: 0.13\n",
      "It: 850, Loss: 1.208e-02, Lambda_1: 0.438, Lambda_2: 0.002282, Time: 0.13\n",
      "It: 860, Loss: 1.195e-02, Lambda_1: 0.441, Lambda_2: 0.002287, Time: 0.13\n",
      "It: 870, Loss: 1.184e-02, Lambda_1: 0.444, Lambda_2: 0.002292, Time: 0.13\n",
      "It: 880, Loss: 1.174e-02, Lambda_1: 0.448, Lambda_2: 0.002298, Time: 0.14\n",
      "It: 890, Loss: 1.162e-02, Lambda_1: 0.452, Lambda_2: 0.002304, Time: 0.14\n",
      "It: 900, Loss: 1.214e-02, Lambda_1: 0.455, Lambda_2: 0.002309, Time: 0.13\n",
      "It: 910, Loss: 1.623e-02, Lambda_1: 0.457, Lambda_2: 0.002313, Time: 0.14\n",
      "It: 920, Loss: 2.304e-02, Lambda_1: 0.457, Lambda_2: 0.002312, Time: 0.15\n",
      "It: 930, Loss: 1.381e-02, Lambda_1: 0.456, Lambda_2: 0.002312, Time: 0.13\n",
      "It: 940, Loss: 1.204e-02, Lambda_1: 0.456, Lambda_2: 0.002314, Time: 0.15\n",
      "It: 950, Loss: 1.167e-02, Lambda_1: 0.456, Lambda_2: 0.002317, Time: 0.13\n",
      "It: 960, Loss: 1.144e-02, Lambda_1: 0.458, Lambda_2: 0.002320, Time: 0.14\n",
      "It: 970, Loss: 1.128e-02, Lambda_1: 0.461, Lambda_2: 0.002323, Time: 0.15\n",
      "It: 980, Loss: 1.117e-02, Lambda_1: 0.464, Lambda_2: 0.002327, Time: 0.14\n",
      "It: 990, Loss: 1.109e-02, Lambda_1: 0.467, Lambda_2: 0.002331, Time: 0.13\n",
      "Error u: 1.556091e-01\n",
      "Error l1: 53.07180%\n",
      "Error l2: 26.67672%\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\": \n",
    "     \n",
    "    nu = 0.01/np.pi\n",
    "\n",
    "    N_u = 2000\n",
    "    layers = [2, 20, 20, 20, 20, 20, 20, 20, 20, 1]\n",
    "    \n",
    "    data = scipy.io.loadmat(r'C:\\Users\\Omkar\\Downloads\\burgers_shock.mat')\n",
    "    \n",
    "    t = data['t'].flatten()[:,None]\n",
    "    x = data['x'].flatten()[:,None]\n",
    "    Exact = np.real(data['usol']).T\n",
    "    \n",
    "    X, T = np.meshgrid(x,t)\n",
    "    \n",
    "    X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
    "    u_star = Exact.flatten()[:,None]              \n",
    "\n",
    "    # Doman bounds\n",
    "    lb = X_star.min(0)\n",
    "    ub = X_star.max(0)    \n",
    "     \n",
    "    ######################################################################\n",
    "    ######################## Noiseles Data ###############################\n",
    "    ######################################################################\n",
    "    noise = 0.0            \n",
    "             \n",
    "    idx = np.random.choice(X_star.shape[0], N_u, replace=False)\n",
    "    X_u_train = X_star[idx,:]\n",
    "    u_train = u_star[idx,:]\n",
    "    \n",
    "    model = PhysicsInformedNN(X_u_train, u_train, layers, lb, ub)\n",
    "    model.train(1000)\n",
    "    \n",
    "    u_pred, f_pred = model.predict(X_star)\n",
    "            \n",
    "    error_u = np.linalg.norm(u_star-u_pred,2)/np.linalg.norm(u_star,2)\n",
    "    \n",
    "    U_pred = griddata(X_star, u_pred.flatten(), (X, T), method='cubic')\n",
    "        \n",
    "    lambda_1_value = model.sess.run(model.lambda_1)\n",
    "    lambda_2_value = model.sess.run(model.lambda_2)\n",
    "    lambda_2_value = np.exp(lambda_2_value)\n",
    "    \n",
    "    error_lambda_1 = np.abs(lambda_1_value - 1.0)*100\n",
    "    error_lambda_2 = np.abs(lambda_2_value - nu)/nu * 100\n",
    "    \n",
    "    print('Error u: %e' % (error_u))    \n",
    "    print('Error l1: %.5f%%' % (error_lambda_1))                             \n",
    "    print('Error l2: %.5f%%' % (error_lambda_2))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc37daa",
   "metadata": {},
   "source": [
    "# Complete disection and analysis of PINN code on burgers equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "719b5e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading and exploring data:\n",
    "data = scipy.io.loadmat(r'C:\\Users\\Omkar\\Downloads\\burgers_shock.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ae30bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = data['t'].flatten()[:,None]\n",
    "x = data['x'].flatten()[:,None]\n",
    "Exact = np.real(data['usol']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99c61d43",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  ],\n",
       "       [0.01],\n",
       "       [0.02],\n",
       "       [0.03],\n",
       "       [0.04],\n",
       "       [0.05],\n",
       "       [0.06],\n",
       "       [0.07],\n",
       "       [0.08],\n",
       "       [0.09],\n",
       "       [0.1 ],\n",
       "       [0.11],\n",
       "       [0.12],\n",
       "       [0.13],\n",
       "       [0.14],\n",
       "       [0.15],\n",
       "       [0.16],\n",
       "       [0.17],\n",
       "       [0.18],\n",
       "       [0.19],\n",
       "       [0.2 ],\n",
       "       [0.21],\n",
       "       [0.22],\n",
       "       [0.23],\n",
       "       [0.24],\n",
       "       [0.25],\n",
       "       [0.26],\n",
       "       [0.27],\n",
       "       [0.28],\n",
       "       [0.29],\n",
       "       [0.3 ],\n",
       "       [0.31],\n",
       "       [0.32],\n",
       "       [0.33],\n",
       "       [0.34],\n",
       "       [0.35],\n",
       "       [0.36],\n",
       "       [0.37],\n",
       "       [0.38],\n",
       "       [0.39],\n",
       "       [0.4 ],\n",
       "       [0.41],\n",
       "       [0.42],\n",
       "       [0.43],\n",
       "       [0.44],\n",
       "       [0.45],\n",
       "       [0.46],\n",
       "       [0.47],\n",
       "       [0.48],\n",
       "       [0.49],\n",
       "       [0.5 ],\n",
       "       [0.51],\n",
       "       [0.52],\n",
       "       [0.53],\n",
       "       [0.54],\n",
       "       [0.55],\n",
       "       [0.56],\n",
       "       [0.57],\n",
       "       [0.58],\n",
       "       [0.59],\n",
       "       [0.6 ],\n",
       "       [0.61],\n",
       "       [0.62],\n",
       "       [0.63],\n",
       "       [0.64],\n",
       "       [0.65],\n",
       "       [0.66],\n",
       "       [0.67],\n",
       "       [0.68],\n",
       "       [0.69],\n",
       "       [0.7 ],\n",
       "       [0.71],\n",
       "       [0.72],\n",
       "       [0.73],\n",
       "       [0.74],\n",
       "       [0.75],\n",
       "       [0.76],\n",
       "       [0.77],\n",
       "       [0.78],\n",
       "       [0.79],\n",
       "       [0.8 ],\n",
       "       [0.81],\n",
       "       [0.82],\n",
       "       [0.83],\n",
       "       [0.84],\n",
       "       [0.85],\n",
       "       [0.86],\n",
       "       [0.87],\n",
       "       [0.88],\n",
       "       [0.89],\n",
       "       [0.9 ],\n",
       "       [0.91],\n",
       "       [0.92],\n",
       "       [0.93],\n",
       "       [0.94],\n",
       "       [0.95],\n",
       "       [0.96],\n",
       "       [0.97],\n",
       "       [0.98],\n",
       "       [0.99]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# time data : 100 data points equally spaced last element is 0.99\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "229e6a5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#shape of time data\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9e843f6",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.        ],\n",
       "       [-0.99215686],\n",
       "       [-0.98431373],\n",
       "       [-0.97647059],\n",
       "       [-0.96862745],\n",
       "       [-0.96078431],\n",
       "       [-0.95294118],\n",
       "       [-0.94509804],\n",
       "       [-0.9372549 ],\n",
       "       [-0.92941176],\n",
       "       [-0.92156863],\n",
       "       [-0.91372549],\n",
       "       [-0.90588235],\n",
       "       [-0.89803922],\n",
       "       [-0.89019608],\n",
       "       [-0.88235294],\n",
       "       [-0.8745098 ],\n",
       "       [-0.86666667],\n",
       "       [-0.85882353],\n",
       "       [-0.85098039],\n",
       "       [-0.84313725],\n",
       "       [-0.83529412],\n",
       "       [-0.82745098],\n",
       "       [-0.81960784],\n",
       "       [-0.81176471],\n",
       "       [-0.80392157],\n",
       "       [-0.79607843],\n",
       "       [-0.78823529],\n",
       "       [-0.78039216],\n",
       "       [-0.77254902],\n",
       "       [-0.76470588],\n",
       "       [-0.75686275],\n",
       "       [-0.74901961],\n",
       "       [-0.74117647],\n",
       "       [-0.73333333],\n",
       "       [-0.7254902 ],\n",
       "       [-0.71764706],\n",
       "       [-0.70980392],\n",
       "       [-0.70196078],\n",
       "       [-0.69411765],\n",
       "       [-0.68627451],\n",
       "       [-0.67843137],\n",
       "       [-0.67058824],\n",
       "       [-0.6627451 ],\n",
       "       [-0.65490196],\n",
       "       [-0.64705882],\n",
       "       [-0.63921569],\n",
       "       [-0.63137255],\n",
       "       [-0.62352941],\n",
       "       [-0.61568627],\n",
       "       [-0.60784314],\n",
       "       [-0.6       ],\n",
       "       [-0.59215686],\n",
       "       [-0.58431373],\n",
       "       [-0.57647059],\n",
       "       [-0.56862745],\n",
       "       [-0.56078431],\n",
       "       [-0.55294118],\n",
       "       [-0.54509804],\n",
       "       [-0.5372549 ],\n",
       "       [-0.52941176],\n",
       "       [-0.52156863],\n",
       "       [-0.51372549],\n",
       "       [-0.50588235],\n",
       "       [-0.49803922],\n",
       "       [-0.49019608],\n",
       "       [-0.48235294],\n",
       "       [-0.4745098 ],\n",
       "       [-0.46666667],\n",
       "       [-0.45882353],\n",
       "       [-0.45098039],\n",
       "       [-0.44313725],\n",
       "       [-0.43529412],\n",
       "       [-0.42745098],\n",
       "       [-0.41960784],\n",
       "       [-0.41176471],\n",
       "       [-0.40392157],\n",
       "       [-0.39607843],\n",
       "       [-0.38823529],\n",
       "       [-0.38039216],\n",
       "       [-0.37254902],\n",
       "       [-0.36470588],\n",
       "       [-0.35686275],\n",
       "       [-0.34901961],\n",
       "       [-0.34117647],\n",
       "       [-0.33333333],\n",
       "       [-0.3254902 ],\n",
       "       [-0.31764706],\n",
       "       [-0.30980392],\n",
       "       [-0.30196078],\n",
       "       [-0.29411765],\n",
       "       [-0.28627451],\n",
       "       [-0.27843137],\n",
       "       [-0.27058824],\n",
       "       [-0.2627451 ],\n",
       "       [-0.25490196],\n",
       "       [-0.24705882],\n",
       "       [-0.23921569],\n",
       "       [-0.23137255],\n",
       "       [-0.22352941],\n",
       "       [-0.21568627],\n",
       "       [-0.20784314],\n",
       "       [-0.2       ],\n",
       "       [-0.19215686],\n",
       "       [-0.18431373],\n",
       "       [-0.17647059],\n",
       "       [-0.16862745],\n",
       "       [-0.16078431],\n",
       "       [-0.15294118],\n",
       "       [-0.14509804],\n",
       "       [-0.1372549 ],\n",
       "       [-0.12941176],\n",
       "       [-0.12156863],\n",
       "       [-0.11372549],\n",
       "       [-0.10588235],\n",
       "       [-0.09803922],\n",
       "       [-0.09019608],\n",
       "       [-0.08235294],\n",
       "       [-0.0745098 ],\n",
       "       [-0.06666667],\n",
       "       [-0.05882353],\n",
       "       [-0.05098039],\n",
       "       [-0.04313725],\n",
       "       [-0.03529412],\n",
       "       [-0.02745098],\n",
       "       [-0.01960784],\n",
       "       [-0.01176471],\n",
       "       [-0.00392157],\n",
       "       [ 0.00392157],\n",
       "       [ 0.01176471],\n",
       "       [ 0.01960784],\n",
       "       [ 0.02745098],\n",
       "       [ 0.03529412],\n",
       "       [ 0.04313725],\n",
       "       [ 0.05098039],\n",
       "       [ 0.05882353],\n",
       "       [ 0.06666667],\n",
       "       [ 0.0745098 ],\n",
       "       [ 0.08235294],\n",
       "       [ 0.09019608],\n",
       "       [ 0.09803922],\n",
       "       [ 0.10588235],\n",
       "       [ 0.11372549],\n",
       "       [ 0.12156863],\n",
       "       [ 0.12941176],\n",
       "       [ 0.1372549 ],\n",
       "       [ 0.14509804],\n",
       "       [ 0.15294118],\n",
       "       [ 0.16078431],\n",
       "       [ 0.16862745],\n",
       "       [ 0.17647059],\n",
       "       [ 0.18431373],\n",
       "       [ 0.19215686],\n",
       "       [ 0.2       ],\n",
       "       [ 0.20784314],\n",
       "       [ 0.21568627],\n",
       "       [ 0.22352941],\n",
       "       [ 0.23137255],\n",
       "       [ 0.23921569],\n",
       "       [ 0.24705882],\n",
       "       [ 0.25490196],\n",
       "       [ 0.2627451 ],\n",
       "       [ 0.27058824],\n",
       "       [ 0.27843137],\n",
       "       [ 0.28627451],\n",
       "       [ 0.29411765],\n",
       "       [ 0.30196078],\n",
       "       [ 0.30980392],\n",
       "       [ 0.31764706],\n",
       "       [ 0.3254902 ],\n",
       "       [ 0.33333333],\n",
       "       [ 0.34117647],\n",
       "       [ 0.34901961],\n",
       "       [ 0.35686275],\n",
       "       [ 0.36470588],\n",
       "       [ 0.37254902],\n",
       "       [ 0.38039216],\n",
       "       [ 0.38823529],\n",
       "       [ 0.39607843],\n",
       "       [ 0.40392157],\n",
       "       [ 0.41176471],\n",
       "       [ 0.41960784],\n",
       "       [ 0.42745098],\n",
       "       [ 0.43529412],\n",
       "       [ 0.44313725],\n",
       "       [ 0.45098039],\n",
       "       [ 0.45882353],\n",
       "       [ 0.46666667],\n",
       "       [ 0.4745098 ],\n",
       "       [ 0.48235294],\n",
       "       [ 0.49019608],\n",
       "       [ 0.49803922],\n",
       "       [ 0.50588235],\n",
       "       [ 0.51372549],\n",
       "       [ 0.52156863],\n",
       "       [ 0.52941176],\n",
       "       [ 0.5372549 ],\n",
       "       [ 0.54509804],\n",
       "       [ 0.55294118],\n",
       "       [ 0.56078431],\n",
       "       [ 0.56862745],\n",
       "       [ 0.57647059],\n",
       "       [ 0.58431373],\n",
       "       [ 0.59215686],\n",
       "       [ 0.6       ],\n",
       "       [ 0.60784314],\n",
       "       [ 0.61568627],\n",
       "       [ 0.62352941],\n",
       "       [ 0.63137255],\n",
       "       [ 0.63921569],\n",
       "       [ 0.64705882],\n",
       "       [ 0.65490196],\n",
       "       [ 0.6627451 ],\n",
       "       [ 0.67058824],\n",
       "       [ 0.67843137],\n",
       "       [ 0.68627451],\n",
       "       [ 0.69411765],\n",
       "       [ 0.70196078],\n",
       "       [ 0.70980392],\n",
       "       [ 0.71764706],\n",
       "       [ 0.7254902 ],\n",
       "       [ 0.73333333],\n",
       "       [ 0.74117647],\n",
       "       [ 0.74901961],\n",
       "       [ 0.75686275],\n",
       "       [ 0.76470588],\n",
       "       [ 0.77254902],\n",
       "       [ 0.78039216],\n",
       "       [ 0.78823529],\n",
       "       [ 0.79607843],\n",
       "       [ 0.80392157],\n",
       "       [ 0.81176471],\n",
       "       [ 0.81960784],\n",
       "       [ 0.82745098],\n",
       "       [ 0.83529412],\n",
       "       [ 0.84313725],\n",
       "       [ 0.85098039],\n",
       "       [ 0.85882353],\n",
       "       [ 0.86666667],\n",
       "       [ 0.8745098 ],\n",
       "       [ 0.88235294],\n",
       "       [ 0.89019608],\n",
       "       [ 0.89803922],\n",
       "       [ 0.90588235],\n",
       "       [ 0.91372549],\n",
       "       [ 0.92156863],\n",
       "       [ 0.92941176],\n",
       "       [ 0.9372549 ],\n",
       "       [ 0.94509804],\n",
       "       [ 0.95294118],\n",
       "       [ 0.96078431],\n",
       "       [ 0.96862745],\n",
       "       [ 0.97647059],\n",
       "       [ 0.98431373],\n",
       "       [ 0.99215686],\n",
       "       [ 1.        ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# position data : 256 data points equally spaced between -1 to 1\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c268675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#shape of x\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d604ba6e",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.22464680e-16,  2.46374492e-02,  4.92599411e-02, ...,\n",
       "        -4.92599411e-02, -2.46374492e-02, -1.22464680e-16],\n",
       "       [ 2.95362215e-17,  2.38801772e-02,  4.77471404e-02, ...,\n",
       "        -4.77471404e-02, -2.38801772e-02, -1.08544418e-16],\n",
       "       [ 1.08470836e-16,  2.31684474e-02,  4.63251758e-02, ...,\n",
       "        -4.63251758e-02, -2.31684474e-02, -1.29376253e-16],\n",
       "       ...,\n",
       "       [ 3.57297976e-16,  6.07646192e-03,  1.21528662e-02, ...,\n",
       "        -1.21528662e-02, -6.07646192e-03,  1.04877526e-16],\n",
       "       [ 2.61833228e-16,  6.02971754e-03,  1.20593792e-02, ...,\n",
       "        -1.20593792e-02, -6.02971754e-03,  2.17508692e-16],\n",
       "       [ 9.39536897e-17,  5.98368729e-03,  1.19673204e-02, ...,\n",
       "        -1.19673204e-02, -5.98368729e-03,  1.12388795e-16]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#solution data\n",
    "Exact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f3af1c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 256)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Shape of solution data u(t,x)\n",
    "Exact.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fe9584e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#discovering the function\n",
    "X, T = np.meshgrid(x,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1d6bca69",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.        , -0.99215686, -0.98431373, ...,  0.98431373,\n",
       "         0.99215686,  1.        ],\n",
       "       [-1.        , -0.99215686, -0.98431373, ...,  0.98431373,\n",
       "         0.99215686,  1.        ],\n",
       "       [-1.        , -0.99215686, -0.98431373, ...,  0.98431373,\n",
       "         0.99215686,  1.        ],\n",
       "       ...,\n",
       "       [-1.        , -0.99215686, -0.98431373, ...,  0.98431373,\n",
       "         0.99215686,  1.        ],\n",
       "       [-1.        , -0.99215686, -0.98431373, ...,  0.98431373,\n",
       "         0.99215686,  1.        ],\n",
       "       [-1.        , -0.99215686, -0.98431373, ...,  0.98431373,\n",
       "         0.99215686,  1.        ]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X has a shape of (100,256) and all rows are the same that is  256 x- values -1 to 1 written 100 times \n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f6ba14fe",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , ..., 0.  , 0.  , 0.  ],\n",
       "       [0.01, 0.01, 0.01, ..., 0.01, 0.01, 0.01],\n",
       "       [0.02, 0.02, 0.02, ..., 0.02, 0.02, 0.02],\n",
       "       ...,\n",
       "       [0.97, 0.97, 0.97, ..., 0.97, 0.97, 0.97],\n",
       "       [0.98, 0.98, 0.98, ..., 0.98, 0.98, 0.98],\n",
       "       [0.99, 0.99, 0.99, ..., 0.99, 0.99, 0.99]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this has shape (100,256) aswell but as we keep traversing the rows the value increments. So T[0] is all 0s 256 times,\n",
    "#T[1] is 0.01 written 256 times and so on\n",
    "T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ead9c5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#making of X_star an u_star\n",
    "X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
    "u_star = Exact.flatten()[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4c9928de",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.        ,  0.        ],\n",
       "       [-0.99215686,  0.        ],\n",
       "       [-0.98431373,  0.        ],\n",
       "       ...,\n",
       "       [ 0.98431373,  0.99      ],\n",
       "       [ 0.99215686,  0.99      ],\n",
       "       [ 1.        ,  0.99      ]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#X_star has shape (25600,2) so (256*100,2), the first row is [-1,0] second is [-0.99,0] and this goes on till u have\n",
    "# [1,0] - the 255th index then at 256 index the x resets to -1 and t becomes 0.01 and this is the matrix so on \n",
    "X_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2ed9f138",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.22464680e-16],\n",
       "       [ 2.46374492e-02],\n",
       "       [ 4.92599411e-02],\n",
       "       ...,\n",
       "       [-1.19673204e-02],\n",
       "       [-5.98368729e-03],\n",
       "       [ 1.12388795e-16]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#shape of (25600,1) this gives the corresponding solution (u) values for the above X_star. So \n",
    "#the 0th index is the solution at [-1,0] written in (x,t) order and the solution is u[0] Note all of these till now \n",
    "# are nparrays\n",
    "u_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c47045f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain bounds\n",
    "lb = X_star.min(0).astype('float32') # [-1,0]\n",
    "ub = X_star.max(0).astype('float32') # [1,0.99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9dde62e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#making of training data \n",
    "N_u = 2000 # no. of training samples from data\n",
    "idx = np.random.choice(X_star.shape[0], N_u, replace=False) # gives random index array\n",
    "X_u_train = X_star[idx,:] # shape (N_u,2) in the format (x,t)\n",
    "u_train = u_star[idx,:] # shape (2000,1) the solution to those 2000 values of x,t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f1342ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_u_train=X_u_train.astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f63bfc",
   "metadata": {},
   "source": [
    "## Understanding whats inside the PINN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b05a2226",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = X_u_train[:,0:1] #- takes the X column from train data and outputs (2000,1) shape\n",
    "t = X_u_train[:,1:2] #- takes the t column from train data and outputs (2000,1) shape\n",
    "u = u_train # - takes the u from train data and outputs (2000,1) shape\n",
    "# self.layers = layers # gets the layers list (NOT AN NP ARRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bda3430",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3a9c701a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#creating the Session and its variables n losses\n",
    "sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True,\n",
    "                                                     log_device_placement=True))\n",
    "        \n",
    "# Initialize parameters\n",
    "lambda_1 = tf.Variable([0.0], dtype=tf.float32)\n",
    "lambda_2 = tf.Variable([-6.0], dtype=tf.float32)\n",
    "\n",
    "x_tf = tf.placeholder(tf.float32, shape=[None,1])\n",
    "t_tf = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "u_tf = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "u_pred = net_u(x_tf, t_tf)\n",
    "f_pred = net_f(x_tf, t_tf)\n",
    "\n",
    "loss = tf.reduce_mean(tf.square(u_tf - u_pred)) + \\\n",
    "            tf.reduce_mean(tf.square(f_pred))\n",
    "\n",
    "optimizer = tf.contrib.opt.ScipyOptimizerInterface(loss, \n",
    "                                                        method = 'L-BFGS-B', \n",
    "                                                        options = {'maxiter': 50000,\n",
    "                                                                   'maxfun': 50000,\n",
    "                                                                   'maxcor': 50,\n",
    "                                                                   'maxls': 50,\n",
    "                                                                   'ftol' : 1.0 * np.finfo(float).eps})\n",
    "\n",
    "optimizer_Adam = tf.train.AdamOptimizer()\n",
    "train_op_Adam = optimizer_Adam.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37188cd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf69f89f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e2c9a501",
   "metadata": {},
   "outputs": [],
   "source": [
    "#size is a list \n",
    "def xavier_init(size):\n",
    "        in_dim = size[0]\n",
    "        out_dim = size[1]        \n",
    "        xavier_stddev = np.sqrt(2/(in_dim + out_dim))\n",
    "        return tf.Variable(tf.truncated_normal([in_dim, out_dim], stddev=xavier_stddev), dtype=tf.float32) \n",
    "                #returns a tf.Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3d948ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example\n",
    "a=xavier_init([1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dab03879",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-a7ebc0fc9784>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0minit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# actually initialize all the variables\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m#this will show the actual numbers output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "#running the code\n",
    "init = tf.global_variables_initializer() # prepare an init node\n",
    "with tf.Session() as sess:\n",
    "    init.run() # actually initialize all the variables\n",
    "    result = a.eval()\n",
    "    print(result)\n",
    "    #this will show the actual numbers output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475c4f6d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ef2c4b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this returns the neural network weights and biases. NOTE both weights and biases are lists\n",
    "def initialize_NN(layers):        \n",
    "        weights = []\n",
    "        biases = []\n",
    "        num_layers = len(layers) \n",
    "        for l in range(0,num_layers-1):\n",
    "            W = xavier_init(size=[layers[l], layers[l+1]])\n",
    "            b = tf.Variable(tf.zeros([1,layers[l+1]], dtype=tf.float32), dtype=tf.float32)\n",
    "            weights.append(W)\n",
    "            biases.append(b)        \n",
    "        return weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79dbeaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example code to run initializeNN\n",
    "# layers = [1,2,1]\n",
    "layers=[2,20,20,20,20,20,20,20,20,1]\n",
    "w , b= initialize_NN(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b8648759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'Variable_14:0' shape=(1, 2) dtype=float32_ref>, <tf.Variable 'Variable_16:0' shape=(2, 1) dtype=float32_ref>]\n",
      "[<tf.Variable 'Variable_15:0' shape=(1, 2) dtype=float32_ref>, <tf.Variable 'Variable_17:0' shape=(1, 1) dtype=float32_ref>]\n"
     ]
    }
   ],
   "source": [
    "#running the code\n",
    "print(w) # this is a list of tf.Variables\n",
    "print(b) # this is a list of tf.Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b795466",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.03307999  0.00552576 -0.14778398  0.1631586  -0.08150904 -0.15934165\n",
      "   0.07438523  0.45788297 -0.22241917  0.02204545 -0.28173542 -0.26593822\n",
      "  -0.17192322  0.23722447  0.54474473 -0.5335447  -0.04868652 -0.11398798\n",
      "  -0.06433471  0.06431513]\n",
      " [-0.03353772  0.12033412 -0.09941009  0.00816181 -0.30426386  0.02664434\n",
      "   0.0694659  -0.33017966 -0.08288395 -0.21282418  0.21034858  0.13623093\n",
      "  -0.22780806 -0.47640064 -0.11849657 -0.37377158 -0.24949227  0.13145612\n",
      "  -0.16492261 -0.10654896]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#running the code\n",
    "init = tf.global_variables_initializer() # prepare an init node\n",
    "with tf.Session() as sess:\n",
    "    init.run() # actually initialize all the variables\n",
    "    W1 = w[0].eval() # this will output the weights of the first element in the weights list \n",
    "    B1= b[0].eval() # this will output the bias of the first element in the bias list \n",
    "    print(W1)\n",
    "    print(B1)\n",
    "    #this will show the actual numbers output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575d9aec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "db93bced",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_net(X, weights, biases):\n",
    "        num_layers = len(weights) + 1\n",
    "        \n",
    "        H = 2.0*(X - lb)/(ub - lb) - 1.0  # this normalizes the input values of X\n",
    "        H=tf.cast(H , dtype=tf.float32)  #type casts cuz x might be float64\n",
    "        for l in range(0,num_layers-2):\n",
    "            W = weights[l]\n",
    "            b = biases[l]\n",
    "#             H = tf.math.sigmoid(tf.add(tf.matmul(H, W), b))\n",
    "            H = tf.tanh(tf.add(tf.matmul(H, W), b))\n",
    "        W = weights[-1]\n",
    "        b = biases[-1]\n",
    "        Y = tf.add(tf.matmul(H, W), b)\n",
    "        return Y # this is the final output vector [N_u,1]that the neural network outputs after\n",
    "                 #complete forward propogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a75ca5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3323b0ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "78241ef6",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.]\n",
      " [2.]\n",
      " [3.]]\n"
     ]
    }
   ],
   "source": [
    "#running the code\n",
    "init = tf.global_variables_initializer() # prepare an init node\n",
    "with tf.Session() as sess:\n",
    "    init.run() # actually initialize all the variables\n",
    "    result = arr.eval()\n",
    "    print(result)\n",
    "    #this will show the actual numbers output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3e5fee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "981d5b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#making vanilla network\n",
    "def net_u(x,t):  \n",
    "        u = neural_net(tf.concat([x,t],1),w,b) # these weights and Biases are the dummy ones i made on top\n",
    "        return u # this basically returns the forward prop values. All we are doing is creating a NN called u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779f9722",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3b562916",
   "metadata": {},
   "outputs": [],
   "source": [
    "def net_f(x, t):\n",
    "        lambda_1_ = lambda_1        \n",
    "        lambda_2_ = tf.exp(lambda_2)\n",
    "        u = net_u(x,t)\n",
    "        u_t = tf.gradients(u, t)[0]\n",
    "        u_x = tf.gradients(u, x)[0]\n",
    "        u_xx = tf.gradients(u_x, x)[0]\n",
    "        f = u_t + lambda_1_*u*u_x - tf.exp(lambda_2_)*u_xx\n",
    "        \n",
    "        return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87d1dd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b475e893",
   "metadata": {},
   "outputs": [],
   "source": [
    "def callback(loss, lambda_1, lambda_2):\n",
    "        print('Loss: %e, l1: %.5f, l2: %.5f' % (loss, lambda_1, np.exp(lambda_2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bae41aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "08b11f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(nIter):\n",
    "        tf_dict = {x_tf: X_u_train[:,0:1], t_tf: X_u_train[:,1:2], u_tf: u_train}\n",
    "        \n",
    "        start_time = time.time()\n",
    "        for it in range(nIter):\n",
    "            sess.run(train_op_Adam, tf_dict)\n",
    "            \n",
    "            # Print\n",
    "            if it % 10 == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                loss_value = sess.run(loss, tf_dict)\n",
    "                lambda_1_value = sess.run(lambda_1)\n",
    "                lambda_2_value = np.exp(sess.run(lambda_2))\n",
    "                print('It: %d, Loss: %.3e, Lambda_1: %.3f, Lambda_2: %.6f, Time: %.2f' % \n",
    "                      (it, loss_value, lambda_1_value, lambda_2_value, elapsed))\n",
    "                start_time = time.time()\n",
    "        \n",
    "        optimizer.minimize(sess,\n",
    "                                feed_dict = tf_dict,\n",
    "                                fetches = [loss, lambda_1, lambda_2],\n",
    "                                loss_callback = callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efde2e2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9575b07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62330fd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714ec4a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e64a2e01",
   "metadata": {},
   "source": [
    "# Building and testing PINN for Rod Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e08ad7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b402aabc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79ab48f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#making of Rod data Set\n",
    "dataframe1 = pd.read_csv(r\"C:\\Users\\Omkar\\Downloads\\Rod data.csv\")\n",
    "x_values=dataframe1.loc[:,\"Length [mm]\"] \n",
    "x_values=np.array(x_values)\n",
    "x_values=x_values.reshape(257,1)\n",
    "T_values=dataframe1.loc[:,\"Value [K]\"] \n",
    "T_values=np.array(T_values)\n",
    "T_values=T_values.reshape(257,1)\n",
    "x_star=x_values.astype('float32')\n",
    "x_tensor=tf.Variable(x_star, dtype = tf.float32)\n",
    "T_star=T_values.astype('float32')\n",
    "T_tensor=tf.Variable(T_star, dtype = tf.float32)\n",
    "lb = np.array([0.]).astype('float32')\n",
    "ub = np.array([0.3]).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab40ec39",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1800.  ],\n",
       "       [1794.1 ],\n",
       "       [1788.3 ],\n",
       "       [1782.4 ],\n",
       "       [1776.6 ],\n",
       "       [1770.7 ],\n",
       "       [1764.8 ],\n",
       "       [1759.  ],\n",
       "       [1753.1 ],\n",
       "       [1747.3 ],\n",
       "       [1741.4 ],\n",
       "       [1735.5 ],\n",
       "       [1729.7 ],\n",
       "       [1723.8 ],\n",
       "       [1718.  ],\n",
       "       [1712.1 ],\n",
       "       [1706.2 ],\n",
       "       [1700.4 ],\n",
       "       [1694.5 ],\n",
       "       [1688.7 ],\n",
       "       [1682.8 ],\n",
       "       [1677.  ],\n",
       "       [1671.1 ],\n",
       "       [1665.2 ],\n",
       "       [1659.4 ],\n",
       "       [1653.5 ],\n",
       "       [1647.7 ],\n",
       "       [1641.8 ],\n",
       "       [1635.9 ],\n",
       "       [1630.1 ],\n",
       "       [1624.2 ],\n",
       "       [1618.4 ],\n",
       "       [1612.5 ],\n",
       "       [1606.6 ],\n",
       "       [1600.8 ],\n",
       "       [1594.9 ],\n",
       "       [1589.1 ],\n",
       "       [1583.2 ],\n",
       "       [1577.3 ],\n",
       "       [1571.5 ],\n",
       "       [1565.6 ],\n",
       "       [1559.8 ],\n",
       "       [1553.9 ],\n",
       "       [1548.  ],\n",
       "       [1542.2 ],\n",
       "       [1536.3 ],\n",
       "       [1530.5 ],\n",
       "       [1524.6 ],\n",
       "       [1518.7 ],\n",
       "       [1512.9 ],\n",
       "       [1507.  ],\n",
       "       [1501.2 ],\n",
       "       [1495.3 ],\n",
       "       [1489.5 ],\n",
       "       [1483.6 ],\n",
       "       [1477.7 ],\n",
       "       [1471.9 ],\n",
       "       [1466.  ],\n",
       "       [1460.2 ],\n",
       "       [1454.3 ],\n",
       "       [1448.4 ],\n",
       "       [1442.6 ],\n",
       "       [1436.7 ],\n",
       "       [1430.9 ],\n",
       "       [1425.  ],\n",
       "       [1419.1 ],\n",
       "       [1413.3 ],\n",
       "       [1407.4 ],\n",
       "       [1401.6 ],\n",
       "       [1395.7 ],\n",
       "       [1389.8 ],\n",
       "       [1384.  ],\n",
       "       [1378.1 ],\n",
       "       [1372.3 ],\n",
       "       [1366.4 ],\n",
       "       [1360.5 ],\n",
       "       [1354.7 ],\n",
       "       [1348.8 ],\n",
       "       [1343.  ],\n",
       "       [1337.1 ],\n",
       "       [1331.2 ],\n",
       "       [1325.4 ],\n",
       "       [1319.5 ],\n",
       "       [1313.7 ],\n",
       "       [1307.8 ],\n",
       "       [1302.  ],\n",
       "       [1296.1 ],\n",
       "       [1290.2 ],\n",
       "       [1284.4 ],\n",
       "       [1278.5 ],\n",
       "       [1272.7 ],\n",
       "       [1266.8 ],\n",
       "       [1260.9 ],\n",
       "       [1255.1 ],\n",
       "       [1249.2 ],\n",
       "       [1243.4 ],\n",
       "       [1237.5 ],\n",
       "       [1231.6 ],\n",
       "       [1225.8 ],\n",
       "       [1219.9 ],\n",
       "       [1214.1 ],\n",
       "       [1208.2 ],\n",
       "       [1202.3 ],\n",
       "       [1196.5 ],\n",
       "       [1190.6 ],\n",
       "       [1184.8 ],\n",
       "       [1178.9 ],\n",
       "       [1173.  ],\n",
       "       [1167.2 ],\n",
       "       [1161.3 ],\n",
       "       [1155.5 ],\n",
       "       [1149.6 ],\n",
       "       [1143.7 ],\n",
       "       [1137.9 ],\n",
       "       [1132.  ],\n",
       "       [1126.2 ],\n",
       "       [1120.3 ],\n",
       "       [1114.5 ],\n",
       "       [1108.6 ],\n",
       "       [1102.7 ],\n",
       "       [1096.9 ],\n",
       "       [1091.  ],\n",
       "       [1085.2 ],\n",
       "       [1079.3 ],\n",
       "       [1073.4 ],\n",
       "       [1067.6 ],\n",
       "       [1061.7 ],\n",
       "       [1055.9 ],\n",
       "       [1050.  ],\n",
       "       [1044.1 ],\n",
       "       [1038.3 ],\n",
       "       [1032.4 ],\n",
       "       [1026.6 ],\n",
       "       [1020.7 ],\n",
       "       [1014.8 ],\n",
       "       [1009.  ],\n",
       "       [1003.1 ],\n",
       "       [ 997.27],\n",
       "       [ 991.41],\n",
       "       [ 985.55],\n",
       "       [ 979.69],\n",
       "       [ 973.83],\n",
       "       [ 967.97],\n",
       "       [ 962.11],\n",
       "       [ 956.25],\n",
       "       [ 950.39],\n",
       "       [ 944.53],\n",
       "       [ 938.67],\n",
       "       [ 932.81],\n",
       "       [ 926.95],\n",
       "       [ 921.09],\n",
       "       [ 915.23],\n",
       "       [ 909.37],\n",
       "       [ 903.52],\n",
       "       [ 897.66],\n",
       "       [ 891.8 ],\n",
       "       [ 885.94],\n",
       "       [ 880.08],\n",
       "       [ 874.22],\n",
       "       [ 868.36],\n",
       "       [ 862.5 ],\n",
       "       [ 856.64],\n",
       "       [ 850.78],\n",
       "       [ 844.92],\n",
       "       [ 839.06],\n",
       "       [ 833.2 ],\n",
       "       [ 827.34],\n",
       "       [ 821.48],\n",
       "       [ 815.62],\n",
       "       [ 809.77],\n",
       "       [ 803.91],\n",
       "       [ 798.05],\n",
       "       [ 792.19],\n",
       "       [ 786.33],\n",
       "       [ 780.47],\n",
       "       [ 774.61],\n",
       "       [ 768.75],\n",
       "       [ 762.89],\n",
       "       [ 757.03],\n",
       "       [ 751.17],\n",
       "       [ 745.31],\n",
       "       [ 739.45],\n",
       "       [ 733.59],\n",
       "       [ 727.73],\n",
       "       [ 721.88],\n",
       "       [ 716.02],\n",
       "       [ 710.16],\n",
       "       [ 704.3 ],\n",
       "       [ 698.44],\n",
       "       [ 692.58],\n",
       "       [ 686.72],\n",
       "       [ 680.86],\n",
       "       [ 675.  ],\n",
       "       [ 669.14],\n",
       "       [ 663.28],\n",
       "       [ 657.42],\n",
       "       [ 651.56],\n",
       "       [ 645.7 ],\n",
       "       [ 639.84],\n",
       "       [ 633.98],\n",
       "       [ 628.13],\n",
       "       [ 622.27],\n",
       "       [ 616.41],\n",
       "       [ 610.55],\n",
       "       [ 604.69],\n",
       "       [ 598.83],\n",
       "       [ 592.97],\n",
       "       [ 587.11],\n",
       "       [ 581.25],\n",
       "       [ 575.39],\n",
       "       [ 569.53],\n",
       "       [ 563.67],\n",
       "       [ 557.81],\n",
       "       [ 551.95],\n",
       "       [ 546.09],\n",
       "       [ 540.23],\n",
       "       [ 534.38],\n",
       "       [ 528.52],\n",
       "       [ 522.66],\n",
       "       [ 516.8 ],\n",
       "       [ 510.94],\n",
       "       [ 505.08],\n",
       "       [ 499.22],\n",
       "       [ 493.36],\n",
       "       [ 487.5 ],\n",
       "       [ 481.64],\n",
       "       [ 475.78],\n",
       "       [ 469.92],\n",
       "       [ 464.06],\n",
       "       [ 458.2 ],\n",
       "       [ 452.34],\n",
       "       [ 446.48],\n",
       "       [ 440.63],\n",
       "       [ 434.77],\n",
       "       [ 428.91],\n",
       "       [ 423.05],\n",
       "       [ 417.19],\n",
       "       [ 411.33],\n",
       "       [ 405.47],\n",
       "       [ 399.61],\n",
       "       [ 393.75],\n",
       "       [ 387.89],\n",
       "       [ 382.03],\n",
       "       [ 376.17],\n",
       "       [ 370.31],\n",
       "       [ 364.45],\n",
       "       [ 358.59],\n",
       "       [ 352.73],\n",
       "       [ 346.87],\n",
       "       [ 341.02],\n",
       "       [ 335.16],\n",
       "       [ 329.3 ],\n",
       "       [ 323.44],\n",
       "       [ 317.58],\n",
       "       [ 311.72],\n",
       "       [ 305.86],\n",
       "       [ 300.  ]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bc0238",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the Session and its variables n losses\n",
    "sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True,\n",
    "                                                     log_device_placement=True))\n",
    "        \n",
    "# Initialize parameters\n",
    "lambda_1 = tf.Variable([0.0], dtype=tf.float32) #Making lambda 1 value \n",
    "\n",
    "x_tf = tf.placeholder(tf.float32, shape=[None,1])\n",
    "u_tf = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "u_pred = net_u(x_tf)\n",
    "f_pred = net_f(x_tf)\n",
    "\n",
    "loss = tf.reduce_mean(tf.square(u_tf - u_pred)) + tf.reduce_mean(tf.square(f_pred))\n",
    "\n",
    "optimizer = tf.contrib.opt.ScipyOptimizerInterface(loss, \n",
    "                                                        method = 'L-BFGS-B', \n",
    "                                                        options = {'maxiter': 50000,\n",
    "                                                                   'maxfun': 50000,\n",
    "                                                                   'maxcor': 50,\n",
    "                                                                   'maxls': 50,\n",
    "                                                                   'ftol' : 1.0 * np.finfo(float).eps})\n",
    "\n",
    "optimizer_Adam = tf.train.AdamOptimizer()\n",
    "train_op_Adam = optimizer_Adam.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72db1717",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427098e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#size is a list \n",
    "def xavier_init(size):\n",
    "        in_dim = size[0]\n",
    "        out_dim = size[1]        \n",
    "        xavier_stddev = np.sqrt(2/(in_dim + out_dim))\n",
    "        return tf.Variable(tf.truncated_normal([in_dim, out_dim], stddev=xavier_stddev), dtype=tf.float32) \n",
    "                #returns a tf.Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b77501f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521fa48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this returns the neural network weights and biases. NOTE both weights and biases are lists\n",
    "def initialize_NN(layers):        \n",
    "        weights = []\n",
    "        biases = []\n",
    "        num_layers = len(layers) \n",
    "        for l in range(0,num_layers-1):\n",
    "            W = xavier_init(size=[layers[l], layers[l+1]])\n",
    "            b = tf.Variable(tf.zeros([1,layers[l+1]], dtype=tf.float32), dtype=tf.float32)\n",
    "            weights.append(W)\n",
    "            biases.append(b)        \n",
    "        return weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb631f73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3c2b14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is an integral step and will be explained in detail\n",
    "def neural_net(X, weights, biases):\n",
    "        num_layers = len(weights) + 1\n",
    "        \n",
    "#         H = 2.0*(X - lb)/(ub - lb) - 1.0\n",
    "        H = (X - lb)/(ub - lb) # this normalizes the input values of X\n",
    "#         H=H.astype('float32')  #type casts cuz x might be float64\n",
    "        for l in range(0,num_layers-2):\n",
    "            W = weights[l]\n",
    "            b = biases[l]\n",
    "            H = tf.math.sigmoid(tf.add(tf.matmul(H, W), b))\n",
    "#             H = tf.tanh(tf.add(tf.matmul(H, W), b))\n",
    "        W = weights[-1]\n",
    "        b = biases[-1]\n",
    "        Y = tf.add(tf.matmul(H, W), b)\n",
    "        return Y # this is the final output vector [N_u,1]that the neural network outputs after\n",
    "                 #complete forward propogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80b969e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7146703",
   "metadata": {},
   "outputs": [],
   "source": [
    "#making vanilla network\n",
    "def net_u(x):  \n",
    "        u = neural_net(tf.concat([x],1),w,b) # these weights and Biases are the dummy ones i made on top\n",
    "        return u # this basically returns the forward prop values. All we are doing is creating a NN called u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe457da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec2e74e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#making PINN\n",
    "#set lambda_1\n",
    "# lambda_1=tf.Variable([0.0], dtype=tf.float32)\n",
    "def net_f(x):\n",
    "#         lambda_1 = self.lambda_1        \n",
    "#         lambda_2 = tf.exp(self.lambda_2)\n",
    "        u = net_u(x)\n",
    "#         u_t = tf.gradients(u, t)[0]\n",
    "        u_x = tf.gradients(u, x)[0]\n",
    "        u_xx = tf.gradients(u_x, x)[0]\n",
    "        f = -1*lambda_1*u_xx\n",
    "        \n",
    "        return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29273896",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ac285f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is only for one dependent variable x\n",
    "#lets take random X of shape (3,1) for simplicity with data 1 2 3 column vector\n",
    "# X=np.array([1.0,2.0,3.0]).reshape(3,1)\n",
    "# arr = X.astype('float32')\n",
    "arr=tf.Variable([[1.0],[2.0],[3.0]],dtype=tf.float32,shape=(3,1)) #making it a tensor\n",
    "#we will take an assumed set of weights and biases as well for a small layers size of [1,2,1]\n",
    "var1 =tf.Variable([[1.11,-0.98]], shape=(1,2), dtype=tf.float32)\n",
    "var2=tf.Variable([[0.31],[ 0.68]] , shape=(2,1),dtype=tf.float32)\n",
    "b1=tf.Variable([[0.0,0.0]], shape=(1,2),dtype=tf.float32)\n",
    "b2=tf.Variable([[0.0]], shape=(1,1), dtype=tf.float32)\n",
    "W=[var1 , var2]\n",
    "B=[b1 , b2]\n",
    "lb=np.array([1.0],dtype='float32')\n",
    "ub=np.array([3.0],dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f38d1529",
   "metadata": {},
   "outputs": [],
   "source": [
    "def callback( loss, lambda_1):\n",
    "        print('Loss: %e, l1: %.5f' % (loss, lambda_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5225cb06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21a8dc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#understanding training of network:\n",
    "def train(nIter):\n",
    "        tf_dict = {x_tf: x_star, u_tf: scaled_T}\n",
    "        \n",
    "        start_time = time.time()\n",
    "        for it in range(nIter):\n",
    "            sess.run(train_op_Adam, tf_dict)\n",
    "            \n",
    "            # Print\n",
    "            if it % 10 == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                loss_value = sess.run(loss, tf_dict)\n",
    "                lambda_1_value = sess.run(lambda_1)\n",
    "#                 lambda_2_value = np.exp(self.sess.run(self.lambda_2))\n",
    "                print('It: %d, Loss: %.3e, Lambda_1: %.3f, Time: %.2f' % \n",
    "                      (it, loss_value, lambda_1_value, elapsed))\n",
    "                start_time = time.time()\n",
    "        \n",
    "        optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b14bd521",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It: 0, Loss: 2.231e-01, Lambda_1: 10.000, Time: 0.76\n",
      "It: 10, Loss: 9.612e-02, Lambda_1: 10.000, Time: 0.05\n",
      "It: 20, Loss: 8.704e-02, Lambda_1: 9.998, Time: 0.03\n",
      "It: 30, Loss: 8.835e-02, Lambda_1: 9.996, Time: 0.03\n",
      "It: 40, Loss: 8.401e-02, Lambda_1: 9.993, Time: 0.03\n",
      "It: 50, Loss: 8.450e-02, Lambda_1: 9.990, Time: 0.03\n",
      "It: 60, Loss: 8.403e-02, Lambda_1: 9.986, Time: 0.03\n",
      "It: 70, Loss: 8.403e-02, Lambda_1: 9.981, Time: 0.04\n",
      "It: 80, Loss: 8.399e-02, Lambda_1: 9.975, Time: 0.04\n",
      "It: 90, Loss: 8.398e-02, Lambda_1: 9.967, Time: 0.03\n",
      "It: 100, Loss: 8.397e-02, Lambda_1: 9.958, Time: 0.03\n",
      "It: 110, Loss: 8.397e-02, Lambda_1: 9.948, Time: 0.03\n",
      "It: 120, Loss: 8.397e-02, Lambda_1: 9.936, Time: 0.03\n",
      "It: 130, Loss: 8.397e-02, Lambda_1: 9.924, Time: 0.03\n",
      "It: 140, Loss: 8.397e-02, Lambda_1: 9.910, Time: 0.03\n",
      "It: 150, Loss: 8.397e-02, Lambda_1: 9.895, Time: 0.04\n",
      "It: 160, Loss: 8.396e-02, Lambda_1: 9.879, Time: 0.04\n",
      "It: 170, Loss: 8.396e-02, Lambda_1: 9.862, Time: 0.03\n",
      "It: 180, Loss: 8.396e-02, Lambda_1: 9.844, Time: 0.04\n",
      "It: 190, Loss: 8.396e-02, Lambda_1: 9.825, Time: 0.03\n",
      "It: 200, Loss: 8.396e-02, Lambda_1: 9.806, Time: 0.04\n",
      "It: 210, Loss: 8.396e-02, Lambda_1: 9.786, Time: 0.04\n",
      "It: 220, Loss: 8.395e-02, Lambda_1: 9.765, Time: 0.03\n",
      "It: 230, Loss: 8.395e-02, Lambda_1: 9.745, Time: 0.03\n",
      "It: 240, Loss: 8.395e-02, Lambda_1: 9.723, Time: 0.03\n",
      "It: 250, Loss: 8.395e-02, Lambda_1: 9.702, Time: 0.03\n",
      "It: 260, Loss: 8.394e-02, Lambda_1: 9.681, Time: 0.03\n",
      "It: 270, Loss: 8.394e-02, Lambda_1: 9.659, Time: 0.03\n",
      "It: 280, Loss: 8.394e-02, Lambda_1: 9.637, Time: 0.04\n",
      "It: 290, Loss: 8.393e-02, Lambda_1: 9.616, Time: 0.03\n",
      "It: 300, Loss: 8.393e-02, Lambda_1: 9.594, Time: 0.03\n",
      "It: 310, Loss: 8.393e-02, Lambda_1: 9.573, Time: 0.03\n",
      "It: 320, Loss: 8.392e-02, Lambda_1: 9.552, Time: 0.03\n",
      "It: 330, Loss: 8.392e-02, Lambda_1: 9.531, Time: 0.03\n",
      "It: 340, Loss: 8.391e-02, Lambda_1: 9.510, Time: 0.03\n",
      "It: 350, Loss: 8.390e-02, Lambda_1: 9.489, Time: 0.03\n",
      "It: 360, Loss: 8.390e-02, Lambda_1: 9.469, Time: 0.03\n",
      "It: 370, Loss: 8.389e-02, Lambda_1: 9.448, Time: 0.03\n",
      "It: 380, Loss: 8.388e-02, Lambda_1: 9.427, Time: 0.03\n",
      "It: 390, Loss: 8.387e-02, Lambda_1: 9.405, Time: 0.03\n",
      "It: 400, Loss: 8.386e-02, Lambda_1: 9.381, Time: 0.03\n",
      "It: 410, Loss: 8.385e-02, Lambda_1: 9.355, Time: 0.03\n",
      "It: 420, Loss: 8.384e-02, Lambda_1: 9.328, Time: 0.03\n",
      "It: 430, Loss: 8.383e-02, Lambda_1: 9.299, Time: 0.04\n",
      "It: 440, Loss: 8.382e-02, Lambda_1: 9.270, Time: 0.04\n",
      "It: 450, Loss: 8.380e-02, Lambda_1: 9.241, Time: 0.04\n",
      "It: 460, Loss: 8.379e-02, Lambda_1: 9.213, Time: 0.04\n",
      "It: 470, Loss: 8.378e-02, Lambda_1: 9.187, Time: 0.04\n",
      "It: 480, Loss: 8.377e-02, Lambda_1: 9.161, Time: 0.04\n",
      "It: 490, Loss: 8.375e-02, Lambda_1: 9.136, Time: 0.04\n",
      "It: 500, Loss: 8.374e-02, Lambda_1: 9.111, Time: 0.03\n",
      "It: 510, Loss: 8.372e-02, Lambda_1: 9.086, Time: 0.03\n",
      "It: 520, Loss: 8.371e-02, Lambda_1: 9.062, Time: 0.04\n",
      "It: 530, Loss: 8.369e-02, Lambda_1: 9.038, Time: 0.04\n",
      "It: 540, Loss: 8.367e-02, Lambda_1: 9.014, Time: 0.04\n",
      "It: 550, Loss: 8.365e-02, Lambda_1: 8.990, Time: 0.04\n",
      "It: 560, Loss: 8.363e-02, Lambda_1: 8.967, Time: 0.04\n",
      "It: 570, Loss: 8.361e-02, Lambda_1: 8.943, Time: 0.04\n",
      "It: 580, Loss: 8.359e-02, Lambda_1: 8.919, Time: 0.03\n",
      "It: 590, Loss: 8.357e-02, Lambda_1: 8.896, Time: 0.04\n",
      "It: 600, Loss: 8.354e-02, Lambda_1: 8.873, Time: 0.04\n",
      "It: 610, Loss: 8.352e-02, Lambda_1: 8.849, Time: 0.04\n",
      "It: 620, Loss: 8.349e-02, Lambda_1: 8.826, Time: 0.04\n",
      "It: 630, Loss: 8.346e-02, Lambda_1: 8.802, Time: 0.04\n",
      "It: 640, Loss: 8.343e-02, Lambda_1: 8.779, Time: 0.04\n",
      "It: 650, Loss: 8.339e-02, Lambda_1: 8.756, Time: 0.04\n",
      "It: 660, Loss: 8.336e-02, Lambda_1: 8.732, Time: 0.03\n",
      "It: 670, Loss: 8.332e-02, Lambda_1: 8.709, Time: 0.04\n",
      "It: 680, Loss: 8.327e-02, Lambda_1: 8.685, Time: 0.04\n",
      "It: 690, Loss: 8.322e-02, Lambda_1: 8.661, Time: 0.04\n",
      "It: 700, Loss: 8.316e-02, Lambda_1: 8.638, Time: 0.04\n",
      "It: 710, Loss: 8.309e-02, Lambda_1: 8.613, Time: 0.04\n",
      "It: 720, Loss: 8.297e-02, Lambda_1: 8.589, Time: 0.04\n",
      "It: 730, Loss: 8.277e-02, Lambda_1: 8.566, Time: 0.04\n",
      "It: 740, Loss: 8.238e-02, Lambda_1: 8.547, Time: 0.04\n",
      "It: 750, Loss: 8.177e-02, Lambda_1: 8.536, Time: 0.03\n",
      "It: 760, Loss: 8.109e-02, Lambda_1: 8.529, Time: 0.04\n",
      "It: 770, Loss: 8.045e-02, Lambda_1: 8.513, Time: 0.04\n",
      "It: 780, Loss: 7.985e-02, Lambda_1: 8.494, Time: 0.04\n",
      "It: 790, Loss: 7.915e-02, Lambda_1: 8.472, Time: 0.04\n",
      "It: 800, Loss: 7.835e-02, Lambda_1: 8.446, Time: 0.04\n",
      "It: 810, Loss: 7.790e-02, Lambda_1: 8.410, Time: 0.04\n",
      "It: 820, Loss: 7.705e-02, Lambda_1: 8.368, Time: 0.04\n",
      "It: 830, Loss: 7.612e-02, Lambda_1: 8.329, Time: 0.04\n",
      "It: 840, Loss: 7.533e-02, Lambda_1: 8.290, Time: 0.04\n",
      "It: 850, Loss: 7.419e-02, Lambda_1: 8.252, Time: 0.04\n",
      "It: 860, Loss: 7.530e-02, Lambda_1: 8.210, Time: 0.04\n",
      "It: 870, Loss: 7.271e-02, Lambda_1: 8.169, Time: 0.04\n",
      "It: 880, Loss: 7.208e-02, Lambda_1: 8.135, Time: 0.04\n",
      "It: 890, Loss: 7.093e-02, Lambda_1: 8.107, Time: 0.04\n",
      "It: 900, Loss: 6.980e-02, Lambda_1: 8.080, Time: 0.04\n",
      "It: 910, Loss: 6.912e-02, Lambda_1: 8.052, Time: 0.04\n",
      "It: 920, Loss: 6.760e-02, Lambda_1: 8.019, Time: 0.04\n",
      "It: 930, Loss: 6.722e-02, Lambda_1: 7.987, Time: 0.04\n",
      "It: 940, Loss: 6.570e-02, Lambda_1: 7.957, Time: 0.04\n",
      "It: 950, Loss: 6.446e-02, Lambda_1: 7.928, Time: 0.03\n",
      "It: 960, Loss: 6.883e-02, Lambda_1: 7.901, Time: 0.04\n",
      "It: 970, Loss: 6.571e-02, Lambda_1: 7.862, Time: 0.04\n",
      "It: 980, Loss: 6.192e-02, Lambda_1: 7.829, Time: 0.03\n",
      "It: 990, Loss: 6.151e-02, Lambda_1: 7.804, Time: 0.04\n",
      "It: 1000, Loss: 6.024e-02, Lambda_1: 7.784, Time: 0.04\n",
      "It: 1010, Loss: 5.921e-02, Lambda_1: 7.766, Time: 0.03\n",
      "It: 1020, Loss: 5.811e-02, Lambda_1: 7.749, Time: 0.03\n",
      "It: 1030, Loss: 5.698e-02, Lambda_1: 7.731, Time: 0.03\n",
      "It: 1040, Loss: 6.107e-02, Lambda_1: 7.712, Time: 0.03\n",
      "It: 1050, Loss: 5.819e-02, Lambda_1: 7.682, Time: 0.03\n",
      "It: 1060, Loss: 5.468e-02, Lambda_1: 7.655, Time: 0.03\n",
      "It: 1070, Loss: 5.361e-02, Lambda_1: 7.633, Time: 0.04\n",
      "It: 1080, Loss: 5.290e-02, Lambda_1: 7.614, Time: 0.03\n",
      "It: 1090, Loss: 5.181e-02, Lambda_1: 7.597, Time: 0.03\n",
      "It: 1100, Loss: 5.100e-02, Lambda_1: 7.581, Time: 0.03\n",
      "It: 1110, Loss: 4.979e-02, Lambda_1: 7.559, Time: 0.03\n",
      "It: 1120, Loss: 5.117e-02, Lambda_1: 7.533, Time: 0.03\n",
      "It: 1130, Loss: 4.883e-02, Lambda_1: 7.512, Time: 0.03\n",
      "It: 1140, Loss: 4.764e-02, Lambda_1: 7.493, Time: 0.03\n",
      "It: 1150, Loss: 4.669e-02, Lambda_1: 7.477, Time: 0.04\n",
      "It: 1160, Loss: 4.937e-02, Lambda_1: 7.461, Time: 0.03\n",
      "It: 1170, Loss: 4.827e-02, Lambda_1: 7.438, Time: 0.03\n",
      "It: 1180, Loss: 4.552e-02, Lambda_1: 7.417, Time: 0.03\n",
      "It: 1190, Loss: 4.376e-02, Lambda_1: 7.399, Time: 0.03\n",
      "It: 1200, Loss: 4.420e-02, Lambda_1: 7.383, Time: 0.04\n",
      "It: 1210, Loss: 4.287e-02, Lambda_1: 7.364, Time: 0.03\n",
      "It: 1220, Loss: 4.355e-02, Lambda_1: 7.345, Time: 0.04\n",
      "It: 1230, Loss: 4.126e-02, Lambda_1: 7.327, Time: 0.03\n",
      "It: 1240, Loss: 4.019e-02, Lambda_1: 7.307, Time: 0.03\n",
      "It: 1250, Loss: 4.134e-02, Lambda_1: 7.285, Time: 0.03\n",
      "It: 1260, Loss: 3.907e-02, Lambda_1: 7.267, Time: 0.04\n",
      "It: 1270, Loss: 3.855e-02, Lambda_1: 7.252, Time: 0.03\n",
      "It: 1280, Loss: 3.795e-02, Lambda_1: 7.237, Time: 0.03\n",
      "It: 1290, Loss: 4.008e-02, Lambda_1: 7.219, Time: 0.03\n",
      "It: 1300, Loss: 3.756e-02, Lambda_1: 7.195, Time: 0.03\n",
      "It: 1310, Loss: 3.586e-02, Lambda_1: 7.177, Time: 0.03\n",
      "It: 1320, Loss: 3.538e-02, Lambda_1: 7.162, Time: 0.04\n",
      "It: 1330, Loss: 3.434e-02, Lambda_1: 7.148, Time: 0.03\n",
      "It: 1340, Loss: 3.395e-02, Lambda_1: 7.136, Time: 0.04\n",
      "It: 1350, Loss: 4.789e-02, Lambda_1: 7.123, Time: 0.03\n",
      "It: 1360, Loss: 3.446e-02, Lambda_1: 7.097, Time: 0.03\n",
      "It: 1370, Loss: 3.477e-02, Lambda_1: 7.076, Time: 0.03\n",
      "It: 1380, Loss: 3.193e-02, Lambda_1: 7.060, Time: 0.04\n",
      "It: 1390, Loss: 3.122e-02, Lambda_1: 7.048, Time: 0.03\n",
      "It: 1400, Loss: 3.078e-02, Lambda_1: 7.036, Time: 0.03\n",
      "It: 1410, Loss: 3.009e-02, Lambda_1: 7.026, Time: 0.03\n",
      "It: 1420, Loss: 2.981e-02, Lambda_1: 7.015, Time: 0.04\n",
      "It: 1430, Loss: 5.269e-02, Lambda_1: 7.002, Time: 0.03\n",
      "It: 1440, Loss: 3.559e-02, Lambda_1: 6.977, Time: 0.04\n",
      "It: 1450, Loss: 2.864e-02, Lambda_1: 6.957, Time: 0.04\n",
      "It: 1460, Loss: 2.819e-02, Lambda_1: 6.943, Time: 0.04\n",
      "It: 1470, Loss: 2.765e-02, Lambda_1: 6.931, Time: 0.03\n",
      "It: 1480, Loss: 2.692e-02, Lambda_1: 6.921, Time: 0.03\n",
      "It: 1490, Loss: 2.648e-02, Lambda_1: 6.912, Time: 0.03\n",
      "It: 1500, Loss: 2.596e-02, Lambda_1: 6.902, Time: 0.03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It: 1510, Loss: 2.740e-02, Lambda_1: 6.893, Time: 0.04\n",
      "It: 1520, Loss: 3.173e-02, Lambda_1: 6.873, Time: 0.04\n",
      "It: 1530, Loss: 3.010e-02, Lambda_1: 6.850, Time: 0.04\n",
      "It: 1540, Loss: 2.448e-02, Lambda_1: 6.834, Time: 0.03\n",
      "It: 1550, Loss: 2.462e-02, Lambda_1: 6.821, Time: 0.03\n",
      "It: 1560, Loss: 2.393e-02, Lambda_1: 6.812, Time: 0.04\n",
      "It: 1570, Loss: 2.333e-02, Lambda_1: 6.803, Time: 0.04\n",
      "It: 1580, Loss: 2.279e-02, Lambda_1: 6.794, Time: 0.03\n",
      "It: 1590, Loss: 2.236e-02, Lambda_1: 6.786, Time: 0.04\n",
      "It: 1600, Loss: 2.199e-02, Lambda_1: 6.778, Time: 0.03\n",
      "It: 1610, Loss: 2.799e-02, Lambda_1: 6.769, Time: 0.03\n",
      "It: 1620, Loss: 3.532e-02, Lambda_1: 6.747, Time: 0.03\n",
      "It: 1630, Loss: 2.158e-02, Lambda_1: 6.724, Time: 0.03\n",
      "It: 1640, Loss: 2.209e-02, Lambda_1: 6.708, Time: 0.03\n",
      "It: 1650, Loss: 2.074e-02, Lambda_1: 6.697, Time: 0.03\n",
      "It: 1660, Loss: 2.037e-02, Lambda_1: 6.689, Time: 0.03\n",
      "It: 1670, Loss: 1.981e-02, Lambda_1: 6.681, Time: 0.03\n",
      "It: 1680, Loss: 1.941e-02, Lambda_1: 6.674, Time: 0.03\n",
      "It: 1690, Loss: 1.901e-02, Lambda_1: 6.667, Time: 0.03\n",
      "It: 1700, Loss: 1.865e-02, Lambda_1: 6.659, Time: 0.03\n",
      "It: 1710, Loss: 1.828e-02, Lambda_1: 6.652, Time: 0.03\n",
      "It: 1720, Loss: 1.799e-02, Lambda_1: 6.645, Time: 0.03\n",
      "It: 1730, Loss: 4.860e-02, Lambda_1: 6.636, Time: 0.03\n",
      "It: 1740, Loss: 2.913e-02, Lambda_1: 6.609, Time: 0.03\n",
      "It: 1750, Loss: 1.773e-02, Lambda_1: 6.586, Time: 0.03\n",
      "It: 1760, Loss: 1.721e-02, Lambda_1: 6.572, Time: 0.03\n",
      "It: 1770, Loss: 1.717e-02, Lambda_1: 6.563, Time: 0.03\n",
      "It: 1780, Loss: 1.687e-02, Lambda_1: 6.555, Time: 0.03\n",
      "It: 1790, Loss: 1.629e-02, Lambda_1: 6.549, Time: 0.03\n",
      "It: 1800, Loss: 1.602e-02, Lambda_1: 6.543, Time: 0.03\n",
      "It: 1810, Loss: 1.570e-02, Lambda_1: 6.537, Time: 0.04\n",
      "It: 1820, Loss: 1.539e-02, Lambda_1: 6.531, Time: 0.03\n",
      "It: 1830, Loss: 1.509e-02, Lambda_1: 6.525, Time: 0.03\n",
      "It: 1840, Loss: 1.480e-02, Lambda_1: 6.519, Time: 0.03\n",
      "It: 1850, Loss: 1.452e-02, Lambda_1: 6.513, Time: 0.03\n",
      "It: 1860, Loss: 1.423e-02, Lambda_1: 6.507, Time: 0.03\n",
      "It: 1870, Loss: 1.396e-02, Lambda_1: 6.501, Time: 0.04\n",
      "It: 1880, Loss: 1.845e-02, Lambda_1: 6.495, Time: 0.03\n",
      "It: 1890, Loss: 4.556e-02, Lambda_1: 6.472, Time: 0.03\n",
      "It: 1900, Loss: 2.250e-02, Lambda_1: 6.446, Time: 0.03\n",
      "It: 1910, Loss: 1.688e-02, Lambda_1: 6.429, Time: 0.04\n",
      "It: 1920, Loss: 1.454e-02, Lambda_1: 6.419, Time: 0.04\n",
      "It: 1930, Loss: 1.352e-02, Lambda_1: 6.412, Time: 0.03\n",
      "It: 1940, Loss: 1.298e-02, Lambda_1: 6.407, Time: 0.04\n",
      "It: 1950, Loss: 1.260e-02, Lambda_1: 6.402, Time: 0.04\n",
      "It: 1960, Loss: 1.236e-02, Lambda_1: 6.397, Time: 0.03\n",
      "It: 1970, Loss: 1.212e-02, Lambda_1: 6.393, Time: 0.03\n",
      "It: 1980, Loss: 1.189e-02, Lambda_1: 6.388, Time: 0.03\n",
      "It: 1990, Loss: 1.166e-02, Lambda_1: 6.384, Time: 0.03\n",
      "It: 2000, Loss: 1.144e-02, Lambda_1: 6.379, Time: 0.04\n",
      "It: 2010, Loss: 1.122e-02, Lambda_1: 6.374, Time: 0.03\n",
      "It: 2020, Loss: 1.100e-02, Lambda_1: 6.370, Time: 0.04\n",
      "It: 2030, Loss: 1.079e-02, Lambda_1: 6.365, Time: 0.03\n",
      "It: 2040, Loss: 1.058e-02, Lambda_1: 6.360, Time: 0.03\n",
      "It: 2050, Loss: 1.038e-02, Lambda_1: 6.355, Time: 0.03\n",
      "It: 2060, Loss: 1.018e-02, Lambda_1: 6.350, Time: 0.04\n",
      "It: 2070, Loss: 1.030e-02, Lambda_1: 6.345, Time: 0.03\n",
      "It: 2080, Loss: 6.161e-02, Lambda_1: 6.334, Time: 0.03\n",
      "It: 2090, Loss: 1.861e-02, Lambda_1: 6.304, Time: 0.04\n",
      "It: 2100, Loss: 1.371e-02, Lambda_1: 6.284, Time: 0.03\n",
      "It: 2110, Loss: 1.186e-02, Lambda_1: 6.273, Time: 0.04\n",
      "It: 2120, Loss: 1.038e-02, Lambda_1: 6.266, Time: 0.04\n",
      "It: 2130, Loss: 9.718e-03, Lambda_1: 6.261, Time: 0.04\n",
      "It: 2140, Loss: 9.392e-03, Lambda_1: 6.257, Time: 0.03\n",
      "It: 2150, Loss: 9.147e-03, Lambda_1: 6.253, Time: 0.03\n",
      "It: 2160, Loss: 8.943e-03, Lambda_1: 6.250, Time: 0.03\n",
      "It: 2170, Loss: 8.775e-03, Lambda_1: 6.246, Time: 0.03\n",
      "It: 2180, Loss: 8.603e-03, Lambda_1: 6.243, Time: 0.03\n",
      "It: 2190, Loss: 8.439e-03, Lambda_1: 6.239, Time: 0.04\n",
      "It: 2200, Loss: 8.278e-03, Lambda_1: 6.235, Time: 0.03\n",
      "It: 2210, Loss: 8.120e-03, Lambda_1: 6.232, Time: 0.03\n",
      "It: 2220, Loss: 7.965e-03, Lambda_1: 6.228, Time: 0.03\n",
      "It: 2230, Loss: 7.814e-03, Lambda_1: 6.225, Time: 0.03\n",
      "It: 2240, Loss: 7.666e-03, Lambda_1: 6.221, Time: 0.04\n",
      "It: 2250, Loss: 7.520e-03, Lambda_1: 6.217, Time: 0.03\n",
      "It: 2260, Loss: 7.378e-03, Lambda_1: 6.213, Time: 0.03\n",
      "It: 2270, Loss: 7.238e-03, Lambda_1: 6.210, Time: 0.03\n",
      "It: 2280, Loss: 7.101e-03, Lambda_1: 6.206, Time: 0.03\n",
      "It: 2290, Loss: 6.966e-03, Lambda_1: 6.202, Time: 0.03\n",
      "It: 2300, Loss: 6.841e-03, Lambda_1: 6.198, Time: 0.03\n",
      "It: 2310, Loss: 1.502e-02, Lambda_1: 6.194, Time: 0.04\n",
      "It: 2320, Loss: 3.597e-02, Lambda_1: 6.171, Time: 0.03\n",
      "It: 2330, Loss: 1.844e-02, Lambda_1: 6.146, Time: 0.03\n",
      "It: 2340, Loss: 8.248e-03, Lambda_1: 6.130, Time: 0.03\n",
      "It: 2350, Loss: 6.782e-03, Lambda_1: 6.122, Time: 0.03\n",
      "It: 2360, Loss: 6.913e-03, Lambda_1: 6.117, Time: 0.03\n",
      "It: 2370, Loss: 6.667e-03, Lambda_1: 6.113, Time: 0.03\n",
      "It: 2380, Loss: 6.434e-03, Lambda_1: 6.110, Time: 0.03\n",
      "It: 2390, Loss: 6.266e-03, Lambda_1: 6.107, Time: 0.03\n",
      "It: 2400, Loss: 6.127e-03, Lambda_1: 6.105, Time: 0.03\n",
      "It: 2410, Loss: 6.004e-03, Lambda_1: 6.102, Time: 0.03\n",
      "It: 2420, Loss: 5.890e-03, Lambda_1: 6.100, Time: 0.04\n",
      "It: 2430, Loss: 5.778e-03, Lambda_1: 6.097, Time: 0.03\n",
      "It: 2440, Loss: 5.668e-03, Lambda_1: 6.094, Time: 0.03\n",
      "It: 2450, Loss: 5.561e-03, Lambda_1: 6.092, Time: 0.03\n",
      "It: 2460, Loss: 5.457e-03, Lambda_1: 6.089, Time: 0.03\n",
      "It: 2470, Loss: 5.355e-03, Lambda_1: 6.086, Time: 0.03\n",
      "It: 2480, Loss: 5.256e-03, Lambda_1: 6.084, Time: 0.03\n",
      "It: 2490, Loss: 5.159e-03, Lambda_1: 6.081, Time: 0.03\n",
      "It: 2500, Loss: 5.064e-03, Lambda_1: 6.078, Time: 0.03\n",
      "It: 2510, Loss: 4.970e-03, Lambda_1: 6.075, Time: 0.04\n",
      "It: 2520, Loss: 4.879e-03, Lambda_1: 6.073, Time: 0.04\n",
      "It: 2530, Loss: 4.790e-03, Lambda_1: 6.070, Time: 0.03\n",
      "It: 2540, Loss: 4.703e-03, Lambda_1: 6.067, Time: 0.04\n",
      "It: 2550, Loss: 4.617e-03, Lambda_1: 6.064, Time: 0.03\n",
      "It: 2560, Loss: 4.533e-03, Lambda_1: 6.062, Time: 0.03\n",
      "It: 2570, Loss: 4.451e-03, Lambda_1: 6.059, Time: 0.03\n",
      "It: 2580, Loss: 4.375e-03, Lambda_1: 6.056, Time: 0.03\n",
      "It: 2590, Loss: 5.879e-03, Lambda_1: 6.053, Time: 0.03\n",
      "It: 2600, Loss: 5.847e-03, Lambda_1: 6.040, Time: 0.03\n",
      "It: 2610, Loss: 1.536e-02, Lambda_1: 6.017, Time: 0.03\n",
      "It: 2620, Loss: 4.973e-03, Lambda_1: 6.003, Time: 0.03\n",
      "It: 2630, Loss: 4.298e-03, Lambda_1: 5.994, Time: 0.04\n",
      "It: 2640, Loss: 4.411e-03, Lambda_1: 5.990, Time: 0.04\n",
      "It: 2650, Loss: 4.331e-03, Lambda_1: 5.987, Time: 0.03\n",
      "It: 2660, Loss: 4.088e-03, Lambda_1: 5.984, Time: 0.04\n",
      "It: 2670, Loss: 3.986e-03, Lambda_1: 5.982, Time: 0.05\n",
      "It: 2680, Loss: 3.911e-03, Lambda_1: 5.980, Time: 0.04\n",
      "It: 2690, Loss: 3.833e-03, Lambda_1: 5.978, Time: 0.04\n",
      "It: 2700, Loss: 3.759e-03, Lambda_1: 5.975, Time: 0.03\n",
      "It: 2710, Loss: 3.690e-03, Lambda_1: 5.973, Time: 0.03\n",
      "It: 2720, Loss: 3.622e-03, Lambda_1: 5.971, Time: 0.03\n",
      "It: 2730, Loss: 3.556e-03, Lambda_1: 5.969, Time: 0.03\n",
      "It: 2740, Loss: 3.491e-03, Lambda_1: 5.967, Time: 0.03\n",
      "It: 2750, Loss: 3.428e-03, Lambda_1: 5.965, Time: 0.03\n",
      "It: 2760, Loss: 3.367e-03, Lambda_1: 5.963, Time: 0.03\n",
      "It: 2770, Loss: 3.307e-03, Lambda_1: 5.961, Time: 0.03\n",
      "It: 2780, Loss: 3.248e-03, Lambda_1: 5.958, Time: 0.03\n",
      "It: 2790, Loss: 3.191e-03, Lambda_1: 5.956, Time: 0.03\n",
      "It: 2800, Loss: 3.135e-03, Lambda_1: 5.954, Time: 0.03\n",
      "It: 2810, Loss: 3.080e-03, Lambda_1: 5.952, Time: 0.03\n",
      "It: 2820, Loss: 3.026e-03, Lambda_1: 5.950, Time: 0.03\n",
      "It: 2830, Loss: 2.974e-03, Lambda_1: 5.948, Time: 0.03\n",
      "It: 2840, Loss: 2.922e-03, Lambda_1: 5.945, Time: 0.03\n",
      "It: 2850, Loss: 2.875e-03, Lambda_1: 5.943, Time: 0.03\n",
      "It: 2860, Loss: 3.917e-03, Lambda_1: 5.941, Time: 0.03\n",
      "It: 2870, Loss: 2.440e-02, Lambda_1: 5.930, Time: 0.03\n",
      "It: 2880, Loss: 3.542e-03, Lambda_1: 5.907, Time: 0.04\n",
      "It: 2890, Loss: 5.220e-03, Lambda_1: 5.893, Time: 0.03\n",
      "It: 2900, Loss: 4.480e-03, Lambda_1: 5.885, Time: 0.03\n",
      "It: 2910, Loss: 3.193e-03, Lambda_1: 5.881, Time: 0.04\n",
      "It: 2920, Loss: 2.749e-03, Lambda_1: 5.878, Time: 0.03\n",
      "It: 2930, Loss: 2.757e-03, Lambda_1: 5.876, Time: 0.03\n",
      "It: 2940, Loss: 2.658e-03, Lambda_1: 5.874, Time: 0.03\n",
      "It: 2950, Loss: 2.604e-03, Lambda_1: 5.872, Time: 0.03\n",
      "It: 2960, Loss: 2.550e-03, Lambda_1: 5.871, Time: 0.03\n",
      "It: 2970, Loss: 2.504e-03, Lambda_1: 5.869, Time: 0.03\n",
      "It: 2980, Loss: 2.457e-03, Lambda_1: 5.867, Time: 0.03\n",
      "It: 2990, Loss: 2.412e-03, Lambda_1: 5.866, Time: 0.03\n"
     ]
    }
   ],
   "source": [
    "train(3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "81957a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "mini=tf.train.GradientDescentOptimizer(0.001).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "c1cd96e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=tf.global_variables_initializer()\n",
    "s=tf.Session()\n",
    "s.run(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd508e4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f59e289b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "model1 = scaler.fit(x_star)\n",
    "scaled_x = model1.transform(x_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9de4173",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = scaler.fit(T_star)\n",
    "scaled_T = model2.transform(T_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "82daf9a0",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        ],\n",
       "       [0.99606667],\n",
       "       [0.9922    ],\n",
       "       [0.98826667],\n",
       "       [0.9844    ],\n",
       "       [0.98046667],\n",
       "       [0.97653333],\n",
       "       [0.97266667],\n",
       "       [0.96873333],\n",
       "       [0.96486667],\n",
       "       [0.96093333],\n",
       "       [0.957     ],\n",
       "       [0.95313333],\n",
       "       [0.9492    ],\n",
       "       [0.94533333],\n",
       "       [0.9414    ],\n",
       "       [0.93746667],\n",
       "       [0.9336    ],\n",
       "       [0.92966667],\n",
       "       [0.9258    ],\n",
       "       [0.92186667],\n",
       "       [0.918     ],\n",
       "       [0.91406667],\n",
       "       [0.91013333],\n",
       "       [0.90626667],\n",
       "       [0.90233333],\n",
       "       [0.89846667],\n",
       "       [0.89453333],\n",
       "       [0.8906    ],\n",
       "       [0.88673333],\n",
       "       [0.8828    ],\n",
       "       [0.87893333],\n",
       "       [0.875     ],\n",
       "       [0.87106667],\n",
       "       [0.8672    ],\n",
       "       [0.86326667],\n",
       "       [0.8594    ],\n",
       "       [0.85546667],\n",
       "       [0.85153333],\n",
       "       [0.84766667],\n",
       "       [0.84373333],\n",
       "       [0.83986667],\n",
       "       [0.83593333],\n",
       "       [0.832     ],\n",
       "       [0.82813333],\n",
       "       [0.8242    ],\n",
       "       [0.82033333],\n",
       "       [0.8164    ],\n",
       "       [0.81246667],\n",
       "       [0.8086    ],\n",
       "       [0.80466667],\n",
       "       [0.8008    ],\n",
       "       [0.79686667],\n",
       "       [0.793     ],\n",
       "       [0.78906667],\n",
       "       [0.78513333],\n",
       "       [0.78126667],\n",
       "       [0.77733333],\n",
       "       [0.77346667],\n",
       "       [0.76953333],\n",
       "       [0.7656    ],\n",
       "       [0.76173333],\n",
       "       [0.7578    ],\n",
       "       [0.75393333],\n",
       "       [0.75      ],\n",
       "       [0.74606667],\n",
       "       [0.7422    ],\n",
       "       [0.73826667],\n",
       "       [0.7344    ],\n",
       "       [0.73046667],\n",
       "       [0.72653333],\n",
       "       [0.72266667],\n",
       "       [0.71873333],\n",
       "       [0.71486667],\n",
       "       [0.71093333],\n",
       "       [0.707     ],\n",
       "       [0.70313333],\n",
       "       [0.6992    ],\n",
       "       [0.69533333],\n",
       "       [0.6914    ],\n",
       "       [0.68746667],\n",
       "       [0.6836    ],\n",
       "       [0.67966667],\n",
       "       [0.6758    ],\n",
       "       [0.67186667],\n",
       "       [0.668     ],\n",
       "       [0.66406667],\n",
       "       [0.66013333],\n",
       "       [0.65626667],\n",
       "       [0.65233333],\n",
       "       [0.64846667],\n",
       "       [0.64453333],\n",
       "       [0.6406    ],\n",
       "       [0.63673333],\n",
       "       [0.6328    ],\n",
       "       [0.62893333],\n",
       "       [0.625     ],\n",
       "       [0.62106667],\n",
       "       [0.6172    ],\n",
       "       [0.61326667],\n",
       "       [0.6094    ],\n",
       "       [0.60546667],\n",
       "       [0.60153333],\n",
       "       [0.59766667],\n",
       "       [0.59373333],\n",
       "       [0.58986667],\n",
       "       [0.58593333],\n",
       "       [0.582     ],\n",
       "       [0.57813333],\n",
       "       [0.5742    ],\n",
       "       [0.57033333],\n",
       "       [0.5664    ],\n",
       "       [0.56246667],\n",
       "       [0.5586    ],\n",
       "       [0.55466667],\n",
       "       [0.5508    ],\n",
       "       [0.54686667],\n",
       "       [0.543     ],\n",
       "       [0.53906667],\n",
       "       [0.53513333],\n",
       "       [0.53126667],\n",
       "       [0.52733333],\n",
       "       [0.52346667],\n",
       "       [0.51953333],\n",
       "       [0.5156    ],\n",
       "       [0.51173333],\n",
       "       [0.5078    ],\n",
       "       [0.50393333],\n",
       "       [0.5       ],\n",
       "       [0.49606667],\n",
       "       [0.4922    ],\n",
       "       [0.48826667],\n",
       "       [0.4844    ],\n",
       "       [0.48046667],\n",
       "       [0.47653333],\n",
       "       [0.47266667],\n",
       "       [0.46873333],\n",
       "       [0.46484667],\n",
       "       [0.46094   ],\n",
       "       [0.45703333],\n",
       "       [0.45312667],\n",
       "       [0.44922   ],\n",
       "       [0.44531333],\n",
       "       [0.44140667],\n",
       "       [0.4375    ],\n",
       "       [0.43359333],\n",
       "       [0.42968667],\n",
       "       [0.42578   ],\n",
       "       [0.42187333],\n",
       "       [0.41796667],\n",
       "       [0.41406   ],\n",
       "       [0.41015333],\n",
       "       [0.40624667],\n",
       "       [0.40234667],\n",
       "       [0.39844   ],\n",
       "       [0.39453333],\n",
       "       [0.39062667],\n",
       "       [0.38672   ],\n",
       "       [0.38281333],\n",
       "       [0.37890667],\n",
       "       [0.375     ],\n",
       "       [0.37109333],\n",
       "       [0.36718667],\n",
       "       [0.36328   ],\n",
       "       [0.35937333],\n",
       "       [0.35546667],\n",
       "       [0.35156   ],\n",
       "       [0.34765333],\n",
       "       [0.34374667],\n",
       "       [0.33984667],\n",
       "       [0.33594   ],\n",
       "       [0.33203333],\n",
       "       [0.32812667],\n",
       "       [0.32422   ],\n",
       "       [0.32031333],\n",
       "       [0.31640667],\n",
       "       [0.3125    ],\n",
       "       [0.30859333],\n",
       "       [0.30468667],\n",
       "       [0.30078   ],\n",
       "       [0.29687333],\n",
       "       [0.29296667],\n",
       "       [0.28906   ],\n",
       "       [0.28515333],\n",
       "       [0.28125333],\n",
       "       [0.27734667],\n",
       "       [0.27344   ],\n",
       "       [0.26953333],\n",
       "       [0.26562667],\n",
       "       [0.26172   ],\n",
       "       [0.25781333],\n",
       "       [0.25390667],\n",
       "       [0.25      ],\n",
       "       [0.24609333],\n",
       "       [0.24218667],\n",
       "       [0.23828   ],\n",
       "       [0.23437333],\n",
       "       [0.23046667],\n",
       "       [0.22656   ],\n",
       "       [0.22265333],\n",
       "       [0.21875333],\n",
       "       [0.21484667],\n",
       "       [0.21094   ],\n",
       "       [0.20703333],\n",
       "       [0.20312667],\n",
       "       [0.19922   ],\n",
       "       [0.19531333],\n",
       "       [0.19140667],\n",
       "       [0.1875    ],\n",
       "       [0.18359333],\n",
       "       [0.17968667],\n",
       "       [0.17578   ],\n",
       "       [0.17187333],\n",
       "       [0.16796667],\n",
       "       [0.16406   ],\n",
       "       [0.16015333],\n",
       "       [0.15625333],\n",
       "       [0.15234667],\n",
       "       [0.14844   ],\n",
       "       [0.14453333],\n",
       "       [0.14062667],\n",
       "       [0.13672   ],\n",
       "       [0.13281333],\n",
       "       [0.12890667],\n",
       "       [0.125     ],\n",
       "       [0.12109333],\n",
       "       [0.11718667],\n",
       "       [0.11328   ],\n",
       "       [0.10937333],\n",
       "       [0.10546667],\n",
       "       [0.10156   ],\n",
       "       [0.09765333],\n",
       "       [0.09375333],\n",
       "       [0.08984667],\n",
       "       [0.08594   ],\n",
       "       [0.08203333],\n",
       "       [0.07812667],\n",
       "       [0.07422   ],\n",
       "       [0.07031333],\n",
       "       [0.06640667],\n",
       "       [0.0625    ],\n",
       "       [0.05859333],\n",
       "       [0.05468667],\n",
       "       [0.05078   ],\n",
       "       [0.04687333],\n",
       "       [0.04296667],\n",
       "       [0.03906   ],\n",
       "       [0.03515333],\n",
       "       [0.03124667],\n",
       "       [0.02734667],\n",
       "       [0.02344   ],\n",
       "       [0.01953333],\n",
       "       [0.01562667],\n",
       "       [0.01172   ],\n",
       "       [0.00781333],\n",
       "       [0.00390667],\n",
       "       [0.        ]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "9fce2ba5",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lol u got error 0.20335515\n",
      "current 0.20018157\n",
      "lol u got error 0.20018157\n",
      "current 0.1970922\n",
      "lol u got error 0.1970922\n",
      "current 0.1940849\n",
      "lol u got error 0.1940849\n",
      "current 0.19115739\n",
      "lol u got error 0.19115739\n",
      "current 0.18830764\n",
      "lol u got error 0.18830764\n",
      "current 0.18553357\n",
      "lol u got error 0.18553357\n",
      "current 0.18283315\n",
      "lol u got error 0.18283315\n",
      "current 0.18020442\n",
      "lol u got error 0.18020442\n",
      "current 0.17764552\n",
      "lol u got error 0.17764552\n",
      "current 0.17515454\n",
      "lol u got error 0.17515454\n",
      "current 0.17272975\n",
      "lol u got error 0.17272975\n",
      "current 0.17036937\n",
      "lol u got error 0.17036937\n",
      "current 0.16807163\n",
      "lol u got error 0.16807163\n",
      "current 0.165835\n",
      "lol u got error 0.165835\n",
      "current 0.1636577\n",
      "lol u got error 0.1636577\n",
      "current 0.16153829\n",
      "lol u got error 0.16153829\n",
      "current 0.15947516\n",
      "lol u got error 0.15947516\n",
      "current 0.15746686\n",
      "lol u got error 0.15746686\n",
      "current 0.15551187\n",
      "lol u got error 0.15551187\n",
      "current 0.15360887\n",
      "lol u got error 0.15360887\n",
      "current 0.1517564\n",
      "lol u got error 0.1517564\n",
      "current 0.14995317\n",
      "lol u got error 0.14995317\n",
      "current 0.14819783\n",
      "lol u got error 0.14819783\n",
      "current 0.14648914\n",
      "lol u got error 0.14648914\n",
      "current 0.14482588\n",
      "lol u got error 0.14482588\n",
      "current 0.1432068\n",
      "lol u got error 0.1432068\n",
      "current 0.14163075\n",
      "lol u got error 0.14163075\n",
      "current 0.1400966\n",
      "lol u got error 0.1400966\n",
      "current 0.13860324\n",
      "lol u got error 0.13860324\n",
      "current 0.13714954\n",
      "lol u got error 0.13714954\n",
      "current 0.13573447\n",
      "lol u got error 0.13573447\n",
      "current 0.13435705\n",
      "lol u got error 0.13435705\n",
      "current 0.13301626\n",
      "lol u got error 0.13301626\n",
      "current 0.13171108\n",
      "lol u got error 0.13171108\n",
      "current 0.13044061\n",
      "lol u got error 0.13044061\n",
      "current 0.1292039\n",
      "lol u got error 0.1292039\n",
      "current 0.12800007\n",
      "lol u got error 0.12800007\n",
      "current 0.12682827\n",
      "lol u got error 0.12682827\n",
      "current 0.12568761\n",
      "lol u got error 0.12568761\n",
      "current 0.12457731\n",
      "lol u got error 0.12457731\n",
      "current 0.1234965\n",
      "lol u got error 0.1234965\n",
      "current 0.12244444\n",
      "lol u got error 0.12244444\n",
      "current 0.12142036\n",
      "lol u got error 0.12142036\n",
      "current 0.12042352\n",
      "lol u got error 0.12042352\n",
      "current 0.119453184\n",
      "lol u got error 0.119453184\n",
      "current 0.118508674\n",
      "lol u got error 0.118508674\n",
      "current 0.11758928\n",
      "lol u got error 0.11758928\n",
      "current 0.11669436\n",
      "lol u got error 0.11669436\n",
      "current 0.11582322\n",
      "lol u got error 0.11582322\n",
      "current 0.11497525\n",
      "lol u got error 0.11497525\n",
      "current 0.11414985\n",
      "lol u got error 0.11414985\n",
      "current 0.113346405\n",
      "lol u got error 0.113346405\n",
      "current 0.112564355\n",
      "lol u got error 0.112564355\n",
      "current 0.1118031\n",
      "lol u got error 0.1118031\n",
      "current 0.111062095\n",
      "lol u got error 0.111062095\n",
      "current 0.1103408\n",
      "lol u got error 0.1103408\n",
      "current 0.10963872\n",
      "lol u got error 0.10963872\n",
      "current 0.108955316\n",
      "lol u got error 0.108955316\n",
      "current 0.10829009\n",
      "lol u got error 0.10829009\n",
      "current 0.107642554\n",
      "lol u got error 0.107642554\n",
      "current 0.10701228\n",
      "lol u got error 0.10701228\n",
      "current 0.10639877\n",
      "lol u got error 0.10639877\n",
      "current 0.1058016\n",
      "lol u got error 0.1058016\n",
      "current 0.10522032\n",
      "lol u got error 0.10522032\n",
      "current 0.1046545\n",
      "lol u got error 0.1046545\n",
      "current 0.10410375\n",
      "lol u got error 0.10410375\n",
      "current 0.10356767\n",
      "lol u got error 0.10356767\n",
      "current 0.10304584\n",
      "lol u got error 0.10304584\n",
      "current 0.102537915\n",
      "lol u got error 0.102537915\n",
      "current 0.10204351\n",
      "lol u got error 0.10204351\n",
      "current 0.10156227\n",
      "lol u got error 0.10156227\n",
      "current 0.10109383\n",
      "lol u got error 0.10109383\n",
      "current 0.10063788\n",
      "lol u got error 0.10063788\n",
      "current 0.10019407\n",
      "lol u got error 0.10019407\n",
      "current 0.099762075\n",
      "lol u got error 0.099762075\n",
      "current 0.09934157\n",
      "lol u got error 0.09934157\n",
      "current 0.09893228\n",
      "lol u got error 0.09893228\n",
      "current 0.09853389\n",
      "lol u got error 0.09853389\n",
      "current 0.098146096\n",
      "lol u got error 0.098146096\n",
      "current 0.09776863\n",
      "lol u got error 0.09776863\n",
      "current 0.09740123\n",
      "lol u got error 0.09740123\n",
      "current 0.0970436\n",
      "lol u got error 0.0970436\n",
      "current 0.0966955\n",
      "lol u got error 0.0966955\n",
      "current 0.09635669\n",
      "lol u got error 0.09635669\n",
      "current 0.096026875\n",
      "lol u got error 0.096026875\n",
      "current 0.09570587\n",
      "lol u got error 0.09570587\n",
      "current 0.0953934\n",
      "lol u got error 0.0953934\n",
      "current 0.09508925\n",
      "lol u got error 0.09508925\n",
      "current 0.094793215\n",
      "lol u got error 0.094793215\n",
      "current 0.09450507\n",
      "lol u got error 0.09450507\n",
      "current 0.09422458\n",
      "lol u got error 0.09422458\n",
      "current 0.093951575\n",
      "lol u got error 0.093951575\n",
      "current 0.093685865\n",
      "lol u got error 0.093685865\n",
      "current 0.0934272\n",
      "lol u got error 0.0934272\n",
      "current 0.09317545\n",
      "lol u got error 0.09317545\n",
      "current 0.09293039\n",
      "lol u got error 0.09293039\n",
      "current 0.092691876\n",
      "lol u got error 0.092691876\n",
      "current 0.09245969\n",
      "lol u got error 0.09245969\n",
      "current 0.09223372\n",
      "lol u got error 0.09223372\n",
      "current 0.092013754\n",
      "lol u got error 0.092013754\n",
      "current 0.09179968\n",
      "lol u got error 0.09179968\n",
      "current 0.091591276\n",
      "lol u got error 0.091591276\n",
      "current 0.09138843\n",
      "lol u got error 0.09138843\n",
      "current 0.091191\n",
      "lol u got error 0.091191\n",
      "current 0.09099883\n",
      "lol u got error 0.09099883\n",
      "current 0.090811774\n",
      "lol u got error 0.090811774\n",
      "current 0.09062971\n",
      "lol u got error 0.09062971\n",
      "current 0.09045251\n",
      "lol u got error 0.09045251\n",
      "current 0.090280004\n",
      "lol u got error 0.090280004\n",
      "current 0.09011211\n",
      "lol u got error 0.09011211\n",
      "current 0.08994871\n",
      "lol u got error 0.08994871\n",
      "current 0.08978964\n",
      "lol u got error 0.08978964\n",
      "current 0.08963482\n",
      "lol u got error 0.08963482\n",
      "current 0.089484125\n",
      "lol u got error 0.089484125\n",
      "current 0.089337446\n",
      "lol u got error 0.089337446\n",
      "current 0.089194685\n",
      "lol u got error 0.089194685\n",
      "current 0.089055724\n",
      "lol u got error 0.089055724\n",
      "current 0.088920474\n",
      "lol u got error 0.088920474\n",
      "current 0.088788815\n",
      "lol u got error 0.088788815\n",
      "current 0.088660665\n",
      "lol u got error 0.088660665\n",
      "current 0.08853595\n",
      "lol u got error 0.08853595\n",
      "current 0.08841456\n",
      "lol u got error 0.08841456\n",
      "current 0.08829639\n",
      "lol u got error 0.08829639\n",
      "current 0.08818138\n",
      "lol u got error 0.08818138\n",
      "current 0.08806944\n",
      "lol u got error 0.08806944\n",
      "current 0.08796047\n",
      "lol u got error 0.08796047\n",
      "current 0.087854415\n",
      "lol u got error 0.087854415\n",
      "current 0.08775119\n",
      "lol u got error 0.08775119\n",
      "current 0.08765072\n",
      "lol u got error 0.08765072\n",
      "current 0.08755292\n",
      "lol u got error 0.08755292\n",
      "current 0.087457746\n",
      "lol u got error 0.087457746\n",
      "current 0.08736509\n",
      "lol u got error 0.08736509\n",
      "current 0.0872749\n",
      "lol u got error 0.0872749\n",
      "current 0.08718713\n",
      "lol u got error 0.08718713\n",
      "current 0.0871017\n",
      "lol u got error 0.0871017\n",
      "current 0.087018535\n",
      "lol u got error 0.087018535\n",
      "current 0.08693761\n",
      "lol u got error 0.08693761\n",
      "current 0.086858824\n",
      "lol u got error 0.086858824\n",
      "current 0.086782135\n",
      "lol u got error 0.086782135\n",
      "current 0.08670751\n",
      "lol u got error 0.08670751\n",
      "current 0.08663486\n",
      "lol u got error 0.08663486\n",
      "current 0.08656416\n",
      "lol u got error 0.08656416\n",
      "current 0.086495355\n",
      "lol u got error 0.086495355\n",
      "current 0.08642837\n",
      "lol u got error 0.08642837\n",
      "current 0.08636317\n",
      "lol u got error 0.08636317\n",
      "current 0.08629971\n",
      "lol u got error 0.08629971\n",
      "current 0.08623793\n",
      "lol u got error 0.08623793\n",
      "current 0.086177826\n",
      "lol u got error 0.086177826\n",
      "current 0.08611931\n",
      "lol u got error 0.08611931\n",
      "current 0.08606235\n",
      "lol u got error 0.08606235\n",
      "current 0.08600692\n",
      "lol u got error 0.08600692\n",
      "current 0.08595296\n",
      "lol u got error 0.08595296\n",
      "current 0.08590045\n",
      "lol u got error 0.08590045\n",
      "current 0.08584933\n",
      "lol u got error 0.08584933\n",
      "current 0.08579958\n",
      "lol u got error 0.08579958\n",
      "current 0.08575116\n",
      "lol u got error 0.08575116\n",
      "current 0.08570402\n",
      "lol u got error 0.08570402\n",
      "current 0.085658155\n",
      "lol u got error 0.085658155\n",
      "current 0.085613504\n",
      "lol u got error 0.085613504\n",
      "current 0.08557004\n",
      "lol u got error 0.08557004\n",
      "current 0.08552773\n",
      "lol u got error 0.08552773\n",
      "current 0.085486576\n",
      "lol u got error 0.085486576\n",
      "current 0.0854465\n",
      "lol u got error 0.0854465\n",
      "current 0.085407495\n",
      "lol u got error 0.085407495\n",
      "current 0.085369535\n",
      "lol u got error 0.085369535\n",
      "current 0.08533258\n",
      "lol u got error 0.08533258\n",
      "current 0.08529661\n",
      "lol u got error 0.08529661\n",
      "current 0.0852616\n",
      "lol u got error 0.0852616\n",
      "current 0.08522754\n",
      "lol u got error 0.08522754\n",
      "current 0.08519438\n",
      "lol u got error 0.08519438\n",
      "current 0.0851621\n",
      "lol u got error 0.0851621\n",
      "current 0.08513068\n",
      "lol u got error 0.08513068\n",
      "current 0.0851001\n",
      "lol u got error 0.0851001\n",
      "current 0.08507034\n",
      "lol u got error 0.08507034\n",
      "current 0.08504138\n",
      "lol u got error 0.08504138\n",
      "current 0.08501318\n",
      "lol u got error 0.08501318\n",
      "current 0.08498574\n",
      "lol u got error 0.08498574\n",
      "current 0.084959015\n",
      "lol u got error 0.084959015\n",
      "current 0.084933035\n",
      "lol u got error 0.084933035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current 0.084907725\n",
      "lol u got error 0.084907725\n",
      "current 0.084883094\n",
      "lol u got error 0.084883094\n",
      "current 0.08485913\n",
      "lol u got error 0.08485913\n",
      "current 0.084835805\n",
      "lol u got error 0.084835805\n",
      "current 0.08481309\n",
      "lol u got error 0.08481309\n",
      "current 0.084790975\n",
      "lol u got error 0.084790975\n",
      "current 0.084769465\n",
      "lol u got error 0.084769465\n",
      "current 0.08474854\n",
      "lol u got error 0.08474854\n",
      "current 0.08472816\n",
      "lol u got error 0.08472816\n",
      "current 0.0847083\n",
      "lol u got error 0.0847083\n",
      "current 0.084689006\n",
      "lol u got error 0.084689006\n",
      "current 0.084670216\n",
      "lol u got error 0.084670216\n",
      "current 0.084651925\n",
      "lol u got error 0.084651925\n",
      "current 0.08463412\n",
      "lol u got error 0.08463412\n",
      "current 0.08461678\n",
      "lol u got error 0.08461678\n",
      "current 0.08459993\n",
      "lol u got error 0.08459993\n",
      "current 0.08458352\n",
      "lol u got error 0.08458352\n",
      "current 0.08456753\n",
      "lol u got error 0.08456753\n",
      "current 0.08455198\n",
      "lol u got error 0.08455198\n",
      "current 0.08453685\n",
      "lol u got error 0.08453685\n",
      "current 0.08452212\n",
      "lol u got error 0.08452212\n",
      "current 0.08450778\n",
      "lol u got error 0.08450778\n",
      "current 0.08449383\n",
      "lol u got error 0.08449383\n",
      "current 0.08448024\n",
      "lol u got error 0.08448024\n",
      "current 0.084467016\n",
      "lol u got error 0.084467016\n",
      "current 0.08445414\n",
      "lol u got error 0.08445414\n",
      "current 0.084441625\n",
      "lol u got error 0.084441625\n",
      "current 0.08442945\n",
      "lol u got error 0.08442945\n",
      "current 0.08441758\n",
      "lol u got error 0.08441758\n",
      "current 0.084406026\n",
      "lol u got error 0.084406026\n",
      "current 0.08439478\n",
      "lol u got error 0.08439478\n",
      "current 0.08438385\n",
      "lol u got error 0.08438385\n",
      "current 0.0843732\n",
      "lol u got error 0.0843732\n",
      "current 0.084362835\n",
      "lol u got error 0.084362835\n",
      "current 0.084352754\n",
      "lol u got error 0.084352754\n",
      "current 0.084342934\n",
      "lol u got error 0.084342934\n",
      "current 0.08433337\n",
      "lol u got error 0.08433337\n",
      "current 0.08432408\n",
      "lol u got error 0.08432408\n",
      "current 0.08431504\n",
      "lol u got error 0.08431504\n",
      "current 0.08430621\n",
      "lol u got error 0.08430621\n",
      "current 0.08429764\n",
      "lol u got error 0.08429764\n",
      "current 0.08428929\n",
      "lol u got error 0.08428929\n",
      "current 0.08428117\n",
      "lol u got error 0.08428117\n",
      "current 0.08427327\n",
      "lol u got error 0.08427327\n",
      "current 0.084265575\n",
      "lol u got error 0.084265575\n",
      "current 0.08425808\n",
      "lol u got error 0.08425808\n",
      "current 0.08425078\n",
      "lol u got error 0.08425078\n",
      "current 0.08424369\n",
      "lol u got error 0.08424369\n",
      "current 0.084236786\n",
      "lol u got error 0.084236786\n",
      "current 0.08423006\n",
      "lol u got error 0.08423006\n",
      "current 0.084223524\n",
      "lol u got error 0.084223524\n",
      "current 0.08421715\n",
      "lol u got error 0.08421715\n",
      "current 0.08421095\n",
      "lol u got error 0.08421095\n",
      "current 0.08420493\n",
      "lol u got error 0.08420493\n",
      "current 0.08419905\n",
      "lol u got error 0.08419905\n",
      "current 0.08419333\n",
      "lol u got error 0.08419333\n",
      "current 0.08418777\n",
      "lol u got error 0.08418777\n",
      "current 0.08418235\n",
      "lol u got error 0.08418235\n",
      "current 0.084177084\n",
      "lol u got error 0.084177084\n",
      "current 0.08417196\n",
      "lol u got error 0.08417196\n",
      "current 0.08416696\n",
      "lol u got error 0.08416696\n",
      "current 0.08416211\n",
      "lol u got error 0.08416211\n",
      "current 0.08415738\n",
      "lol u got error 0.08415738\n",
      "current 0.08415277\n",
      "lol u got error 0.08415277\n",
      "current 0.084148295\n",
      "lol u got error 0.084148295\n",
      "current 0.08414394\n",
      "lol u got error 0.08414394\n",
      "current 0.0841397\n",
      "lol u got error 0.0841397\n",
      "current 0.08413557\n",
      "lol u got error 0.08413557\n",
      "current 0.08413155\n",
      "lol u got error 0.08413155\n",
      "current 0.084127635\n",
      "lol u got error 0.084127635\n",
      "current 0.08412382\n",
      "lol u got error 0.08412382\n",
      "current 0.08412012\n",
      "lol u got error 0.08412012\n",
      "current 0.08411651\n",
      "lol u got error 0.08411651\n",
      "current 0.084113\n",
      "lol u got error 0.084113\n",
      "current 0.08410958\n",
      "lol u got error 0.08410958\n",
      "current 0.08410627\n",
      "lol u got error 0.08410627\n",
      "current 0.084103025\n",
      "lol u got error 0.084103025\n",
      "current 0.084099874\n",
      "lol u got error 0.084099874\n",
      "current 0.0840968\n",
      "lol u got error 0.0840968\n",
      "current 0.08409381\n",
      "lol u got error 0.08409381\n",
      "current 0.08409091\n",
      "lol u got error 0.08409091\n",
      "current 0.08408809\n",
      "lol u got error 0.08408809\n",
      "current 0.08408533\n",
      "lol u got error 0.08408533\n",
      "current 0.08408265\n",
      "lol u got error 0.08408265\n",
      "current 0.08408004\n",
      "lol u got error 0.08408004\n",
      "current 0.0840775\n",
      "lol u got error 0.0840775\n",
      "current 0.08407503\n",
      "lol u got error 0.08407503\n",
      "current 0.084072635\n",
      "lol u got error 0.084072635\n",
      "current 0.08407029\n",
      "lol u got error 0.08407029\n",
      "current 0.084068015\n",
      "lol u got error 0.084068015\n",
      "current 0.084065795\n",
      "lol u got error 0.084065795\n",
      "current 0.08406364\n",
      "lol u got error 0.08406364\n",
      "current 0.08406154\n",
      "lol u got error 0.08406154\n",
      "current 0.08405949\n",
      "lol u got error 0.08405949\n",
      "current 0.0840575\n",
      "lol u got error 0.0840575\n",
      "current 0.08405558\n",
      "lol u got error 0.08405558\n",
      "current 0.08405368\n",
      "lol u got error 0.08405368\n",
      "current 0.084051855\n",
      "lol u got error 0.084051855\n",
      "current 0.084050074\n",
      "lol u got error 0.084050074\n",
      "current 0.08404833\n",
      "lol u got error 0.08404833\n",
      "current 0.08404664\n",
      "lol u got error 0.08404664\n",
      "current 0.08404499\n",
      "lol u got error 0.08404499\n",
      "current 0.0840434\n",
      "lol u got error 0.0840434\n",
      "current 0.08404184\n",
      "lol u got error 0.08404184\n",
      "current 0.084040314\n",
      "lol u got error 0.084040314\n",
      "current 0.08403884\n",
      "lol u got error 0.08403884\n",
      "current 0.08403741\n",
      "lol u got error 0.08403741\n",
      "current 0.084036015\n",
      "lol u got error 0.084036015\n",
      "current 0.084034644\n",
      "lol u got error 0.084034644\n",
      "current 0.08403331\n",
      "lol u got error 0.08403331\n",
      "current 0.08403202\n",
      "lol u got error 0.08403202\n",
      "current 0.08403077\n",
      "lol u got error 0.08403077\n",
      "current 0.08402955\n",
      "lol u got error 0.08402955\n",
      "current 0.084028356\n",
      "lol u got error 0.084028356\n",
      "current 0.0840272\n",
      "lol u got error 0.0840272\n",
      "current 0.084026076\n",
      "lol u got error 0.084026076\n",
      "current 0.08402498\n",
      "lol u got error 0.08402498\n",
      "current 0.084023915\n",
      "lol u got error 0.084023915\n",
      "current 0.08402288\n",
      "lol u got error 0.08402288\n",
      "current 0.08402186\n",
      "lol u got error 0.08402186\n",
      "current 0.084020875\n",
      "lol u got error 0.084020875\n",
      "current 0.08401991\n",
      "lol u got error 0.08401991\n",
      "current 0.08401898\n",
      "lol u got error 0.08401898\n",
      "current 0.08401808\n",
      "lol u got error 0.08401808\n",
      "current 0.084017195\n",
      "lol u got error 0.084017195\n",
      "current 0.08401633\n",
      "lol u got error 0.08401633\n",
      "current 0.0840155\n",
      "lol u got error 0.0840155\n",
      "current 0.08401469\n",
      "lol u got error 0.08401469\n",
      "current 0.084013894\n",
      "lol u got error 0.084013894\n",
      "current 0.08401313\n",
      "lol u got error 0.08401313\n",
      "current 0.084012374\n",
      "lol u got error 0.084012374\n",
      "current 0.084011644\n",
      "lol u got error 0.084011644\n",
      "current 0.08401092\n",
      "lol u got error 0.08401092\n",
      "current 0.08401022\n",
      "lol u got error 0.08401022\n",
      "current 0.084009565\n",
      "lol u got error 0.084009565\n",
      "current 0.0840089\n",
      "lol u got error 0.0840089\n",
      "current 0.08400827\n",
      "lol u got error 0.08400827\n",
      "current 0.08400764\n",
      "lol u got error 0.08400764\n",
      "current 0.084007055\n",
      "lol u got error 0.084007055\n",
      "current 0.08400644\n",
      "lol u got error 0.08400644\n",
      "current 0.08400588\n",
      "lol u got error 0.08400588\n",
      "current 0.084005326\n",
      "lol u got error 0.084005326\n",
      "current 0.08400478\n",
      "lol u got error 0.08400478\n",
      "current 0.084004246\n",
      "lol u got error 0.084004246\n",
      "current 0.08400373\n",
      "lol u got error 0.08400373\n",
      "current 0.08400323\n",
      "lol u got error 0.08400323\n",
      "current 0.08400275\n",
      "lol u got error 0.08400275\n",
      "current 0.08400228\n",
      "lol u got error 0.08400228\n",
      "current 0.084001824\n",
      "lol u got error 0.084001824\n",
      "current 0.08400136\n",
      "lol u got error 0.08400136\n",
      "current 0.08400093\n",
      "lol u got error 0.08400093\n",
      "current 0.084000506\n",
      "lol u got error 0.084000506\n",
      "current 0.0840001\n",
      "lol u got error 0.0840001\n",
      "current 0.08399969\n",
      "lol u got error 0.08399969\n",
      "current 0.0839993\n",
      "lol u got error 0.0839993\n",
      "current 0.08399891\n",
      "lol u got error 0.08399891\n",
      "current 0.08399854\n",
      "lol u got error 0.08399854\n",
      "current 0.08399818\n",
      "lol u got error 0.08399818\n",
      "current 0.08399783\n",
      "lol u got error 0.08399783\n",
      "current 0.08399749\n",
      "lol u got error 0.08399749\n",
      "current 0.08399715\n",
      "lol u got error 0.08399715\n",
      "current 0.08399683\n",
      "lol u got error 0.08399683\n",
      "current 0.08399651\n",
      "lol u got error 0.08399651\n",
      "current 0.08399621\n",
      "lol u got error 0.08399621\n",
      "current 0.08399591\n",
      "lol u got error 0.08399591\n",
      "current 0.08399562\n",
      "lol u got error 0.08399562\n",
      "current 0.08399533\n",
      "lol u got error 0.08399533\n",
      "current 0.08399506\n",
      "lol u got error 0.08399506\n",
      "current 0.0839948\n",
      "lol u got error 0.0839948\n",
      "current 0.08399453\n",
      "lol u got error 0.08399453\n",
      "current 0.08399428\n",
      "lol u got error 0.08399428\n",
      "current 0.08399403\n",
      "lol u got error 0.08399403\n",
      "current 0.083993785\n",
      "lol u got error 0.083993785\n",
      "current 0.08399355\n",
      "lol u got error 0.08399355\n",
      "current 0.083993316\n",
      "lol u got error 0.083993316\n",
      "current 0.0839931\n",
      "lol u got error 0.0839931\n",
      "current 0.08399288\n",
      "lol u got error 0.08399288\n",
      "current 0.08399267\n",
      "lol u got error 0.08399267\n",
      "current 0.083992474\n",
      "lol u got error 0.083992474\n",
      "current 0.08399227\n",
      "lol u got error 0.08399227\n",
      "current 0.08399207\n",
      "lol u got error 0.08399207\n",
      "current 0.083991885\n",
      "lol u got error 0.083991885\n",
      "current 0.0839917\n",
      "lol u got error 0.0839917\n",
      "current 0.08399153\n",
      "lol u got error 0.08399153\n",
      "current 0.08399135\n",
      "lol u got error 0.08399135\n",
      "current 0.083991185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lol u got error 0.083991185\n",
      "current 0.08399102\n",
      "lol u got error 0.08399102\n",
      "current 0.08399085\n",
      "lol u got error 0.08399085\n",
      "current 0.08399069\n",
      "lol u got error 0.08399069\n",
      "current 0.08399055\n",
      "lol u got error 0.08399055\n",
      "current 0.08399039\n",
      "lol u got error 0.08399039\n",
      "current 0.08399026\n",
      "lol u got error 0.08399026\n",
      "current 0.08399011\n",
      "lol u got error 0.08399011\n",
      "current 0.08398998\n",
      "lol u got error 0.08398998\n",
      "current 0.08398984\n",
      "lol u got error 0.08398984\n",
      "current 0.08398972\n",
      "lol u got error 0.08398972\n",
      "current 0.083989576\n",
      "lol u got error 0.083989576\n",
      "current 0.083989464\n",
      "lol u got error 0.083989464\n",
      "current 0.08398934\n",
      "lol u got error 0.08398934\n",
      "current 0.083989225\n",
      "lol u got error 0.083989225\n",
      "current 0.08398913\n",
      "lol u got error 0.08398913\n",
      "current 0.08398902\n",
      "lol u got error 0.08398902\n",
      "current 0.08398889\n",
      "lol u got error 0.08398889\n",
      "current 0.08398879\n",
      "lol u got error 0.08398879\n",
      "current 0.0839887\n",
      "lol u got error 0.0839887\n",
      "current 0.08398861\n",
      "lol u got error 0.08398861\n",
      "current 0.0839885\n",
      "lol u got error 0.0839885\n",
      "current 0.08398841\n",
      "lol u got error 0.08398841\n",
      "current 0.08398832\n",
      "lol u got error 0.08398832\n",
      "current 0.08398823\n",
      "lol u got error 0.08398823\n",
      "current 0.083988145\n",
      "lol u got error 0.083988145\n",
      "current 0.083988056\n",
      "lol u got error 0.083988056\n",
      "current 0.08398797\n",
      "lol u got error 0.08398797\n",
      "current 0.0839879\n",
      "lol u got error 0.0839879\n",
      "current 0.083987825\n",
      "lol u got error 0.083987825\n",
      "current 0.08398774\n",
      "lol u got error 0.08398774\n",
      "current 0.08398767\n",
      "lol u got error 0.08398767\n",
      "current 0.0839876\n",
      "lol u got error 0.0839876\n",
      "current 0.08398753\n",
      "lol u got error 0.08398753\n",
      "current 0.08398746\n",
      "lol u got error 0.08398746\n",
      "current 0.0839874\n",
      "lol u got error 0.0839874\n",
      "current 0.08398734\n",
      "lol u got error 0.08398734\n",
      "current 0.08398727\n",
      "lol u got error 0.08398727\n",
      "current 0.083987206\n",
      "lol u got error 0.083987206\n",
      "current 0.083987154\n",
      "lol u got error 0.083987154\n",
      "current 0.083987094\n",
      "lol u got error 0.083987094\n",
      "current 0.083987035\n",
      "lol u got error 0.083987035\n",
      "current 0.08398699\n",
      "lol u got error 0.08398699\n",
      "current 0.08398694\n",
      "lol u got error 0.08398694\n",
      "current 0.08398688\n",
      "lol u got error 0.08398688\n",
      "current 0.08398684\n",
      "lol u got error 0.08398684\n",
      "current 0.08398678\n",
      "lol u got error 0.08398678\n",
      "current 0.08398674\n",
      "lol u got error 0.08398674\n",
      "current 0.08398669\n",
      "lol u got error 0.08398669\n",
      "current 0.08398664\n",
      "lol u got error 0.08398664\n",
      "current 0.083986595\n",
      "lol u got error 0.083986595\n",
      "current 0.08398655\n",
      "lol u got error 0.08398655\n",
      "current 0.08398651\n",
      "lol u got error 0.08398651\n",
      "current 0.08398647\n",
      "lol u got error 0.08398647\n",
      "current 0.08398644\n",
      "lol u got error 0.08398644\n",
      "current 0.0839864\n",
      "lol u got error 0.0839864\n",
      "current 0.08398636\n",
      "lol u got error 0.08398636\n",
      "current 0.08398633\n",
      "lol u got error 0.08398633\n",
      "current 0.08398629\n",
      "lol u got error 0.08398629\n",
      "current 0.083986245\n",
      "lol u got error 0.083986245\n",
      "current 0.08398622\n",
      "lol u got error 0.08398622\n",
      "current 0.083986185\n",
      "lol u got error 0.083986185\n",
      "current 0.08398615\n",
      "lol u got error 0.08398615\n",
      "current 0.08398612\n",
      "lol u got error 0.08398612\n",
      "current 0.083986096\n",
      "lol u got error 0.083986096\n",
      "current 0.08398606\n",
      "lol u got error 0.08398606\n",
      "current 0.08398603\n",
      "lol u got error 0.08398603\n",
      "current 0.08398601\n",
      "lol u got error 0.08398601\n",
      "current 0.083985984\n",
      "lol u got error 0.083985984\n",
      "current 0.08398597\n",
      "lol u got error 0.08398597\n",
      "current 0.08398594\n",
      "lol u got error 0.08398594\n",
      "current 0.0839859\n",
      "lol u got error 0.0839859\n",
      "current 0.08398588\n",
      "lol u got error 0.08398588\n",
      "current 0.083985865\n",
      "lol u got error 0.083985865\n",
      "current 0.083985835\n",
      "lol u got error 0.083985835\n",
      "current 0.08398581\n",
      "lol u got error 0.08398581\n",
      "current 0.08398579\n",
      "lol u got error 0.08398579\n",
      "current 0.08398576\n",
      "lol u got error 0.08398576\n",
      "current 0.08398575\n",
      "lol u got error 0.08398575\n",
      "current 0.08398572\n",
      "lol u got error 0.08398572\n",
      "current 0.0839857\n",
      "lol u got error 0.0839857\n",
      "current 0.08398569\n",
      "lol u got error 0.08398569\n",
      "current 0.083985664\n",
      "lol u got error 0.083985664\n",
      "current 0.08398565\n",
      "lol u got error 0.08398565\n",
      "current 0.083985634\n",
      "lol u got error 0.083985634\n",
      "current 0.08398562\n",
      "lol u got error 0.08398562\n",
      "current 0.083985604\n",
      "lol u got error 0.083985604\n",
      "current 0.08398559\n",
      "lol u got error 0.08398559\n",
      "current 0.083985575\n",
      "lol u got error 0.083985575\n",
      "current 0.08398555\n",
      "lol u got error 0.08398555\n",
      "current 0.083985545\n",
      "lol u got error 0.083985545\n",
      "current 0.08398552\n",
      "lol u got error 0.08398552\n",
      "current 0.083985515\n",
      "lol u got error 0.083985515\n",
      "current 0.0839855\n",
      "lol u got error 0.0839855\n",
      "current 0.08398547\n",
      "lol u got error 0.08398547\n",
      "current 0.08398546\n",
      "lol u got error 0.08398546\n",
      "current 0.083985455\n",
      "lol u got error 0.083985455\n",
      "current 0.08398545\n",
      "lol u got error 0.08398545\n",
      "current 0.08398544\n",
      "lol u got error 0.08398544\n",
      "current 0.083985426\n",
      "lol u got error 0.083985426\n",
      "current 0.08398541\n",
      "lol u got error 0.08398541\n",
      "current 0.083985396\n",
      "lol u got error 0.083985396\n",
      "current 0.08398539\n",
      "lol u got error 0.08398539\n",
      "current 0.08398538\n",
      "lol u got error 0.08398538\n",
      "current 0.083985366\n",
      "lol u got error 0.083985366\n",
      "current 0.08398535\n",
      "lol u got error 0.08398535\n",
      "current 0.08398534\n",
      "lol u got error 0.08398534\n",
      "current 0.08398534\n",
      "lol u got error 0.08398534\n",
      "current 0.083985336\n",
      "lol u got error 0.083985336\n",
      "current 0.08398533\n",
      "lol u got error 0.08398533\n",
      "current 0.08398532\n",
      "lol u got error 0.08398532\n",
      "current 0.08398531\n",
      "lol u got error 0.08398531\n",
      "current 0.0839853\n",
      "lol u got error 0.0839853\n",
      "current 0.0839853\n",
      "lol u got error 0.0839853\n",
      "current 0.08398529\n",
      "lol u got error 0.08398529\n",
      "current 0.083985284\n",
      "lol u got error 0.083985284\n",
      "current 0.08398527\n",
      "lol u got error 0.08398527\n",
      "current 0.08398527\n",
      "lol u got error 0.08398527\n",
      "current 0.08398526\n",
      "lol u got error 0.08398526\n",
      "current 0.08398525\n",
      "lol u got error 0.08398525\n",
      "current 0.08398525\n",
      "lol u got error 0.08398525\n",
      "current 0.08398524\n",
      "lol u got error 0.08398524\n",
      "current 0.08398523\n",
      "lol u got error 0.08398523\n",
      "current 0.08398523\n",
      "lol u got error 0.08398523\n",
      "current 0.083985224\n",
      "lol u got error 0.083985224\n",
      "current 0.08398521\n",
      "lol u got error 0.08398521\n",
      "current 0.08398521\n",
      "lol u got error 0.08398521\n",
      "current 0.0839852\n",
      "lol u got error 0.0839852\n",
      "current 0.08398519\n",
      "lol u got error 0.08398519\n",
      "current 0.083985195\n",
      "lol u got error 0.083985195\n",
      "current 0.08398519\n",
      "lol u got error 0.08398519\n",
      "current 0.08398518\n",
      "lol u got error 0.08398518\n",
      "current 0.08398519\n",
      "lol u got error 0.08398519\n",
      "current 0.083985165\n",
      "lol u got error 0.083985165\n",
      "current 0.08398517\n",
      "lol u got error 0.08398517\n",
      "current 0.083985165\n",
      "lol u got error 0.083985165\n",
      "current 0.083985165\n",
      "lol u got error 0.083985165\n",
      "current 0.08398516\n",
      "lol u got error 0.08398516\n",
      "current 0.08398515\n",
      "lol u got error 0.08398515\n",
      "current 0.08398515\n",
      "lol u got error 0.08398515\n",
      "current 0.08398514\n",
      "lol u got error 0.08398514\n",
      "current 0.083985135\n",
      "lol u got error 0.083985135\n",
      "current 0.08398514\n",
      "lol u got error 0.08398514\n",
      "current 0.08398513\n",
      "lol u got error 0.08398513\n",
      "current 0.083985135\n",
      "lol u got error 0.083985135\n",
      "current 0.08398513\n",
      "lol u got error 0.08398513\n",
      "current 0.08398512\n",
      "lol u got error 0.08398512\n",
      "current 0.08398511\n",
      "lol u got error 0.08398511\n",
      "current 0.08398512\n",
      "lol u got error 0.08398512\n",
      "current 0.08398512\n",
      "lol u got error 0.08398512\n",
      "current 0.083985105\n",
      "lol u got error 0.083985105\n",
      "current 0.08398511\n",
      "lol u got error 0.08398511\n",
      "current 0.0839851\n",
      "lol u got error 0.0839851\n",
      "current 0.0839851\n",
      "lol u got error 0.0839851\n",
      "current 0.0839851\n",
      "lol u got error 0.0839851\n",
      "current 0.08398508\n",
      "lol u got error 0.08398508\n",
      "current 0.083985105\n",
      "lol u got error 0.083985105\n",
      "current 0.08398509\n",
      "lol u got error 0.08398509\n",
      "current 0.08398509\n",
      "lol u got error 0.08398509\n",
      "current 0.08398509\n",
      "lol u got error 0.08398509\n",
      "current 0.08398508\n",
      "lol u got error 0.08398508\n",
      "current 0.08398508\n",
      "lol u got error 0.08398508\n",
      "current 0.083985075\n",
      "lol u got error 0.083985075\n",
      "current 0.083985075\n",
      "lol u got error 0.083985075\n",
      "current 0.083985075\n",
      "lol u got error 0.083985075\n",
      "current 0.083985075\n",
      "lol u got error 0.083985075\n",
      "current 0.083985075\n",
      "lol u got error 0.083985075\n",
      "current 0.083985075\n",
      "lol u got error 0.083985075\n",
      "current 0.08398506\n",
      "lol u got error 0.08398506\n",
      "current 0.08398507\n",
      "lol u got error 0.08398507\n",
      "current 0.08398507\n",
      "lol u got error 0.08398507\n",
      "current 0.08398507\n",
      "lol u got error 0.08398507\n",
      "current 0.08398506\n",
      "lol u got error 0.08398506\n",
      "current 0.08398506\n",
      "lol u got error 0.08398506\n",
      "current 0.08398506\n",
      "lol u got error 0.08398506\n",
      "current 0.08398505\n",
      "lol u got error 0.08398505\n",
      "current 0.08398505\n",
      "lol u got error 0.08398505\n",
      "current 0.08398506\n",
      "lol u got error 0.08398506\n",
      "current 0.083985046\n",
      "lol u got error 0.083985046\n",
      "current 0.08398506\n",
      "lol u got error 0.08398506\n",
      "current 0.08398505\n",
      "lol u got error 0.08398505\n",
      "current 0.08398505\n",
      "lol u got error 0.08398505\n",
      "current 0.08398505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lol u got error 0.08398505\n",
      "current 0.08398505\n",
      "lol u got error 0.08398505\n",
      "current 0.083985046\n",
      "lol u got error 0.083985046\n",
      "current 0.083985046\n",
      "lol u got error 0.083985046\n",
      "current 0.083985046\n",
      "lol u got error 0.083985046\n",
      "current 0.083985046\n",
      "lol u got error 0.083985046\n",
      "current 0.083985046\n",
      "lol u got error 0.083985046\n",
      "current 0.083985046\n",
      "lol u got error 0.083985046\n",
      "current 0.083985046\n",
      "lol u got error 0.083985046\n",
      "current 0.08398504\n",
      "lol u got error 0.08398504\n",
      "current 0.08398503\n",
      "lol u got error 0.08398503\n",
      "current 0.08398504\n",
      "lol u got error 0.08398504\n",
      "current 0.08398504\n",
      "lol u got error 0.08398504\n",
      "current 0.08398503\n",
      "lol u got error 0.08398503\n",
      "current 0.08398503\n",
      "lol u got error 0.08398503\n",
      "current 0.08398503\n",
      "lol u got error 0.08398503\n",
      "current 0.08398503\n",
      "lol u got error 0.08398503\n",
      "current 0.08398503\n",
      "lol u got error 0.08398503\n",
      "current 0.08398503\n",
      "lol u got error 0.08398503\n",
      "current 0.08398503\n",
      "lol u got error 0.08398503\n",
      "current 0.08398504\n",
      "lol u got error 0.08398504\n",
      "current 0.08398503\n",
      "lol u got error 0.08398503\n",
      "current 0.08398502\n",
      "lol u got error 0.08398502\n",
      "current 0.08398503\n",
      "lol u got error 0.08398503\n",
      "current 0.08398503\n",
      "lol u got error 0.08398503\n",
      "current 0.08398503\n",
      "lol u got error 0.08398503\n",
      "current 0.08398503\n",
      "lol u got error 0.08398503\n",
      "current 0.083985016\n",
      "lol u got error 0.083985016\n",
      "current 0.08398502\n",
      "lol u got error 0.08398502\n",
      "current 0.083985016\n",
      "lol u got error 0.083985016\n",
      "current 0.08398502\n",
      "lol u got error 0.08398502\n",
      "current 0.083985016\n",
      "lol u got error 0.083985016\n",
      "current 0.08398503\n",
      "lol u got error 0.08398503\n",
      "current 0.083985016\n",
      "lol u got error 0.083985016\n",
      "current 0.08398502\n",
      "lol u got error 0.08398502\n",
      "current 0.08398502\n",
      "lol u got error 0.08398502\n",
      "current 0.083985016\n",
      "lol u got error 0.083985016\n",
      "current 0.08398502\n",
      "lol u got error 0.08398502\n",
      "current 0.08398502\n",
      "lol u got error 0.08398502\n",
      "current 0.083985016\n",
      "lol u got error 0.083985016\n",
      "current 0.083985016\n",
      "lol u got error 0.083985016\n",
      "current 0.083985016\n",
      "lol u got error 0.083985016\n",
      "current 0.083985016\n",
      "lol u got error 0.083985016\n",
      "current 0.08398502\n",
      "lol u got error 0.08398502\n",
      "current 0.083985016\n",
      "lol u got error 0.083985016\n",
      "current 0.08398501\n",
      "lol u got error 0.08398501\n",
      "current 0.083985016\n",
      "lol u got error 0.083985016\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.08398501\n",
      "lol u got error 0.08398501\n",
      "current 0.083985016\n",
      "lol u got error 0.083985016\n",
      "current 0.08398502\n",
      "lol u got error 0.08398502\n",
      "current 0.08398501\n",
      "lol u got error 0.08398501\n",
      "current 0.083985016\n",
      "lol u got error 0.083985016\n",
      "current 0.08398501\n",
      "lol u got error 0.08398501\n",
      "current 0.083985016\n",
      "lol u got error 0.083985016\n",
      "current 0.083985016\n",
      "lol u got error 0.083985016\n",
      "current 0.08398501\n",
      "lol u got error 0.08398501\n",
      "current 0.08398501\n",
      "lol u got error 0.08398501\n",
      "current 0.083985016\n",
      "lol u got error 0.083985016\n",
      "current 0.08398501\n",
      "lol u got error 0.08398501\n",
      "current 0.083985016\n",
      "lol u got error 0.083985016\n",
      "current 0.08398501\n",
      "lol u got error 0.08398501\n",
      "current 0.08398501\n",
      "lol u got error 0.08398501\n",
      "current 0.083985016\n",
      "lol u got error 0.083985016\n",
      "current 0.083985016\n",
      "lol u got error 0.083985016\n",
      "current 0.08398501\n",
      "lol u got error 0.08398501\n",
      "current 0.083985016\n",
      "lol u got error 0.083985016\n",
      "current 0.083985016\n",
      "lol u got error 0.083985016\n",
      "current 0.08398501\n",
      "lol u got error 0.08398501\n",
      "current 0.08398501\n",
      "lol u got error 0.08398501\n",
      "current 0.08398501\n",
      "lol u got error 0.08398501\n",
      "current 0.08398501\n",
      "lol u got error 0.08398501\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985016\n",
      "lol u got error 0.083985016\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.08398501\n",
      "lol u got error 0.08398501\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.08398501\n",
      "lol u got error 0.08398501\n",
      "current 0.083985016\n",
      "lol u got error 0.083985016\n",
      "current 0.08398501\n",
      "lol u got error 0.08398501\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.08398501\n",
      "lol u got error 0.08398501\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.08398501\n",
      "lol u got error 0.08398501\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.08398501\n",
      "lol u got error 0.08398501\n",
      "current 0.08398501\n",
      "lol u got error 0.08398501\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.08398501\n",
      "lol u got error 0.08398501\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.08398501\n",
      "lol u got error 0.08398501\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.08398501\n",
      "lol u got error 0.08398501\n",
      "current 0.08398501\n",
      "lol u got error 0.08398501\n",
      "current 0.08398501\n",
      "lol u got error 0.08398501\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.08398501\n",
      "lol u got error 0.08398501\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.08398501\n",
      "lol u got error 0.08398501\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.08398501\n",
      "lol u got error 0.08398501\n",
      "current 0.08398501\n",
      "lol u got error 0.08398501\n",
      "current 0.08398501\n",
      "lol u got error 0.08398501\n",
      "current 0.08398501\n",
      "lol u got error 0.08398501\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.08398501\n",
      "lol u got error 0.08398501\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.08398501\n",
      "lol u got error 0.08398501\n",
      "current 0.08398501\n",
      "lol u got error 0.08398501\n",
      "current 0.08398501\n",
      "lol u got error 0.08398501\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.08398501\n",
      "lol u got error 0.08398501\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.08398501\n",
      "lol u got error 0.08398501\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.08398501\n",
      "lol u got error 0.08398501\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.08398501\n",
      "lol u got error 0.08398501\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.08398501\n",
      "lol u got error 0.08398501\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.08398501\n",
      "lol u got error 0.08398501\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.08398501\n",
      "lol u got error 0.08398501\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.08398499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lol u got error 0.08398499\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.08398501\n",
      "lol u got error 0.08398501\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.08398501\n",
      "lol u got error 0.08398501\n",
      "current 0.08398501\n",
      "lol u got error 0.08398501\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.08398501\n",
      "lol u got error 0.08398501\n",
      "current 0.08398501\n",
      "lol u got error 0.08398501\n",
      "current 0.08398501\n",
      "lol u got error 0.08398501\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.08398501\n",
      "lol u got error 0.08398501\n",
      "current 0.08398501\n",
      "lol u got error 0.08398501\n",
      "current 0.08398501\n",
      "lol u got error 0.08398501\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083984986\n",
      "lol u got error 0.083984986\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.08398501\n",
      "lol u got error 0.08398501\n",
      "current 0.08398501\n",
      "lol u got error 0.08398501\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.08398501\n",
      "lol u got error 0.08398501\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.08398501\n",
      "lol u got error 0.08398501\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.083984986\n",
      "lol u got error 0.083984986\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083984986\n",
      "lol u got error 0.083984986\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.083984986\n",
      "lol u got error 0.083984986\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.08398501\n",
      "lol u got error 0.08398501\n",
      "current 0.08398501\n",
      "lol u got error 0.08398501\n",
      "current 0.08398501\n",
      "lol u got error 0.08398501\n",
      "current 0.08398501\n",
      "lol u got error 0.08398501\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985016\n",
      "lol u got error 0.083985016\n",
      "current 0.083985016\n",
      "lol u got error 0.083985016\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.083984986\n",
      "lol u got error 0.083984986\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083984986\n",
      "lol u got error 0.083984986\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.083984986\n",
      "lol u got error 0.083984986\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.083984986\n",
      "lol u got error 0.083984986\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083984986\n",
      "lol u got error 0.083984986\n",
      "current 0.083984986\n",
      "lol u got error 0.083984986\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.083984986\n",
      "lol u got error 0.083984986\n",
      "current 0.083984986\n",
      "lol u got error 0.083984986\n",
      "current 0.083984986\n",
      "lol u got error 0.083984986\n",
      "current 0.083984986\n",
      "lol u got error 0.083984986\n",
      "current 0.083984986\n",
      "lol u got error 0.083984986\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083984986\n",
      "lol u got error 0.083984986\n",
      "current 0.083984986\n",
      "lol u got error 0.083984986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083984986\n",
      "lol u got error 0.083984986\n",
      "current 0.083984986\n",
      "lol u got error 0.083984986\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.083984986\n",
      "lol u got error 0.083984986\n",
      "current 0.083984986\n",
      "lol u got error 0.083984986\n",
      "current 0.083984986\n",
      "lol u got error 0.083984986\n",
      "current 0.083984986\n",
      "lol u got error 0.083984986\n",
      "current 0.083984986\n",
      "lol u got error 0.083984986\n",
      "current 0.083984986\n",
      "lol u got error 0.083984986\n",
      "current 0.083984986\n",
      "lol u got error 0.083984986\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083984986\n",
      "lol u got error 0.083984986\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.083984986\n",
      "lol u got error 0.083984986\n",
      "current 0.083984986\n",
      "lol u got error 0.083984986\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083984986\n",
      "lol u got error 0.083984986\n",
      "current 0.083984986\n",
      "lol u got error 0.083984986\n",
      "current 0.083984986\n",
      "lol u got error 0.083984986\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.08398501\n",
      "lol u got error 0.08398501\n",
      "current 0.083984986\n",
      "lol u got error 0.083984986\n",
      "current 0.08398501\n",
      "lol u got error 0.08398501\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.08398501\n",
      "lol u got error 0.08398501\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.08398501\n",
      "lol u got error 0.08398501\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.083984986\n",
      "lol u got error 0.083984986\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.08398499\n",
      "lol u got error 0.08398499\n",
      "current 0.083985\n",
      "lol u got error 0.083985\n",
      "current 0.08398501\n",
      "lol u got error 0.08398501\n",
      "current 0.083985\n"
     ]
    }
   ],
   "source": [
    "fd = {x_tf: scaled_x , u_tf: scaled_T}\n",
    "for i in range(1000):\n",
    "    print(\"lol u got error\" , s.run(loss,fd))\n",
    "    s.run(mini,fd)\n",
    "    print(\"current\" , s.run(loss,fd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "2b5d3e63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.], dtype=float32)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.run(lambda_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2150ec89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baac5172",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4722442c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lolz=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "552f87be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(0,257):\n",
    "    lolz.append(8.0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1b00c3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_= np.array(lolz).reshape(257,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3b18cb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "k=tf.Variable(k_ , dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "68cf4d19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable_256:0' shape=(257, 1) dtype=float32_ref>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a33164d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def suh(x):\n",
    "    print(\"suh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ae45b926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suh\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6a1347",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24621b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d35ae8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abde48b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b188070e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1a4635",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535012da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8aba508",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e9bd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for 1d Rod simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d9958c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhysicsInformedNN2:\n",
    "    # Initialize the class\n",
    "    def __init__(self, X, u, layers, lb, ub):\n",
    "        \n",
    "        self.lb = lb\n",
    "        self.ub = ub\n",
    "        \n",
    "        self.x = X[:,0:1]\n",
    "#         self.t = X[:,1:2]\n",
    "        self.u = u\n",
    "        \n",
    "        self.layers = layers\n",
    "        \n",
    "        # Initialize NNs\n",
    "        self.weights, self.biases = self.initialize_NN(layers)\n",
    "        \n",
    "        # tf placeholders and graph\n",
    "        self.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True,\n",
    "                                                     log_device_placement=True))\n",
    "\n",
    "        # Initialize parameters\n",
    "        self.lambda_1 = tf.Variable([1.0], dtype=tf.float32)\n",
    "#         self.lambda_2 = tf.Variable([-6.0], dtype=tf.float32)\n",
    "        \n",
    "        self.x_tf = tf.placeholder(tf.float32, shape=[None, self.x.shape[1]])\n",
    "#         self.t_tf = tf.placeholder(tf.float32, shape=[None, self.t.shape[1]])\n",
    "        self.u_tf = tf.placeholder(tf.float32, shape=[None, self.u.shape[1]])\n",
    "                \n",
    "        self.u_pred = self.net_u(self.x_tf)\n",
    "        self.f_pred = self.net_f(self.x_tf)\n",
    "        \n",
    "        self.loss = tf.reduce_mean(tf.square(self.u_tf - self.u_pred)) + \\\n",
    "                    tf.reduce_mean(tf.square(self.f_pred))\n",
    "        \n",
    "#         self.optimizer = tf.contrib.opt.ScipyOptimizerInterface(self.loss, \n",
    "#                                                                 method = 'L-BFGS-B', \n",
    "#                                                                 options = {'maxiter': 50000,\n",
    "#                                                                            'maxfun': 50000,\n",
    "#                                                                            'maxcor': 50,\n",
    "#                                                                            'maxls': 50,\n",
    "#                                                                            'ftol' : 1.0 * np.finfo(float).eps})\n",
    "    \n",
    "        self.optimizer_Adam = tf.train.AdamOptimizer()\n",
    "        self.train_op_Adam = self.optimizer_Adam.minimize(self.loss)\n",
    "        \n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess.run(init)\n",
    "\n",
    "    def initialize_NN(self, layers):        \n",
    "        weights = []\n",
    "        biases = []\n",
    "        num_layers = len(layers) \n",
    "        for l in range(0,num_layers-1):\n",
    "            W = self.xavier_init(size=[layers[l], layers[l+1]])\n",
    "            b = tf.Variable(tf.zeros([1,layers[l+1]], dtype=tf.float32), dtype=tf.float32)\n",
    "            weights.append(W)\n",
    "            biases.append(b)        \n",
    "        return weights, biases\n",
    "        \n",
    "    def xavier_init(self, size):\n",
    "        in_dim = size[0]\n",
    "        out_dim = size[1]        \n",
    "        xavier_stddev = np.sqrt(2/(in_dim + out_dim))\n",
    "        return tf.Variable(tf.truncated_normal([in_dim, out_dim], stddev=xavier_stddev), dtype=tf.float32)\n",
    "    \n",
    "    def neural_net(self, X, weights, biases):\n",
    "        num_layers = len(weights) + 1\n",
    "        \n",
    "        H = (X - self.lb)/(self.ub - self.lb)\n",
    "        for l in range(0,num_layers-2):\n",
    "            W = weights[l]\n",
    "            b = biases[l]\n",
    "            H = tf.math.sigmoid(tf.add(tf.matmul(H, W), b))\n",
    "#             H = tf.tanh(tf.add(tf.matmul(H, W), b))\n",
    "        W = weights[-1]\n",
    "        b = biases[-1]\n",
    "        Y = tf.add(tf.matmul(H, W), b)\n",
    "        return Y\n",
    "    \n",
    "        \n",
    "            \n",
    "    def net_u(self, x):  \n",
    "        u = self.neural_net(tf.concat([x],1), self.weights, self.biases)\n",
    "        return u\n",
    "    \n",
    "    def net_f(self, x):\n",
    "        lambda_1 = self.lambda_1       \n",
    "#         lambda_2 = tf.exp(self.lambda_2)\n",
    "        u = self.net_u(x)\n",
    "        u_x = tf.gradients(u, x)[0]\n",
    "        u_xx = tf.gradients(u_x, x)[0]\n",
    "        f= (lambda_1+1)*u_xx\n",
    "#         f = -1*lambda_1*u_xx \n",
    "#         f = -1*lambda_1*u_xx\n",
    "        \n",
    "        return f\n",
    "    \n",
    "    def callback(self, loss, lambda_1):\n",
    "        print('Loss: %e, l1: %.5f' % (loss, lambda_1))\n",
    "        \n",
    "        \n",
    "    def train(self, nIter):\n",
    "        tf_dict = {self.x_tf: self.x, self.u_tf: self.u}\n",
    "        \n",
    "        start_time = time.time()\n",
    "        for it in range(nIter):\n",
    "            self.sess.run(self.train_op_Adam, tf_dict)\n",
    "            \n",
    "            # Print\n",
    "            if it % 10 == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                loss_value = self.sess.run(self.loss, tf_dict)\n",
    "                lambda_1_value = self.sess.run(self.lambda_1)\n",
    "#                 lambda_2_value = np.exp(self.sess.run(self.lambda_2))\n",
    "                print('It: %d, Loss: %.3e, Lambda_1: %.3f, Time: %.2f' % \n",
    "                      (it, loss_value, lambda_1_value, elapsed))\n",
    "                u_pred_x = tf.gradients(self.sess.run(self.u_pred,tf_dict),self.sess.run(self.x_tf,tf_dict))[0]\n",
    "#                 u_pred_xx = tf.gradients(u_pred_x,self.sess.run(self.x_tf,tf_dict))[0]\n",
    "                \n",
    "                print(u_pred_x.eval())\n",
    "                print((self.net_f(self.sess.run(self.x_tf,tf_dict))))\n",
    "                \n",
    "#                 print(self.sess.run(self.u_tf,tf_dict) ,\" \" , self.sess.run(self.u_pred,tf_dict))\n",
    "                start_time = time.time()\n",
    "        \n",
    "#         self.optimizer.minimize(self.sess,\n",
    "#                                 feed_dict = tf_dict,\n",
    "#                                 fetches = [self.loss, self.lambda_1],\n",
    "#                                 loss_callback = self.callback)\n",
    "        \n",
    "        \n",
    "    def predict(self, X_star):\n",
    "        \n",
    "        tf_dict = {self.x_tf: X_star}\n",
    "        \n",
    "        u_star = self.sess.run(self.u_pred, tf_dict)\n",
    "        f_star = self.sess.run(self.f_pred, tf_dict)\n",
    "        \n",
    "        return u_star, f_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac4e501e",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers2 = [1, 20, 20, 20, 20, 20, 20, 20, 20, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3d6d74bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "\n",
      "It: 0, Loss: 5.064e-01, Lambda_1: 1.000, Time: 5.11\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'eval'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-fc1ba84adf65>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPhysicsInformedNN2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscaled_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscaled_T\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayers2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mub\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-46-9a3041a9ab0a>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, nIter)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[1;31m#                 u_pred_xx = tf.gradients(u_pred_x,self.sess.run(self.x_tf,tf_dict))[0]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mu_pred_x\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnet_f\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx_tf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtf_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'eval'"
     ]
    }
   ],
   "source": [
    "model = PhysicsInformedNN2(scaled_x,scaled_T, layers2, lb, ub)\n",
    "model.train(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "34d48a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b=model.predict(scaled_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "df0bb480",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.32133472e-03],\n",
       "       [-3.06120515e-03],\n",
       "       [-2.81503797e-03],\n",
       "       [-2.58164108e-03],\n",
       "       [-2.36158073e-03],\n",
       "       [-2.15317309e-03],\n",
       "       [-1.95658207e-03],\n",
       "       [-1.77113712e-03],\n",
       "       [-1.59572065e-03],\n",
       "       [-1.43049657e-03],\n",
       "       [-1.27533078e-03],\n",
       "       [-1.12909079e-03],\n",
       "       [-9.91433859e-04],\n",
       "       [-8.61838460e-04],\n",
       "       [-7.40408897e-04],\n",
       "       [-6.26236200e-04],\n",
       "       [-5.19022346e-04],\n",
       "       [-4.18886542e-04],\n",
       "       [-3.25053930e-04],\n",
       "       [-2.36958265e-04],\n",
       "       [-1.54539943e-04],\n",
       "       [-7.75754452e-05],\n",
       "       [-5.60283661e-06],\n",
       "       [ 6.14523888e-05],\n",
       "       [ 1.24335289e-04],\n",
       "       [ 1.82881951e-04],\n",
       "       [ 2.37077475e-04],\n",
       "       [ 2.87979841e-04],\n",
       "       [ 3.35037708e-04],\n",
       "       [ 3.78921628e-04],\n",
       "       [ 4.20004129e-04],\n",
       "       [ 4.57659364e-04],\n",
       "       [ 4.92990017e-04],\n",
       "       [ 5.25474548e-04],\n",
       "       [ 5.55798411e-04],\n",
       "       [ 5.83872199e-04],\n",
       "       [ 6.09889627e-04],\n",
       "       [ 6.34267926e-04],\n",
       "       [ 6.56530261e-04],\n",
       "       [ 6.77138567e-04],\n",
       "       [ 6.96375966e-04],\n",
       "       [ 7.14123249e-04],\n",
       "       [ 7.30171800e-04],\n",
       "       [ 7.45177269e-04],\n",
       "       [ 7.58886337e-04],\n",
       "       [ 7.71939754e-04],\n",
       "       [ 7.83458352e-04],\n",
       "       [ 7.94246793e-04],\n",
       "       [ 8.03679228e-04],\n",
       "       [ 8.13215971e-04],\n",
       "       [ 8.21188092e-04],\n",
       "       [ 8.28564167e-04],\n",
       "       [ 8.35120678e-04],\n",
       "       [ 8.40827823e-04],\n",
       "       [ 8.46356153e-04],\n",
       "       [ 8.50886106e-04],\n",
       "       [ 8.55118036e-04],\n",
       "       [ 8.58709216e-04],\n",
       "       [ 8.61376524e-04],\n",
       "       [ 8.63417983e-04],\n",
       "       [ 8.65146518e-04],\n",
       "       [ 8.66353512e-04],\n",
       "       [ 8.66636634e-04],\n",
       "       [ 8.66919756e-04],\n",
       "       [ 8.66249204e-04],\n",
       "       [ 8.64863396e-04],\n",
       "       [ 8.63328576e-04],\n",
       "       [ 8.61078501e-04],\n",
       "       [ 8.58068466e-04],\n",
       "       [ 8.54521990e-04],\n",
       "       [ 8.50155950e-04],\n",
       "       [ 8.45476985e-04],\n",
       "       [ 8.39948654e-04],\n",
       "       [ 8.33511353e-04],\n",
       "       [ 8.26671720e-04],\n",
       "       [ 8.19057226e-04],\n",
       "       [ 8.10921192e-04],\n",
       "       [ 8.01697373e-04],\n",
       "       [ 7.91743398e-04],\n",
       "       [ 7.81133771e-04],\n",
       "       [ 7.69585371e-04],\n",
       "       [ 7.57262111e-04],\n",
       "       [ 7.43970275e-04],\n",
       "       [ 7.29680061e-04],\n",
       "       [ 7.14927912e-04],\n",
       "       [ 6.99073076e-04],\n",
       "       [ 6.82085752e-04],\n",
       "       [ 6.64681196e-04],\n",
       "       [ 6.46129251e-04],\n",
       "       [ 6.26206398e-04],\n",
       "       [ 6.05851412e-04],\n",
       "       [ 5.84140420e-04],\n",
       "       [ 5.61371446e-04],\n",
       "       [ 5.37976623e-04],\n",
       "       [ 5.12972474e-04],\n",
       "       [ 4.87416983e-04],\n",
       "       [ 4.60192561e-04],\n",
       "       [ 4.32565808e-04],\n",
       "       [ 4.03583050e-04],\n",
       "       [ 3.73557210e-04],\n",
       "       [ 3.42592597e-04],\n",
       "       [ 3.10674310e-04],\n",
       "       [ 2.77251005e-04],\n",
       "       [ 2.43306160e-04],\n",
       "       [ 2.08109617e-04],\n",
       "       [ 1.71720982e-04],\n",
       "       [ 1.34423375e-04],\n",
       "       [ 9.62466002e-05],\n",
       "       [ 5.65797091e-05],\n",
       "       [ 1.63316727e-05],\n",
       "       [-2.53468752e-05],\n",
       "       [-6.78151846e-05],\n",
       "       [-1.11192465e-04],\n",
       "       [-1.55493617e-04],\n",
       "       [-2.00361013e-04],\n",
       "       [-2.46852636e-04],\n",
       "       [-2.93791294e-04],\n",
       "       [-3.41698527e-04],\n",
       "       [-3.90216708e-04],\n",
       "       [-4.39479947e-04],\n",
       "       [-4.90188599e-04],\n",
       "       [-5.40956855e-04],\n",
       "       [-5.92693686e-04],\n",
       "       [-6.44862652e-04],\n",
       "       [-6.97821379e-04],\n",
       "       [-7.51495361e-04],\n",
       "       [-8.05944204e-04],\n",
       "       [-8.60482454e-04],\n",
       "       [-9.15884972e-04],\n",
       "       [-9.70989466e-04],\n",
       "       [-1.02707744e-03],\n",
       "       [-1.08388066e-03],\n",
       "       [-1.14017725e-03],\n",
       "       [-1.19699538e-03],\n",
       "       [-1.25373900e-03],\n",
       "       [-1.31101906e-03],\n",
       "       [-1.36885047e-03],\n",
       "       [-1.42590702e-03],\n",
       "       [-1.48284435e-03],\n",
       "       [-1.54025853e-03],\n",
       "       [-1.59712136e-03],\n",
       "       [-1.65382028e-03],\n",
       "       [-1.71115994e-03],\n",
       "       [-1.76773965e-03],\n",
       "       [-1.82347000e-03],\n",
       "       [-1.87887251e-03],\n",
       "       [-1.93403661e-03],\n",
       "       [-1.98873878e-03],\n",
       "       [-2.04275548e-03],\n",
       "       [-2.09598243e-03],\n",
       "       [-2.14818120e-03],\n",
       "       [-2.19990313e-03],\n",
       "       [-2.25116313e-03],\n",
       "       [-2.30114162e-03],\n",
       "       [-2.35000253e-03],\n",
       "       [-2.39799917e-03],\n",
       "       [-2.44483352e-03],\n",
       "       [-2.49001384e-03],\n",
       "       [-2.53473222e-03],\n",
       "       [-2.57751346e-03],\n",
       "       [-2.61901319e-03],\n",
       "       [-2.65890360e-03],\n",
       "       [-2.69722939e-03],\n",
       "       [-2.73472071e-03],\n",
       "       [-2.76981294e-03],\n",
       "       [-2.80307233e-03],\n",
       "       [-2.83500552e-03],\n",
       "       [-2.86450982e-03],\n",
       "       [-2.89227068e-03],\n",
       "       [-2.91809440e-03],\n",
       "       [-2.94195116e-03],\n",
       "       [-2.96312571e-03],\n",
       "       [-2.98233330e-03],\n",
       "       [-2.99909711e-03],\n",
       "       [-3.01386416e-03],\n",
       "       [-3.02600861e-03],\n",
       "       [-3.03560495e-03],\n",
       "       [-3.04242969e-03],\n",
       "       [-3.04687023e-03],\n",
       "       [-3.04833055e-03],\n",
       "       [-3.04725766e-03],\n",
       "       [-3.04287672e-03],\n",
       "       [-3.03636491e-03],\n",
       "       [-3.02651525e-03],\n",
       "       [-3.01314890e-03],\n",
       "       [-2.99727917e-03],\n",
       "       [-2.97823548e-03],\n",
       "       [-2.95591354e-03],\n",
       "       [-2.92995572e-03],\n",
       "       [-2.90079415e-03],\n",
       "       [-2.86823511e-03],\n",
       "       [-2.83233821e-03],\n",
       "       [-2.79277563e-03],\n",
       "       [-2.74954736e-03],\n",
       "       [-2.70308554e-03],\n",
       "       [-2.65197456e-03],\n",
       "       [-2.59801745e-03],\n",
       "       [-2.54002213e-03],\n",
       "       [-2.47828662e-03],\n",
       "       [-2.41217017e-03],\n",
       "       [-2.34180689e-03],\n",
       "       [-2.26823986e-03],\n",
       "       [-2.19021738e-03],\n",
       "       [-2.10843980e-03],\n",
       "       [-2.02222168e-03],\n",
       "       [-1.93174183e-03],\n",
       "       [-1.83646381e-03],\n",
       "       [-1.73743069e-03],\n",
       "       [-1.63392723e-03],\n",
       "       [-1.52641535e-03],\n",
       "       [-1.41398609e-03],\n",
       "       [-1.29656494e-03],\n",
       "       [-1.17516518e-03],\n",
       "       [-1.04948878e-03],\n",
       "       [-9.19058919e-04],\n",
       "       [-7.84009695e-04],\n",
       "       [-6.43238425e-04],\n",
       "       [-4.98950481e-04],\n",
       "       [-3.49730253e-04],\n",
       "       [-1.96039677e-04],\n",
       "       [-3.74764204e-05],\n",
       "       [ 1.26123428e-04],\n",
       "       [ 2.96026468e-04],\n",
       "       [ 4.69341874e-04],\n",
       "       [ 6.47366047e-04],\n",
       "       [ 8.30352306e-04],\n",
       "       [ 1.01855397e-03],\n",
       "       [ 1.21322274e-03],\n",
       "       [ 1.41161680e-03],\n",
       "       [ 1.61449611e-03],\n",
       "       [ 1.82279944e-03],\n",
       "       [ 2.03610957e-03],\n",
       "       [ 2.25597620e-03],\n",
       "       [ 2.47927010e-03],\n",
       "       [ 2.70782411e-03],\n",
       "       [ 2.94156373e-03],\n",
       "       [ 3.18041444e-03],\n",
       "       [ 3.42437625e-03],\n",
       "       [ 3.67552042e-03],\n",
       "       [ 3.92962992e-03],\n",
       "       [ 4.18895483e-03],\n",
       "       [ 4.45397198e-03],\n",
       "       [ 4.72365320e-03],\n",
       "       [ 5.00118732e-03],\n",
       "       [ 5.28118014e-03],\n",
       "       [ 5.56656718e-03],\n",
       "       [ 5.85672259e-03],\n",
       "       [ 6.15289807e-03],\n",
       "       [ 6.45618141e-03],\n",
       "       [ 6.76223636e-03],\n",
       "       [ 7.07378983e-03],\n",
       "       [ 7.39003718e-03],\n",
       "       [ 7.71157444e-03],\n",
       "       [ 8.03826749e-03],\n",
       "       [ 8.37308168e-03],\n",
       "       [ 8.71001184e-03],\n",
       "       [ 9.05239582e-03]], dtype=float32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc15094e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97018473",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f080d600",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfdbc87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a9aabac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "13baf732",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe1 = pd.read_csv(r\"C:\\Users\\Omkar\\Downloads\\Rod data.csv\")\n",
    " \n",
    "# print(dataframe1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b694839b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values=dataframe1.loc[:,\"Length [mm]\"]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "360f7955",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values=np.array(x_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b6258511",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values=x_values.reshape(257,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e4878882",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values=x_values.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "dc8edcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "T_values=dataframe1.loc[:,\"Value [K]\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "648f41d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "T_values=np.array(T_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f285ddef",
   "metadata": {},
   "outputs": [],
   "source": [
    "T_values=T_values.reshape(257,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "da0ae139",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_star=x_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d59245f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_star=tf.Variable(x_star, dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ba7cf0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "T_star=T_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "01a683ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "T_star=tf.Variable(T_star, dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "749c18c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "z=tf.Variable([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "64960e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "#running the code\n",
    "init = tf.global_variables_initializer() # prepare an init node\n",
    "with tf.Session() as sess:\n",
    "    init.run() # actually initialize all the variables\n",
    "    result = z.eval()\n",
    "    print(result)\n",
    "    #this will show the actual numbers output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c0f88d1e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RefVariable' object has no attribute 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-101-3331ba3f6426>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0ma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'RefVariable' object has no attribute 'numpy'"
     ]
    }
   ],
   "source": [
    "a=z.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34a56fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a521373",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "model1 = scaler.fit(x_star)\n",
    "scaled_x = model1.transform(x_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ebf37d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = scaler.fit(T_star)\n",
    "scaled_T = model2.transform(T_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3dae2d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(257, 1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aeced912",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(257, 1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a01127dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "N=200\n",
    "layers = [1, 20, 20, 20, 20, 20, 20, 20, 20, 1]\n",
    "# layers = [1 , 2 ,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "16820e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = tf.Variable([0.0],dtype=tf.float32)\n",
    "ub = tf.Variable([0.3],dtype=tf.float32) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "0cc58800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.3])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_star.max(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "19a2dba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_star.min(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e706f1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_ = np.random.choice(scaled_x.shape[0], N, replace=False)\n",
    "x_u_train = scaled_x[idx_,:]\n",
    "T_u_train = scaled_T[idx_,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ceb8774",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_u_train[:,0:1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bdf5e54d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 1)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T_u_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9b809a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PINN for rod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ab485f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4b05cf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#understading the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5befb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#xaviers init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "648d5af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample data x_star T_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7462eea6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fe9245a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xavier_init(size):\n",
    "    in_dim = size[0]\n",
    "    out_dim = size[1]        \n",
    "    xavier_stddev = np.sqrt(2/(in_dim + out_dim))\n",
    "    return tf.Variable(tf.truncated_normal([in_dim, out_dim], stddev=xavier_stddev), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a34a817",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_NN(layers):\n",
    "    weights = []\n",
    "    biases = []\n",
    "    num_layers = len(layers) \n",
    "    for l in range(0,num_layers-1):\n",
    "        W = xavier_init(size=[layers[l], layers[l+1]])\n",
    "        b = tf.Variable(tf.zeros([1,layers[l+1]], dtype=tf.float32), dtype=tf.float32)\n",
    "        weights.append(W)\n",
    "        biases.append(b)        \n",
    "    return weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3a5aec21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(2, 1) dtype=float32, numpy=\n",
       "array([[-0.2455011],\n",
       "       [ 0.8066156]], dtype=float32)>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xavier_init([2,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b16dec32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'truncated_normal_104:0' shape=(1, 2) dtype=float32>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.truncated_normal([1, 2], stddev=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5ab9908a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'truncated_normal_103:0' shape=(2,) dtype=float32>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.truncated_normal([2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "b390e923",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y=initialize_NN([1,2,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "89cbecd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'Variable:0' shape=(1, 2) dtype=float32, numpy=array([[-0.62955666,  1.1757891 ]], dtype=float32)>,\n",
       " <tf.Variable 'Variable:0' shape=(2, 1) dtype=float32, numpy=\n",
       " array([[ 0.8035322 ],\n",
       "        [-0.47300494]], dtype=float32)>]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24af35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "bfd2a1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_net( X, weights, biases):\n",
    "    num_layers = len(weights) + 1\n",
    "\n",
    "    H = 2.0*(X - lb)/(ub - lb) - 1.0\n",
    "    for l in range(0,num_layers-2):\n",
    "        W = weights[l]\n",
    "        b = biases[l]\n",
    "#         W = np.array(weights[l])\n",
    "#         b = np.array(biases[l])\n",
    "#         W=W.reshape(layers[l],layers[l+1])\n",
    "#         b=b.reshape(layers[l],layers[l+1])\n",
    "        H = tf.tanh(tf.add(tf.matmul(H, W), b))\n",
    "    W = weights[-1]\n",
    "#     W=W.reshape(layers[-2],layers[-1])\n",
    "    b = biases[-1]\n",
    "#     b=b.reshape(layers[-1],layers[-1])\n",
    "    Y = tf.add(tf.matmul(H, W), b)\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b25986f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=tf.Variable(x_star, dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f2c74f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "29826152",
   "metadata": {},
   "outputs": [],
   "source": [
    "b=x_star.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "4ccb9c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "H = 2.0*(b - lb)/(ub - lb) - 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "52fb8bb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(257), Dimension(1)])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "783b74e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "aec6b934",
   "metadata": {},
   "outputs": [],
   "source": [
    "W=[0.2,0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b414a954",
   "metadata": {},
   "outputs": [],
   "source": [
    "W=np.array(W,dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "10d6567e",
   "metadata": {},
   "outputs": [],
   "source": [
    "W=W.reshape(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "93555e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "b=[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f3469c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "b=np.array(b,dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1ca677dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "b=b.reshape(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9135851c",
   "metadata": {},
   "outputs": [],
   "source": [
    "c=tf.matmul(H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "392ef908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(257), Dimension(2)])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b4184d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "z=tf.add(c, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8d1cfe6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(257, 2)\n"
     ]
    }
   ],
   "source": [
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ec529976",
   "metadata": {},
   "outputs": [],
   "source": [
    "W=np.array(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0183900d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(<tf.Variable 'Variable:0' shape=(1, 2) dtype=float32, numpy=array([[-0.62955666,  1.1757891 ]], dtype=float32)>,\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a4b59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#u net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1709ee51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def net_u(x):  \n",
    "    u = neural_net(tf.concat([x],1),weights,biases)\n",
    "    return u\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717758ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8380195f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#net f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb67129f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def net_f(x):\n",
    "           \n",
    "    u = self.net_u(x)\n",
    "    \n",
    "    u_x = tf.gradients(u, x)[0]\n",
    "    u_xx = tf.gradients(u_x, x)[0]\n",
    "    f = -1*lambda1*u_xx\n",
    "    \n",
    "\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "67960bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#running code\n",
    "f=T_star\n",
    "g=T_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1f4a6760",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    f=np.append(f,g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "aca4c61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "f=f.reshape(25957,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ae0512ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "c=c.reshape(25957,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8e4e51bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "The value of a feed cannot be a tf.Tensor object. Acceptable feed values include Python scalars, strings, lists, numpy ndarrays, or TensorHandles. For reference, the tensor object was Tensor(\"strided_slice:0\", shape=(257, 1), dtype=float32) which was passed to the feed with key Tensor(\"Placeholder:0\", shape=(?, 1), dtype=float32).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-97-70d43ff10042>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPhysicsInformedNN2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_star\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mT_star\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mub\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;31m#     u_pred, f_pred = model.predict(X_star)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-96-3f8d71eabcb4>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, nIter)\u001b[0m\n\u001b[0;32m    102\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mit\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnIter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_op_Adam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m             \u001b[1;31m# Print\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PINNtf1\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    954\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 956\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    957\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PINNtf1\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1129\u001b[0m                             \u001b[1;34m'For reference, the tensor object was '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1130\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_val\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' which was passed to the '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1131\u001b[1;33m                             'feed with key ' + str(feed) + '.')\n\u001b[0m\u001b[0;32m   1132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1133\u001b[0m           \u001b[0msubfeed_dtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_numpy_dtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: The value of a feed cannot be a tf.Tensor object. Acceptable feed values include Python scalars, strings, lists, numpy ndarrays, or TensorHandles. For reference, the tensor object was Tensor(\"strided_slice:0\", shape=(257, 1), dtype=float32) which was passed to the feed with key Tensor(\"Placeholder:0\", shape=(?, 1), dtype=float32)."
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\": \n",
    "     \n",
    "#     nu = 0.01/np.pi\n",
    "\n",
    "#     N_u = 2000\n",
    "#     layers = [2, 20, 20, 20, 20, 20, 20, 20, 20, 1]\n",
    "    \n",
    "#     data = scipy.io.loadmat(r'C:\\Users\\Omkar\\Downloads\\burgers_shock.mat')\n",
    "    \n",
    "#     t = data['t'].flatten()[:,None]\n",
    "#     x = data['x'].flatten()[:,None]\n",
    "#     Exact = np.real(data['usol']).T\n",
    "    \n",
    "#     X, T = np.meshgrid(x,t)\n",
    "    \n",
    "#     X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
    "#     u_star = Exact.flatten()[:,None]              \n",
    "\n",
    "#     # Doman bounds\n",
    "#     lb = X_star.min(0)\n",
    "#     ub = X_star.max(0)    \n",
    "    \n",
    "#     ######################################################################\n",
    "#     ######################## Noiseles Data ###############################\n",
    "#     ######################################################################\n",
    "#     noise = 0.0            \n",
    "             \n",
    "#     idx = np.random.choice(X_star.shape[0], N_u, replace=False)\n",
    "#     X_u_train = X_star[idx,:]\n",
    "#     u_train = u_star[idx,:]\n",
    "    \n",
    "    model = PhysicsInformedNN2(x_star,T_star, layers, lb, ub)\n",
    "    model.train(1000)\n",
    "    \n",
    "#     u_pred, f_pred = model.predict(X_star)\n",
    "            \n",
    "#     error_u = np.linalg.norm(u_star-u_pred,2)/np.linalg.norm(u_star,2)\n",
    "    \n",
    "#     U_pred = griddata(X_star, u_pred.flatten(), (X, T), method='cubic')\n",
    "        \n",
    "    lambda_1_value = model.sess.run(model.lambda_1)\n",
    "#     lambda_2_value = model.sess.run(model.lambda_2)\n",
    "#     lambda_2_value = np.exp(lambda_2_value)\n",
    "    \n",
    "#     error_lambda_1 = np.abs(lambda_1_value - 1.0)*100\n",
    "#     error_lambda_2 = np.abs(lambda_2_value - nu)/nu * 100\n",
    "    \n",
    "#     print('Error u: %e' % (error_u))    \n",
    "#     print('Error l1: %.5f%%' % (error_lambda_1))                             \n",
    "#     print('Error l2: %.5f%%' % (error_lambda_2))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "140f935f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00247875]\n"
     ]
    }
   ],
   "source": [
    "print(lambda_1_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8390285b",
   "metadata": {},
   "source": [
    "class PhysicsInformedNN_rod:\n",
    "    # Initialize the class\n",
    "    def __init__(self, X, u, layers, lb, ub):\n",
    "        \n",
    "        self.lb = lb\n",
    "        self.ub = ub\n",
    "        \n",
    "        self.x = X[:,0:1]\n",
    "        self.u = u\n",
    "        \n",
    "        self.layers = layers\n",
    "        \n",
    "        # Initialize NNs\n",
    "        self.weights, self.biases = self.initialize_NN(layers)\n",
    "        \n",
    "        # tf placeholders and graph\n",
    "        self.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True,\n",
    "                                                     log_device_placement=True))\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self.lambda_1 = tf.Variable([0.0], dtype=tf.float32)\n",
    "        \n",
    "        \n",
    "        self.x_tf = tf.placeholder(tf.float32, shape=[None, self.x.shape[1]])\n",
    "        self.u_tf = tf.placeholder(tf.float32, shape=[None, self.u.shape[1]])\n",
    "        \n",
    "        self.u_pred = self.net_u(self.x_tf)\n",
    "        self.f_pred = self.net_f(self.x_tf)\n",
    "        \n",
    "        self.loss = tf.reduce_mean(tf.square(self.u_tf - self.u_pred)) + \\\n",
    "                    tf.reduce_mean(tf.square(self.f_pred))\n",
    "        \n",
    "        self.optimizer = tf.contrib.opt.ScipyOptimizerInterface(self.loss, \n",
    "                                                                method = 'L-BFGS-B', \n",
    "                                                                options = {'maxiter': 50000,\n",
    "                                                                           'maxfun': 50000,\n",
    "                                                                           'maxcor': 50,\n",
    "                                                                           'maxls': 50,\n",
    "                                                                           'ftol' : 1.0 * np.finfo(float).eps})\n",
    "    \n",
    "        self.optimizer_Adam = tf.train.AdamOptimizer()\n",
    "        self.train_op_Adam = self.optimizer_Adam.minimize(self.loss)\n",
    "        \n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess.run(init)\n",
    "        \n",
    "    def xavier_init(self, size):\n",
    "        in_dim = size[0]\n",
    "        out_dim = size[1]        \n",
    "        xavier_stddev = np.sqrt(2/(in_dim + out_dim))\n",
    "        return tf.Variable(tf.truncated_normal([in_dim, out_dim], stddev=xavier_stddev), dtype=tf.float32)\n",
    "        \n",
    "    def initialize_NN(self, layers):\n",
    "        weights = []\n",
    "        biases = []\n",
    "        num_layers = len(layers) \n",
    "        for l in range(0,num_layers-1):\n",
    "            W = self.xavier_init(size=[layers[l], layers[l+1]])\n",
    "            b = tf.Variable(tf.zeros([1,layers[l+1]], dtype=tf.float32), dtype=tf.float32)\n",
    "            weights.append(W)\n",
    "            biases.append(b)        \n",
    "        return weights, biases\n",
    "        \n",
    "        \n",
    "        \n",
    "    def neural_net(self, X, weights, biases):\n",
    "        num_layers = len(weights) + 1\n",
    "\n",
    "        H = 2.0*(X - self.lb)/(self.ub - self.lb) - 1.0\n",
    "        for l in range(0,num_layers-2):\n",
    "            W = weights[l]\n",
    "            b = biases[l]\n",
    "            H = tf.tanh(tf.add(tf.matmul(H, W), b))\n",
    "        W = weights[-1]\n",
    "        b = biases[-1]\n",
    "        Y = tf.add(tf.matmul(H, W), b)\n",
    "        return Y\n",
    "        \n",
    "    def net_u(self, x):  \n",
    "        u = self.neural_net(tf.concat([x],1), self.weights, self.biases)\n",
    "        return u\n",
    "\n",
    "    def net_f(self, x):\n",
    "        lambda_1 = self.lambda_1        \n",
    "        u = self.net_u(x)\n",
    "        u_x = tf.gradients(u, x)[0]\n",
    "        u_xx = tf.gradients(u_x, x)[0]\n",
    "        f = -1*lambda_1*u_xx\n",
    "\n",
    "        return f\n",
    "        \n",
    "        \n",
    "                \n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    def callback(self, loss, lambda_1):\n",
    "        print('Loss: %e, l1: %.5f' % (loss, lambda_1))\n",
    "        \n",
    "        \n",
    "    def train(self, nIter):\n",
    "        tf_dict = {self.x_tf: self.x,self.u_tf: self.u}\n",
    "        \n",
    "        start_time = time.time()\n",
    "        for it in range(nIter):\n",
    "            self.sess.run(self.train_op_Adam, tf_dict)\n",
    "            \n",
    "            # Print\n",
    "            if it % 10 == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                loss_value = self.sess.run(self.loss, tf_dict)\n",
    "                lambda_1_value = self.sess.run(self.lambda_1)\n",
    "                print('It: %d, Loss: %.3e, Lambda_1: %.3f, Time: %.2f' % \n",
    "                      (it, loss_value, lambda_1_value, elapsed))\n",
    "                start_time = time.time()\n",
    "        \n",
    "        self.optimizer.minimize(self.sess,\n",
    "                                feed_dict = tf_dict,\n",
    "                                fetches = [self.loss, self.lambda_1],\n",
    "                                loss_callback = self.callback)\n",
    "        \n",
    "        \n",
    "    def predict(self, X_star):\n",
    "        \n",
    "        tf_dict = {self.x_tf: X_star[:,0:1]}\n",
    "        \n",
    "        u_star = self.sess.run(self.u_pred, tf_dict)\n",
    "        f_star = self.sess.run(self.f_pred, tf_dict)\n",
    "        \n",
    "        return u_star, f_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6b2a24eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe1 = pd.read_csv(r\"C:\\Users\\Omkar\\Downloads\\Rod data.csv\")\n",
    "x_values=dataframe1.loc[:,\"Length [mm]\"] \n",
    "x_values=np.array(x_values)\n",
    "x_values=x_values.reshape(257,1)\n",
    "T_values=dataframe1.loc[:,\"Value [K]\"] \n",
    "T_values=np.array(T_values)\n",
    "T_values=T_values.reshape(257,1)\n",
    "x_star=x_values\n",
    "# x_star=tf.Variable(x_star, dtype = tf.float32)\n",
    "T_star=T_values\n",
    "# T_star=tf.Variable(T_star, dtype = tf.float32)\n",
    "lb = np.array([0.])\n",
    "ub = np.array([0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27e9b1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_u = 200\n",
    "layers = [1, 2 , 2 , 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c310e0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = 0.0            \n",
    "# Initialize parameters\n",
    "lambda_1 = tf.Variable([17], dtype=tf.float32)\n",
    "idx_ = np.random.choice(x_star.shape[0],N_u , replace=False)\n",
    "x_u_train = x_star[idx_,:]\n",
    "u_train = T_star[idx_,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db72b671",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xavier_init(size):\n",
    "        in_dim = size[0]\n",
    "        out_dim = size[1]        \n",
    "        xavier_stddev = np.sqrt(2/(in_dim + out_dim))\n",
    "        return tf.Variable(tf.truncated_normal([in_dim, out_dim], stddev=xavier_stddev), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96cd4363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_NN(layers):\n",
    "        weights = []\n",
    "        biases = []\n",
    "        \n",
    "        num_layers = len(layers) \n",
    "        for l in range(0,num_layers-1):\n",
    "            W = xavier_init(size=[layers[l], layers[l+1]])\n",
    "            b = tf.Variable(tf.zeros([1,layers[l+1]], dtype=tf.float32), dtype=tf.float32)\n",
    "            weights.append(W)\n",
    "            biases.append(b)        \n",
    "        return weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b2befb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, biases = initialize_NN(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45a556c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'Variable:0' shape=(1, 2) dtype=float32, numpy=array([[0., 0.]], dtype=float32)>,\n",
       " <tf.Variable 'Variable:0' shape=(1, 2) dtype=float32, numpy=array([[0., 0.]], dtype=float32)>,\n",
       " <tf.Variable 'Variable:0' shape=(1, 1) dtype=float32, numpy=array([[0.]], dtype=float32)>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18b9428a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_net(X, weights, biases):\n",
    "    num_layers = len(weights) + 1\n",
    "\n",
    "    H = 2.0*(X - lb)/(ub - lb) - 1.0\n",
    "    for l in range(0,num_layers-2):\n",
    "        W = weights[l]\n",
    "        b = biases[l]\n",
    "        H = tf.tanh(tf.add(tf.matmul(H, W), b))\n",
    "    W = weights[-1]\n",
    "    b = biases[-1]\n",
    "    Y = tf.add(tf.matmul(H, W), b)\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f51315b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=tf.Variable([1,2,3,4])\n",
    "x=tf.Variable([1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92b440d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=tf.gradients(y,x)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0552698",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Fetch argument None has invalid type <class 'NoneType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-dc52df60c080>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0ma_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mb_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mc_value\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PINNtf1\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    954\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 956\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    957\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PINNtf1\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1163\u001b[0m     \u001b[1;31m# Create a fetch handler to take care of the structure of fetches.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1164\u001b[0m     fetch_handler = _FetchHandler(\n\u001b[1;32m-> 1165\u001b[1;33m         self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)\n\u001b[0m\u001b[0;32m   1166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1167\u001b[0m     \u001b[1;31m# Run request and get response.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PINNtf1\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, graph, fetches, feeds, feed_handles)\u001b[0m\n\u001b[0;32m    472\u001b[0m     \"\"\"\n\u001b[0;32m    473\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 474\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetch_mapper\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_FetchMapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    475\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_targets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PINNtf1\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36mfor_fetch\u001b[1;34m(fetch)\u001b[0m\n\u001b[0;32m    261\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfetch\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m       raise TypeError('Fetch argument %r has invalid type %r' %\n\u001b[1;32m--> 263\u001b[1;33m                       (fetch, type(fetch)))\n\u001b[0m\u001b[0;32m    264\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m       \u001b[1;31m# NOTE(touts): This is also the code path for namedtuples.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Fetch argument None has invalid type <class 'NoneType'>"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    a_value = sess.run(y)\n",
    "    b_value = sess.run(x)\n",
    "    c_value= sess.run(a)\n",
    "    print(c_value)\n",
    "    print(b_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4904d92d",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Attempted to use a closed Session.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-46f4ee7a2cc7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\PINNtf1\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    954\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 956\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    957\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PINNtf1\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1101\u001b[0m     \u001b[1;31m# Check session.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_closed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1103\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Attempted to use a closed Session.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1104\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1105\u001b[0m       raise RuntimeError('The Session graph is empty.  Add operations to the '\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Attempted to use a closed Session."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ec023563",
   "metadata": {},
   "outputs": [],
   "source": [
    "def net_u(x):  \n",
    "        u = neural_net(tf.concat([x],1),weights,biases)\n",
    "        return u\n",
    "\n",
    "def net_f(x):        \n",
    "    u = net_u(x)\n",
    "    u_x = tf.gradients(u, x)[0]\n",
    "    u_xx = tf.gradients(u_x, x)[0]\n",
    "    f = -1*lambda_1*u_xx\n",
    "\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "eb485c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tf placeholders and graph\n",
    "sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True,\n",
    "                                             log_device_placement=True))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x_tf = tf.placeholder(tf.float32, shape=[None, a.shape[1]])\n",
    "u_tf = tf.placeholder(tf.float32, shape=[None, b.shape[1]])\n",
    "\n",
    "u_pred = net_u(x_tf)\n",
    "f_pred = net_f(x_tf)\n",
    "\n",
    "loss = tf.reduce_mean(tf.square(u_tf - u_pred)) + \\\n",
    "            tf.reduce_mean(tf.square(f_pred))\n",
    "\n",
    "optimizer = tf.contrib.opt.ScipyOptimizerInterface(loss, \n",
    "                                                        method = 'L-BFGS-B', \n",
    "                                                        options = {'maxiter': 50000,\n",
    "                                                                   'maxfun': 50000,\n",
    "                                                                   'maxcor': 50,\n",
    "                                                                   'maxls': 50,\n",
    "                                                                   'ftol' : 1.0 * np.finfo(float).eps})\n",
    "\n",
    "optimizer_Adam = tf.train.AdamOptimizer()\n",
    "train_op_Adam = optimizer_Adam.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "88677e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def callback(loss, lambda_1):\n",
    "        print('Loss: %e, l1: %.5f' % (loss, lambda_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bb2f0f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(nIter):\n",
    "    tf_dict = {x_tf: x_star,u_tf: T_star}\n",
    "\n",
    "    start_time = time.time()\n",
    "    for it in range(nIter):\n",
    "        sess.run(train_op_Adam, tf_dict)\n",
    "\n",
    "        # Print\n",
    "        if it % 10 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            loss_value = sess.run(loss, tf_dict)\n",
    "            lambda_1_value = sess.run(lambda_1)\n",
    "            print('It: %d, Loss: %.3e, Lambda_1: %.3f, Time: %.2f' % \n",
    "                  (it, loss_value, lambda_1_value, elapsed))\n",
    "            start_time = time.time()\n",
    "\n",
    "    optimizer.minimize(sess,\n",
    "                            feed_dict = tf_dict,\n",
    "                            fetches = [loss, lambda_1],\n",
    "                            loss_callback = callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9433d326",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X_star):\n",
    "        \n",
    "    tf_dict = {x_tf: x_star}\n",
    "\n",
    "    u_star = sess.run(u_pred, tf_dict)\n",
    "    f_star = sess.run(f_pred, tf_dict)\n",
    "\n",
    "    return u_star, f_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "357a9048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a=tf.Variable(x_u_train, dtype = tf.float32)\n",
    "b=tf.Variable(u_train, dtype = tf.float32)\n",
    "model = PhysicsInformedNN_rod(a, u_train, layers, lb, ub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a46d10e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It: 0, Loss: 1.292e+06, Lambda_1: 16.999, Time: 2.00\n",
      "It: 10, Loss: 1.290e+06, Lambda_1: 16.993, Time: 0.47\n",
      "It: 20, Loss: 1.284e+06, Lambda_1: 16.987, Time: 0.48\n",
      "It: 30, Loss: 1.272e+06, Lambda_1: 16.978, Time: 0.48\n",
      "It: 40, Loss: 1.268e+06, Lambda_1: 16.974, Time: 0.48\n",
      "It: 50, Loss: 1.265e+06, Lambda_1: 16.972, Time: 0.48\n",
      "It: 60, Loss: 1.262e+06, Lambda_1: 16.972, Time: 0.48\n",
      "It: 70, Loss: 1.260e+06, Lambda_1: 16.972, Time: 0.47\n",
      "It: 80, Loss: 1.257e+06, Lambda_1: 16.971, Time: 0.59\n",
      "It: 90, Loss: 1.255e+06, Lambda_1: 16.971, Time: 0.81\n",
      "It: 100, Loss: 1.252e+06, Lambda_1: 16.971, Time: 0.76\n",
      "It: 110, Loss: 1.250e+06, Lambda_1: 16.971, Time: 0.74\n",
      "It: 120, Loss: 1.247e+06, Lambda_1: 16.971, Time: 0.75\n",
      "It: 130, Loss: 1.245e+06, Lambda_1: 16.971, Time: 0.74\n",
      "It: 140, Loss: 1.243e+06, Lambda_1: 16.971, Time: 0.75\n",
      "It: 150, Loss: 1.240e+06, Lambda_1: 16.971, Time: 0.75\n",
      "It: 160, Loss: 1.238e+06, Lambda_1: 16.971, Time: 0.75\n",
      "It: 170, Loss: 1.236e+06, Lambda_1: 16.971, Time: 0.75\n",
      "It: 180, Loss: 1.234e+06, Lambda_1: 16.971, Time: 0.74\n",
      "It: 190, Loss: 1.231e+06, Lambda_1: 16.971, Time: 0.76\n",
      "It: 200, Loss: 1.229e+06, Lambda_1: 16.971, Time: 0.67\n",
      "It: 210, Loss: 1.227e+06, Lambda_1: 16.971, Time: 0.73\n",
      "It: 220, Loss: 1.225e+06, Lambda_1: 16.971, Time: 0.68\n",
      "It: 230, Loss: 1.223e+06, Lambda_1: 16.971, Time: 0.65\n",
      "It: 240, Loss: 1.221e+06, Lambda_1: 16.971, Time: 0.62\n",
      "It: 250, Loss: 1.219e+06, Lambda_1: 16.971, Time: 0.47\n",
      "It: 260, Loss: 1.216e+06, Lambda_1: 16.971, Time: 0.72\n",
      "It: 270, Loss: 1.214e+06, Lambda_1: 16.971, Time: 0.55\n",
      "It: 280, Loss: 1.212e+06, Lambda_1: 16.971, Time: 0.45\n",
      "It: 290, Loss: 1.210e+06, Lambda_1: 16.971, Time: 0.72\n",
      "It: 300, Loss: 1.208e+06, Lambda_1: 16.971, Time: 0.87\n",
      "It: 310, Loss: 1.206e+06, Lambda_1: 16.971, Time: 0.79\n",
      "It: 320, Loss: 1.204e+06, Lambda_1: 16.971, Time: 0.80\n",
      "It: 330, Loss: 1.202e+06, Lambda_1: 16.971, Time: 0.51\n",
      "It: 340, Loss: 1.200e+06, Lambda_1: 16.971, Time: 0.46\n",
      "It: 350, Loss: 1.198e+06, Lambda_1: 16.971, Time: 0.47\n",
      "It: 360, Loss: 1.195e+06, Lambda_1: 16.971, Time: 0.47\n",
      "It: 370, Loss: 1.193e+06, Lambda_1: 16.971, Time: 0.48\n",
      "It: 380, Loss: 1.191e+06, Lambda_1: 16.971, Time: 0.48\n",
      "It: 390, Loss: 1.189e+06, Lambda_1: 16.971, Time: 0.48\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-74-bc9369ad93c9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-72-5cdf2ab12ae4>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(nIter)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mit\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnIter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_op_Adam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;31m# Print\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PINNtf1\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    954\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 956\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    957\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PINNtf1\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1178\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1180\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1181\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PINNtf1\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1357\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1359\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1360\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PINNtf1\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1363\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1365\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1366\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PINNtf1\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[1;32m-> 1350\u001b[1;33m                                       target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PINNtf1\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[0;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1443\u001b[1;33m                                             run_metadata)\n\u001b[0m\u001b[0;32m   1444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1445\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "34110054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nu = 0.01/np.pi\n",
    "\n",
    "# N_u = 2000\n",
    "# layers = [1, 20, 20, 20, 20, 20, 20, 20, 20, 1]\n",
    "\n",
    "\n",
    "# data = scipy.io.loadmat(r'C:\\Users\\Omkar\\Downloads\\burgers_shock.mat')\n",
    "\n",
    "# t = data['t'].flatten()[:,None]\n",
    "# x = data['x'].flatten()[:,None]\n",
    "# Exact = np.real(data['usol']).T\n",
    "\n",
    "# X, T = np.meshgrid(x,t)\n",
    "\n",
    "# X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
    "# u_star = Exact.flatten()[:,None]              \n",
    "\n",
    "# Doman bounds\n",
    "\n",
    "\n",
    "######################################################################\n",
    "######################## Noiseles Data ###############################\n",
    "######################################################################\n",
    "# noise = 0.0            \n",
    "\n",
    "# idx_ = np.random.choice(x_star.shape[0],N_u , replace=False)\n",
    "# x_u_train = x_star[idx_,:]\n",
    "# u_train = u_star[idx_,:]\n",
    "\n",
    "model = PhysicsInformedNN(X_u_train, u_train, layers, lb, ub)\n",
    "# model.train(0)\n",
    "\n",
    "# u_pred, f_pred = model.predict(X_star)\n",
    "\n",
    "# error_u = np.linalg.norm(u_star-u_pred,2)/np.linalg.norm(u_star,2)\n",
    "\n",
    "# U_pred = griddata(X_star, u_pred.flatten(), (X, T), method='cubic')\n",
    "\n",
    "# lambda_1_value = model.sess.run(model.lambda_1)\n",
    "# lambda_2_value = model.sess.run(model.lambda_2)\n",
    "# lambda_2_value = np.exp(lambda_2_value)\n",
    "\n",
    "# error_lambda_1 = np.abs(lambda_1_value - 1.0)*100\n",
    "# error_lambda_2 = np.abs(lambda_2_value - nu)/nu * 100\n",
    "\n",
    "# print('Error u: %e' % (error_u))    \n",
    "# print('Error l1: %.5f%%' % (error_lambda_1))                             \n",
    "# print('Error l2: %.5f%%' % (error_lambda_2))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "0ae22b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = scipy.io.loadmat(r'C:\\Users\\Omkar\\Downloads\\burgers_shock.mat')\n",
    "t = data['t'].flatten()[:,None]\n",
    "x = data['x'].flatten()[:,None]\n",
    "Exact = np.real(data['usol']).T\n",
    "\n",
    "X, T = np.meshgrid(x,t)\n",
    "\n",
    "X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
    "u_star = Exact.flatten()[:,None]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "42a01d85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25600, 2)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_star.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869d26b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fe3edf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd4d2ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c945815c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b7bcad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5799819",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47445ed7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c6f7c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0233e693",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732c968b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522821e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50f3132",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221f0a91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf805e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
